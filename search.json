[{"title":"large language model","url":"/2025/07/19/ai/01-ai-foundation/","content":"Large Language ModelDefinitionA large language model is a machine learning model that is trained on a large corpus of text data, such as Wikipedia or the Web. These models can generate high-quality text that is similar to the training data, but can also generate text that is not present in the training data.\nHistory\n&lt; 1990s: IBM‚Äôs statistical language model (SLM)\n1990s-2000s: Neural language models (NLLMs)\n2001s: n-gram model\n2010s: GPT-2, GPT-3\n\nDataset PreprocessingTokenizationSplitting the text into individual words or sub-words\n\nByte-Pair Encoding (BPE): A sub-word unit that is used to represent a word in a sequence. It is a compression algorithm that merges frequent sub-words into a single token.\n\nDataset cleaning\nRemove stop words: Common words that do not provide any useful information\nStemming: Reducing words to their root form\nLemmatization: Reducing words to their base form\n\nSynthetic DataSynthetic data is a technique used to generate new data by combining existing data. It can be used to train a language model on a small dataset and then use it to generate new text.\nTrainingCost\nTraining time: The amount of time it takes to train a large language model\nMemory usage: The amount of memory required to train a large language model\nGPU usage: The amount of GPU memory required to train a large language model\n\nFine-tuningFine-tuning is a technique used to adapt a pre-trained language model to a specific task. It involves unfreezing the layers of the model and re-training them on a new task.\nArchitectureAttention Mechanism and context windowThe attention mechanism is a mechanism that allows the model to focus on specific parts of the input sequence when generating the output. It is based on the context window, which is a window of words that surrounds the current word. The attention mechanism allows the model to pay more attention to the relevant parts of the input sequence, while ignoring irrelevant parts.\nMixture of expertsThe mixture of experts (MoE) is a technique used to train a large language model that can generate text with multiple experts. It is based on the idea that multiple experts can generate different parts of the text, and the model can combine the outputs of these experts to generate the final output.\nParameter sizeThe parameter size of a large language model is the amount of memory required to store its weights. It is important to consider the size of the model when choosing the hardware resources required to train it.\n\nQuantization: Reducing the size of the model by reducing the number of bits used to represent the weights.\n\nReferencesLarge Language Models\nArtificial IntelligenceConceptArtificial Intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. It involves the use of computational processes to simulate intelligent behavior. The term ‚Äúintelligence‚Äù refers to the ability of a machine to acquire and apply knowledge and skills, and to reason and problem-solve autonomously.\nTechniques\nMachine Learning: The process of training a machine to learn from experience and improve its performance on a task.\nDeep Learning: A subset of machine learning that involves training a machine to learn from large datasets using neural networks.\nNatural Language Processing: The use of AI to understand and manipulate human language.\n\nReferencesArtificial Intelligence\nProduces\nGPT\nClaude\nGemini\nLlama\nMistral Large\nGrok\nDeekseek\nTongyi\nKimi\n\n\n","categories":["ai","foundation","large language model"],"tags":["ai"]},{"title":"large language model","url":"/2025/07/19/ai/02-ai-deep-learning/","content":"ReferencesDive Into Deep Learning\nCreate Environmentuse conda or minicondaconda env remove d2l-zhconda create -n d2l-zh -y python=3.8 pipconda activate d2l-zh\ninstall dependenciespip install jupyter d2l torch torchvision\ndownload d2l-zh.zipwget http://zh-v2.d2l.ai/d2l-zh.zipunzip d2l-zh.zipjupyter notebook\n","categories":["ai","foundation","large language model"],"tags":["ai"]},{"title":"Azure Container registries","url":"/2025/07/14/azure/01-container-registries/","content":"After building our artifacts, we need a place to store them. Common options include Docker Hub, Nexus Repository, Google Container Registry, and Amazon Elastic Container Registry.\nIn this section, we‚Äôll focus on Azure Container Registry (ACR).\nCreating an Azure Container Registry\nSelect the appropriate Resource Group.\nEnter a unique Registry Name.\nLeave all other settings at their default values and proceed with the creation.\n\n\nConfigure Credentials for Local Container ManagementThere are two key settings to focus on when configuring credentials:\n\nset username and password to the local container management softwareFor example, when using Podman, you can authenticate with your container registry to enable image pushing and pulling.\n\nPush image to Container Repositoryhits: don‚Äôt forget to tag your image first.\n","categories":["cloud-platform","azure","container-registries"],"tags":["azure"]},{"title":"docker install DVWA","url":"/2025/08/03/docker/01-docker-installDVWA/","content":"Installing DVWA using DockerInstall the Damn Vulnerable Web Application (via a Docker container) quite easily, as it will take care of all the dependencies required (e.g. mySQL).\n\nOpen a terminal within your Ubuntu instance.\nInstall DVWA with the following command: sudo docker run ‚Äîrestart always -p 80:80 vulnerables/web-dvwa\nDVWA should now be installed and running. To access it, open up Firefox and browse to your localhost address. In other words, in the URL toolbar (where you type in URLs), type in localhost and press the ENTER key.\nLogin to DVWA with username admin and password password\nOnce logged in, select the ‚ÄòCreate/Reset Database‚Äô link.\nReturn Ubuntu back to an Internal network (so that Ubuntu and Kali and communicate with each other; refer to the ‚ÄòConnect Ubuntu and Kali‚Äô screencast for a reminder if needed).\nEnsure that they are connected by pinging each other.\n\n","categories":["docker","DVWA"],"tags":["DVWA"]},{"title":"Docker Compose Install MongoDB","url":"/2025/08/04/docker/02-docker-mongodb/","content":"Installing MongoDB using Docker Composeversion: &quot;3.8&quot;services:  mongodb:    image: mongo    container_name: mongodb    ports:      - 27017:27017    volumes:      - ./database:/data/db    environment:      - MONGO_INITDB_ROOT_USERNAME=admin      - MONGO_INITDB_ROOT_PASSWORD=adminpwd  # mongo-express:  #   image: mongo-express  #   container_name: mongo-express  #   restart: always  #   ports:  #     - 8081:8081  #   environment:  #     - ME_CONFIG_MONGODB_ADMINUSERNAME=admin  #     - ME_CONFIG_MONGODB_ADMINPASSWORD=adminpwd  #     - ME_CONFIG_MONGODB_SERVER=mongodbnetworks:  default:    name: mongodb_network","categories":["docker","docker compose","mongodb"],"tags":["mongodb"]},{"title":"docker install supabase","url":"/2025/08/03/docker/02-docker-supabase/","content":"InstructionIt is recommended to use Linux, as issues may occur when connecting to the pool on Windows systems.\ndocker install supabase# Get the codegit clone --depth 1 https://github.com/supabase/supabase# Make your new supabase project directorymkdir supabase-project# Tree should look like this# .# ‚îú‚îÄ‚îÄ supabase# ‚îî‚îÄ‚îÄ supabase-project# Copy the compose files over to your projectcp -rf supabase/docker/* supabase-project# Copy the fake env varscp supabase/docker/.env.example supabase-project/.env# Switch to your project directorycd supabase-project# Pull the latest imagesdocker compose pull# Start the services (in detached mode)docker compose up -d\n\nRestore databseaccess to web service\nconnect database by these parameterhost: localhostport: 5432database:  postgresusername: postgres.your-tenant-idpassword:  your-super-secret-and-long-postgres-password\n\nrestore database from file\nget connection stringbrowser http://localhost:8000input username:  supabasepassword: this_password_is_insecure_and_should_be_updated\nreplace the connection string in .env file\nrun project and testrun project and test\nreferenceinstall supabase with docker\n","categories":["docker","supabase"],"tags":["docker"]},{"title":"Docker Compose Install Mysql","url":"/2025/08/04/docker/03-docker-mysql/","content":"Installing Mysql using Docker Composeversion: &#x27;3&#x27;services:  mysql:    image: mysql:8.0.18    container_name: mysql8    environment:      - MYSQL_ROOT_PASSWORD=root#      - TZ=Asia/Shanghai    volumes:      - ./log:/var/log/mysql      - ./data:/var/lib/mysql      - ./conf/conf.d:/etc/mysql/conf.d      # - /etc/localtime:/etc/localtime:ro    ports:      - 3306:3306    restart: always","categories":["docker","docker compose","mysql"],"tags":["mysql"]},{"title":"Docker Compose Install Postgres","url":"/2025/08/04/docker/04-docker-postgres/","content":"Installing Postgres using Docker Composeversion: &quot;3.9&quot;services:  postgres:    image: postgres:13.22-trixie    container_name: postgres_13    restart: always    environment:      POSTGRES_USER: root      POSTGRES_PASSWORD: root      POSTGRES_DB: mydb    ports:      - &quot;5432:5432&quot;    volumes:      - ./data:/var/lib/postgresql/data    healthcheck:      test: [&quot;CMD-SHELL&quot;, &quot;pg_isready -U root&quot;]","categories":["docker","docker compose","postgres"],"tags":["postgres"]},{"title":"Using Hexo to Build a Personal Blog","url":"/2025/07/15/hexo/01-build-blog/","content":"Build Your Own Blog with HexoThis guide walks you through the process of creating a personal blog using Hexo. It includes environment setup, Hexo installation, theme configuration, and running the site locally.\n\n1. Install Development Environment (Node.js)Download and install the LTS version of Node.js from the official website:\nüîó https://nodejs.org/en\n\n2. Install Hexo and Initialize BlogRun the following commands in your terminal:\nnpm install -g hexo-cli          # Install Hexo CLI globallyhexo init blog                   # Initialize a new Hexo projectcd blog                          # Enter the project directorynpm install                      # Install dependencieshexo server                      # Start the local development server\nüìò Visit the official documentation: https://hexo.io/zh-cn/\n3. Install Butterfly ThemeButterfly is a powerful and beautiful theme for Hexo.butterfly\nTheme configurationJust follow the guidelines to set up almost all the basic configurations.\nbutterfly configutation\nInstall Plugins\ntag cloud: Other tag cloud plugins offer even cooler visual stylestag-cloud\n\nlive2d: A 2D model that makes your blog more lively and engaginglive2d\n\n\n","categories":["hexo"],"tags":["hexo"]},{"title":"terraform","url":"/2025/08/24/iac/01-iac-terraform/","content":"TerraformDocumentationterraform\nTofuDocumentationtofu\n","categories":["iac","terraform"],"tags":["iac"]},{"title":"interview question of Java Backend","url":"/2025/08/24/interview/02-java-backend/","content":"Interview QuestionsCore Java (OOPs, Collections, Concurrency Basics)Explain difference between HashMap, HashTable, and ConcurrentHashMap.\n\n\n\nFeature\nHashMap\nHashtable\nConcurrentHashMap\n\n\n\n\nThread Safety\nNot thread-safe\nThread-safe (synchronized methods)\nThread-safe (lock segmentation / fine-grained locking)\n\n\nNull Keys/Values\nAllows one null key and multiple null values\nDoes not allow any null keys/values\nDoes not allow null keys/values\n\n\nPerformance\nFaster (no synchronization overhead)\nSlower (entire table is locked)\nBetter concurrency ‚Äî multiple threads can read/write different segments simultaneously\n\n\nUse Case\nSingle-threaded apps\nLegacy thread-safe code\nHigh-performance concurrent apps\n\n\n\n\nExample:If multiple threads are accessing and modifying a map, prefer ConcurrentHashMap. For non-concurrent cases, use HashMap. Avoid Hashtable in modern code.\nHow does equals() and hashCode() work together in collections?\nhashCode(): Returns an integer hash value used to place objects in hash-based collections (like HashMap, HashSet).\nequals(): Compares two objects for logical equality.\n\nContract:\n\nIf two objects are equal according to equals(), they must have the same hashCode().\nIf two objects have the same hashCode(), they may or may not be equal.\n\nWhy it matters:    Collections like HashMap use hashCode() to find a bucket, and then equals() to check if the key already exists.Example:@Overridepublic boolean equals(Object o) &#123;    if (this == o) return true;    if (!(o instanceof Person)) return false;    Person p = (Person) o;    return id == p.id;&#125;@Overridepublic int hashCode() &#123;    return Objects.hash(id);&#125;\nDifference between shallow copy vs deep copy in Java.\n\n\n\nType\nDescription\nExample\n\n\n\n\nShallow Copy\nCopies only object references (not the objects they refer to). Both objects share the same nested objects.\nclone() without custom implementation\n\n\nDeep Copy\nCopies everything ‚Äî including new instances of nested objects. Changes in one object don‚Äôt affect the other.\nImplemented manually or via serialization\n\n\n\n\nExample:// Shallow copyEmployee e2 = e1.clone(); // e1.address and e2.address point to same object// Deep copyEmployee e2 = new Employee(e1);e2.address = new Address(e1.address);\nHow do you prevent deadlock in multithreaded applications?Deadlock occurs when two or more threads are waiting on each other‚Äôs locks.\nWays to prevent it:\n\nLock ordering ‚Äì Always acquire locks in a consistent order.\nTry-lock with timeout ‚Äì Use tryLock() from ReentrantLock to avoid waiting forever.\nAvoid nested locks ‚Äì Reduce code sections that acquire multiple locks.\nUse concurrent utilities ‚Äì Prefer ConcurrentHashMap, BlockingQueue, etc., which manage synchronization internally.\nMinimize synchronized blocks ‚Äì Keep critical sections small.\n\nExample:if (lock1.tryLock(50, TimeUnit.MILLISECONDS)) &#123;    if (lock2.tryLock(50, TimeUnit.MILLISECONDS)) &#123;        // work    &#125;&#125;\nExplain volatile vs synchronized ‚Äî when to use each.\n\n\n\nFeature\nvolatile\nsynchronized\n\n\n\n\nPurpose\nGuarantees visibility of changes across threads\nGuarantees atomicity and visibility\n\n\nUse Case\nWhen multiple threads read/write a variable, and writes are independent\nWhen operations on shared data must be atomic\n\n\nLocking\nNo locking (lighter, faster)\nUses intrinsic lock (can block threads)\n\n\nScope\nVariable-level only\nCode block or method\n\n\nExample Use\nStatus flags, configuration values\nCounters, complex state updates\n\n\n\n\nExample:volatile boolean running = true; // visibility onlysynchronized void increment() &#123;  // atomic operation    count++;&#125;Rule of thumb:\n\nUse volatile for simple flags or status updates.\nUse synchronized (or locks) for compound operations that must be atomic.\n\nSpring &amp; Spring BootDifference between @Component, @Service, @Repository, and @Controller.Spring provides several stereotype annotations to define beans, which also give semantic meaning for better readability:\n\n\n\n\nAnnotation\nPurpose\nSpecial Behavior\n\n\n\n\n@Component\nGeneric stereotype for any Spring-managed component\nBase annotation for all other stereotypes\n\n\n@Service\nIndicates a service layer component (business logic)\nMainly semantic, no extra behavior, improves readability\n\n\n@Repository\nIndicates a DAO (Data Access Object) component\nConverts database exceptions into Spring‚Äôs DataAccessException\n\n\n@Controller\nIndicates a web controller handling HTTP requests\nTypically used with @RequestMapping to handle endpoints; works with Spring MVC\n\n\n\n\nSummary:All are detected via component scanning and registered as Spring beans, but @Repository and @Controller have additional framework-specific roles.\nWhat is Spring Boot auto-configuration? How does it work internally?Auto-configuration is a key feature of Spring Boot that automatically configures your application based on the dependencies on the classpath and properties defined in application.properties or application.yml.\nHow it works internally:\n\nSpring Boot uses the @SpringBootApplication annotation, which includes @EnableAutoConfiguration.\n@EnableAutoConfiguration imports spring.factories which lists all auto-configuration classes.\nDuring startup, Spring Boot evaluates each auto-configuration class using @Conditional annotations (e.g., @ConditionalOnClass, @ConditionalOnMissingBean) to decide which beans to create.\nBeans are registered automatically based on what classes and libraries are available on the classpath.\n\nExample:If spring-boot-starter-data-jpa is on the classpath, Spring Boot auto-configures EntityManagerFactory and DataSource beans automatically.\nHow do you implement exception handling globally in Spring Boot REST APIs?Spring Boot provides @ControllerAdvice for global exception handling.\nSteps:\n\nCreate a class annotated with @ControllerAdvice.\nDefine methods annotated with @ExceptionHandler to handle specific exceptions.\nOptionally, use @ResponseStatus or return a custom response body.\n\nExample:\n@ControllerAdvicepublic class GlobalExceptionHandler &#123;    @ExceptionHandler(ResourceNotFoundException.class)    @ResponseStatus(HttpStatus.NOT_FOUND)    public ResponseEntity&lt;String&gt; handleResourceNotFound(ResourceNotFoundException ex) &#123;        return ResponseEntity.status(HttpStatus.NOT_FOUND).body(ex.getMessage());    &#125;    @ExceptionHandler(Exception.class)    @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR)    public ResponseEntity&lt;String&gt; handleGenericException(Exception ex) &#123;        return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)                             .body(&quot;Something went wrong: &quot; + ex.getMessage());    &#125;&#125;\nExplain difference between @RequestParam, @PathVariable, and @RequestBody.\n\n\n\nAnnotation\nUsage\nExample\n\n\n\n\n@RequestParam\nBinds query parameters or form data\nGET /users?id=123 ‚Üí @RequestParam(&quot;id&quot;) Long id\n\n\n@PathVariable\nBinds URI template variables\nGET /users/123 ‚Üí @PathVariable(&quot;id&quot;) Long id\n\n\n@RequestBody\nBinds HTTP request body (JSON/XML) to Java object\nPOST /users with JSON &#123; &quot;name&quot;: &quot;John&quot; &#125; ‚Üí @RequestBody User user\n\n\n\n\nSummary:\n\n@RequestParam ‚Üí query/form parameters\n@PathVariable ‚Üí path segments\n@RequestBody ‚Üí request body content\n\nHow do you implement profiles in Spring Boot for multiple environments?Spring Boot supports profiles to separate environment-specific configurations (dev, test, prod).\nSteps:\n\nCreate profile-specific properties files:\napplication-dev.properties\napplication-prod.properties\n\n\nActivate a profile:\nIn application.properties:spring.profiles.active=dev\nOr via command line:java -jar app.jar --spring.profiles.active=prod\n\n\nUse @Profile annotation to conditionally load beans:@Configuration@Profile(&quot;dev&quot;)public class DevConfig &#123;    @Bean    public DataSource devDataSource() &#123;        return new HikariDataSource();    &#125;&#125;\nSummary:Profiles allow you to maintain different configurations for different environments while keeping a single codebase.\n\nJPA / HibernateWhat are entity states (Transient, Persistent, Detached, Removed) in Hibernate?In Hibernate, an entity can exist in one of four states:\n\n\n\n\nState\nDescription\nExample\n\n\n\n\nTransient\nThe entity is not associated with a Hibernate Session and not persisted in the database.\nUser user = new User(); (new object, no save() called)\n\n\nPersistent\nThe entity is associated with a Hibernate Session and changes will be automatically synchronized with the database.\nsession.save(user);\n\n\nDetached\nThe entity was persistent, but the session is closed or entity is evicted. Changes are not automatically persisted.\nsession.close(); user.setName(&quot;John&quot;);\n\n\nRemoved\nThe entity is scheduled for deletion. It will be deleted when the transaction commits.\nsession.delete(user);\n\n\n\n\nExplain lazy loading and how to avoid LazyInitializationException.Lazy loading is when Hibernate delays loading of associated entities until they are actually accessed.\n\nExample:  @Entityclass Department &#123;    @OneToMany(fetch = FetchType.LAZY)    private List&lt;Employee&gt; employees;&#125;\nemployees are loaded from the database only when getEmployees() is called.\n\nLazyInitializationException occurs if you try to access a lazily loaded collection outside an active session.\nWays to avoid it:\n\nOpen Session in View (not recommended for all cases): Keep session open during view rendering.\nFetch join in JPQL: SELECT d FROM Department d JOIN FETCH d.employees WHERE d.id = :id\nEager fetching (careful with performance): @OneToMany(fetch = FetchType.EAGER)\nInitialize collection manually before session closes: Hibernate.initialize(department.getEmployees());\n\n\nDifference between Criteria API and JPQL.\n\n\n\nFeature\nJPQL\nCriteria API\n\n\n\n\nQuery Type\nString-based queries\nObject-oriented, type-safe queries\n\n\nCompile-time safety\nNo\nYes\n\n\nReadability\nSimple for static queries\nBetter for dynamic queries\n\n\n\n\nExample// SELECT u FROM User u WHERE u.age &gt; 25CriteriaBuilder cb = em.getCriteriaBuilder();CriteriaQuery&lt;User&gt; cq = cb.createQuery(User.class);Root&lt;User&gt; user = cq.from(User.class);cq.select(user).where(cb.gt(user.get(&quot;age&quot;), 25));\nWhat is the difference between CascadeType.ALL and orphanRemoval?\n\n\n\nFeature\nCascadeType.ALL\norphanRemoval = true\n\n\n\n\nPurpose\nApplies all operations (persist, merge, remove, refresh) from parent to child\nDeletes child entities when they are removed from parent collection\n\n\nExample\nSaving parent automatically saves child\nRemoving a child from parent‚Äôs collection deletes it from DB\n\n\nKey Difference\nCascade propagates operations, not necessarily removal\nOrphan removal only deletes orphans but doesn‚Äôt cascade persist\n\n\n\n\nExample:@OneToMany(cascade = CascadeType.ALL, orphanRemoval = true)private List&lt;Employee&gt; employees;\n\ncascade = ALL ‚Üí persisting parent also persists employees\norphanRemoval = true ‚Üí removing employee from list deletes it from DB\n\nHow do you implement pagination and sorting in Spring Data JPA?Spring Data JPA provides Pageable and Sort interfaces for pagination and sorting.\nExample:@Repositorypublic interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123;    Page&lt;User&gt; findByAgeGreaterThan(int age, Pageable pageable);&#125;Usage in service:Pageable pageable = PageRequest.of(0, 10, Sort.by(&quot;name&quot;).ascending());Page&lt;User&gt; users = userRepository.findByAgeGreaterThan(25, pageable);users.getContent().forEach(System.out::println);System.out.println(&quot;Total pages: &quot; + users.getTotalPages());\n\nPageRequest.of(page, size, sort) specifies page number, page size, and sorting.\nReturns a Page which contains content, total pages, total elements, and more.\n\nMicroservices &amp; ArchitectureExplain difference between monolithic and microservices architectures.\n\n\n\nAspect\nMonolithic Architecture\nMicroservices Architecture\n\n\n\n\nStructure\nSingle unified codebase\nCollection of small, independent services\n\n\nDeployment\nOne deployable unit\nEach service deployed independently\n\n\nScaling\nScale entire application\nScale individual services as needed\n\n\nTechnology Stack\nUsually uniform\nCan use different stacks per service\n\n\nFault Isolation\nFailure in one module may affect entire app\nFailures isolated to the specific service\n\n\nDevelopment Speed\nSlower for large teams\nFaster with independent teams\n\n\n\n\nSummary:Monolith = single large app; Microservices = many small, loosely coupled apps that communicate over APIs.\nHow do you implement inter-service communication? (REST, Kafka, gRPC)Microservices need to communicate with each other. Common methods:\n\nREST (HTTP/JSON)  \nSimple, widely used, synchronous request-response.  \nExample:@GetMapping(&quot;/users/&#123;id&#125;&quot;)public User getUser(@PathVariable Long id) &#123; ... &#125;\n\n\nKafka (Event-driven messaging)\nAsynchronous, decoupled communication using topics.\nExample: Service A produces events ‚Üí Service B consumes events.\n\n\ngRPC (Remote Procedure Calls)\nHigh-performance, strongly typed, binary protocol using Protocol Buffers.\nExample:service UserService &#123;    rpc GetUser(UserRequest) returns (UserResponse);&#125;\nSummary:\n\n\n\n\nREST ‚Üí synchronous, simple\nKafka ‚Üí asynchronous, decoupled\ngRPC ‚Üí synchronous or streaming, high performance\n\nWhat is API Gateway and why do we need it?An API Gateway is a single entry point for clients to interact with multiple microservices.\nResponsibilities:\n\nRouting requests to appropriate microservices\nRequest aggregation (combine multiple service responses)\nAuthentication &amp; authorization\nRate limiting, caching, logging\n\nExample:\n\nNetflix Zuul, Spring Cloud Gateway, Kong, NGINX\n\nSummary:API Gateway simplifies client interaction, hides service complexity, and provides cross-cutting features in one place.\nHow do you manage configuration in multiple microservices?For multiple services, externalized and centralized configuration is recommended.\nApproaches:\n\nSpring Cloud Config\nCentral config server serving properties via Git/Filesystem.\nServices fetch config dynamically.\n\n\nEnvironment variables / Kubernetes ConfigMaps &amp; Secrets\nStore environment-specific configuration outside code.\n\n\nConsul / etcd\nDistributed configuration with service discovery integration.\n\n\n\nExample with Spring Cloud Config:spring:  application:    name: user-service  cloud:    config:      uri: http://config-server:8888\nExplain circuit breaker pattern (Hystrix/Resilience4j) with example.The circuit breaker pattern prevents cascading failures in microservices when one service is down or slow.\nHow it works:\n\nCircuit is closed ‚Üí calls pass normally.\nIf failures exceed threshold ‚Üí circuit opens ‚Üí further calls fail fast.\nAfter a timeout ‚Üí circuit half-open ‚Üí test a few requests before closing.\n\nExample with Resilience4j (Spring Boot):@Servicepublic class UserService &#123;    @CircuitBreaker(name = &quot;userService&quot;, fallbackMethod = &quot;fallbackGetUser&quot;)    public User getUser(Long id) &#123;        // Call external service    &#125;    public User fallbackGetUser(Long id, Throwable t) &#123;        return new User(id, &quot;Default User&quot;);    &#125;&#125;Summary:Circuit breakers improve system resilience by preventing cascading failures and allowing graceful degradation.\nCloud &amp; DevOps BasicsWhat is the difference between Docker image and Docker container?\n\n\n\nAspect\nDocker Image\nDocker Container\n\n\n\n\nDefinition\nRead-only template with application and dependencies\nRunning instance of an image\n\n\nLifecycle\nImmutable, stored in registry\nCreated, started, stopped, or destroyed\n\n\nPurpose\nUsed to build containers\nExecutes the application\n\n\nExample\nmyapp:1.0 image\ndocker run myapp:1.0 creates a container\n\n\n\n\nSummary:Image = blueprint; Container = running instance of that blueprint.\nHow does Kubernetes handle scaling and service discovery?Scaling:  \n\nKubernetes can automatically scale pods using:\nManual scaling: kubectl scale deployment myapp --replicas=5\nHorizontal Pod Autoscaler (HPA): Automatically adjusts replicas based on CPU/memory metrics.\n\n\n\nService Discovery:  \n\nPods are ephemeral; services use DNS names for discovery.\nKubernetes Service creates a stable IP and DNS name for a set of pods.  \nExample: http://user-service.default.svc.cluster.local\n\nExplain difference between Blue-Green and Rolling deployments.\n\n\n\nAspect\nBlue-Green Deployment\nRolling Deployment\n\n\n\n\nApproach\nTwo identical environments: Blue (current) &amp; Green (new)\nUpdate pods gradually with new version\n\n\nDowntime\nNear-zero downtime\nMinimal downtime during updates\n\n\nRollback\nSimple: switch traffic back to old environment\nRollback requires reverting pods\n\n\nComplexity\nRequires double resources\nResource-efficient\n\n\n\n\nHow do you monitor Spring Boot microservices in production (Actuator, ELK, Prometheus)?1. Spring Boot Actuator:  \n\nExposes metrics, health checks, and info endpoints.  \nExample: /actuator/health, /actuator/metrics\n\n2. ELK Stack (Elasticsearch, Logstash, Kibana):  \n\nCentralized logging for multiple microservices.  \nCollect logs with Logstash ‚Üí store in Elasticsearch ‚Üí visualize in Kibana.\n\n3. Prometheus &amp; Grafana:  \n\nPrometheus scrapes metrics from Actuator endpoints.  \nGrafana visualizes metrics in dashboards.  \nExample:  management:endpoints:    web:    exposure:        include: health, metrics, prometheus\n\n\nWhat is CI/CD? How have you implemented it in your projects?CI/CD stands for Continuous Integration / Continuous Deployment.\n\nContinuous Integration (CI): Automatically build and test code whenever changes are pushed.\nContinuous Deployment (CD): Automatically deploy code to staging/production after passing tests.\n\nTypical Implementation:\n\nUse GitHub Actions / Jenkins / GitLab CI to build, test, and package Spring Boot services into Docker images.\nPush Docker images to Docker Registry (Docker Hub, ECR, GCR).\nDeploy images to Kubernetes or cloud environment.\nAutomated tests and monitoring ensure quality and quick rollback if needed.\n\nExample GitHub Actions snippet:name: CI/CD Pipelineon: [push]jobs:  build:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v3      - name: Build Docker image        run: docker build -t myapp:$&#123;&#123; github.sha &#125;&#125; .      - name: Push to registry        run: docker push myapp:$&#123;&#123; github.sha &#125;&#125;\nSQL / Database KnowledgeWrite a SQL query to get department-wise average salary.SELECT     department_id,    AVG(salary) AS avg_salaryFROM employeesGROUP BY department_idORDER BY avg_salary DESC;\nExplanation:\n\nGROUP BY groups rows by department.\nAVG() calculates the average salary per department.\nORDER BY sorts the results (optional).\n\nDifference between INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL JOIN.\n\n\n\nType\nDescription\nExample Result\n\n\n\n\nINNER JOIN\nReturns only rows with matching values in both tables.\nEmployees that belong to a department.\n\n\nLEFT JOIN\nReturns all rows from the left table, even if no match exists in the right table.\nAll employees, with department info if available.\n\n\nRIGHT JOIN\nReturns all rows from the right table, even if no match exists in the left table.\nAll departments, even those with no employees.\n\n\nFULL JOIN\nReturns all rows from both tables, with NULL for missing matches.\nAll employees and departments, matched where possible.\n\n\n\n\nExample:SELECT e.name, d.name AS departmentFROM employees eLEFT JOIN departments d ON e.dept_id = d.id;\nHow do you optimize slow-running SQL queries?\nAnalyze the query plan\nUse EXPLAIN (MySQL, PostgreSQL) to view execution details.\nIdentify full table scans, missing indexes, or inefficient joins.\n\n\nCreate appropriate indexes\nAdd indexes on columns used in WHERE, JOIN, and ORDER BY clauses.\n\n\nReduce data retrieval\nUse SELECT specific_columns instead of SELECT *.\nApply filters and limits.\n\n\nOptimize joins and subqueries\nPrefer joins over nested subqueries when possible.\n\n\nUse caching\nCache frequent queries (e.g., Redis, Memcached).\n\n\nPartition large tables\nImproves performance on very large datasets.\n\n\n\nExplain ACID properties and isolation levels in databases.ACID Properties\n\n\n\n\nProperty\nDescription\n\n\n\n\nAtomicity\nAll operations in a transaction succeed or none do.\n\n\nConsistency\nDatabase remains valid before and after the transaction.\n\n\nIsolation\nTransactions do not interfere with each other.\n\n\nDurability\nOnce committed, data changes persist even after failures.\n\n\n\n\nIsolation Levels\n\n\n\n\nLevel\nDescription\nCommon Problems Prevented\n\n\n\n\nREAD UNCOMMITTED\nTransactions can read uncommitted data (dirty reads).\nNone\n\n\nREAD COMMITTED\nPrevents dirty reads, allows non-repeatable reads.\nDirty reads\n\n\nREPEATABLE READ\nPrevents dirty and non-repeatable reads, but phantom reads may occur.\nDirty, non-repeatable reads\n\n\nSERIALIZABLE\nHighest level, ensures full isolation (transactions run sequentially).\nAll\n\n\n\n\nHow do you use indexes, and what are clustered vs non-clustered indexes?What is an index?An index improves data retrieval speed by allowing the database to find rows quickly, similar to a book index.\nSyntax Example:CREATE INDEX idx_employee_name ON employees(name);Types of Indexes\n\n\n\n\nType\nDescription\nExample\n\n\n\n\nClustered Index\nPhysically sorts and stores table data based on the index key. One per table.\nPRIMARY KEY automatically creates a clustered index.\n\n\nNon-Clustered Index\nStores index separately from table data, containing pointers to the actual rows.\nUsed for frequently queried columns not part of the primary key.\n\n\n\n\nSummary:\n\nClustered = data sorted by index\nNon-clustered = separate lookup structure\n\nBest Practices:\n\nUse indexes on columns often used in filters and joins.\nAvoid over-indexing (it slows down inserts/updates).\nRegularly monitor and rebuild fragmented indexes.\n\nDSA &amp; Coding (GlobalLogic Special Focus)Write a program to reverse a linked list in Java (iterative + recursive).Iterative Approach:class Node &#123;    int data;    Node next;    Node(int data) &#123; this.data = data; &#125;&#125;public class ReverseLinkedList &#123;    public static Node reverseIterative(Node head) &#123;        Node prev = null, curr = head;        while (curr != null) &#123;            Node next = curr.next; // store next            curr.next = prev;      // reverse link            prev = curr;           // move prev forward            curr = next;           // move curr forward        &#125;        return prev; // new head    &#125;&#125;Recursive Approach:public static Node reverseRecursive(Node head) &#123;    if (head == null || head.next == null)        return head;    Node newHead = reverseRecursive(head.next);    head.next.next = head;    head.next = null;    return newHead;&#125;\nImplement a program to find the first non-repeating character in a string.Logic: Use a frequency map to track occurrences, then find the first char with count = 1.import java.util.*;public class FirstNonRepeatingChar &#123;    public static char firstNonRepeating(String s) &#123;        Map&lt;Character, Integer&gt; map = new LinkedHashMap&lt;&gt;();        for (char c : s.toCharArray())            map.put(c, map.getOrDefault(c, 0) + 1);        for (Map.Entry&lt;Character, Integer&gt; entry : map.entrySet()) &#123;            if (entry.getValue() == 1)                return entry.getKey();        &#125;        return &#x27;_&#x27;; // if no non-repeating char    &#125;    public static void main(String[] args) &#123;        System.out.println(firstNonRepeating(&quot;swiss&quot;)); // Output: w    &#125;&#125;\nWrite a Java program to find the longest common prefix among a set of strings.Approach: Compare characters of all strings until mismatch occurs.public class LongestCommonPrefix &#123;    public static String longestCommonPrefix(String[] strs) &#123;        if (strs == null || strs.length == 0) return &quot;&quot;;        String prefix = strs[0];        for (int i = 1; i &lt; strs.length; i++) &#123;            while (!strs[i].startsWith(prefix)) &#123;                prefix = prefix.substring(0, prefix.length() - 1);                if (prefix.isEmpty()) return &quot;&quot;;            &#125;        &#125;        return prefix;    &#125;    public static void main(String[] args) &#123;        String[] input = &#123;&quot;flower&quot;, &quot;flow&quot;, &quot;flight&quot;&#125;;        System.out.println(longestCommonPrefix(input)); // Output: &quot;fl&quot;    &#125;&#125;\nImplement LRU Cache in Java.Approach: Use LinkedHashMap with access order.import java.util.*;public class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123;    private final int capacity;    public LRUCache(int capacity) &#123;        super(capacity, 0.75f, true); // accessOrder = true        this.capacity = capacity;    &#125;    @Override    protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123;        return size() &gt; capacity;    &#125;    public static void main(String[] args) &#123;        LRUCache&lt;Integer, String&gt; cache = new LRUCache&lt;&gt;(3);        cache.put(1, &quot;A&quot;);        cache.put(2, &quot;B&quot;);        cache.put(3, &quot;C&quot;);        cache.get(1); // access 1        cache.put(4, &quot;D&quot;); // removes 2 (least recently used)        System.out.println(cache.keySet()); // Output: [3, 1, 4]    &#125;&#125;\nWrite a REST API to fetch last N orders of a customer (Spring Boot + JPA).@Entitypublic class Order &#123;    @Id    @GeneratedValue(strategy = GenerationType.IDENTITY)    private Long id;    private Long customerId;    private LocalDateTime orderDate;    private double amount;    // getters and setters&#125;\n@Repositorypublic interface OrderRepository extends JpaRepository&lt;Order, Long&gt; &#123;    List&lt;Order&gt; findTopNByCustomerIdOrderByOrderDateDesc(Long customerId, Pageable pageable);&#125;\n@Servicepublic class OrderService &#123;    @Autowired    private OrderRepository orderRepository;    public List&lt;Order&gt; getLastNOrders(Long customerId, int n) &#123;        Pageable pageable = PageRequest.of(0, n);        return orderRepository.findByCustomerIdOrderByOrderDateDesc(customerId, pageable);    &#125;&#125;\n@RestController@RequestMapping(&quot;/api/orders&quot;)public class OrderController &#123;    @Autowired    private OrderService orderService;    @GetMapping(&quot;/&#123;customerId&#125;/last&quot;)    public List&lt;Order&gt; getLastNOrders(            @PathVariable Long customerId,            @RequestParam(defaultValue = &quot;5&quot;) int n) &#123;        return orderService.getLastNOrders(customerId, n);    &#125;&#125;\nExample Request:GET /api/orders/101/last?n=3# Response: Returns the last 3 orders for customer 101 sorted by date (descending).\n","categories":["interview-question"],"tags":["interview-question"]},{"title":"interview question of anthrapic","url":"/2025/08/24/interview/01-anthrapic-swe/","content":"Question 1: Concurrent Web Crawler ImplementationDesign and implement a multi-threaded web crawler that efficiently crawls a specific domain and counts unique URLs\nRequirements:\n\nHandle circular references and invalid URLs\nimplement rate limiting and exponential back off\nProcess robots.txt compliance\nSupport different URL schemes and redirects\nOptimize for memory usage with large datasets\n\nOverview[Seed URLs]    |  [Frontier in-memory queue] &lt;--- [Persistent overflow queue (RocksDB/file)]    |  [Worker Pool: N threads]    |---&gt; [RobotsCache] (check)    |---&gt; [URL Normalizer]    |---&gt; [Seen: BloomFilter (mem) + RocksDB (exact)]    |---&gt; [RateLimiter per-host]    |---&gt; [Fetcher (HTTP client)] --&gt; [Redirect Handler]    |                                   |    |                                   v    |                                  [Parser (streaming)]    |                                   |    |------- enqueue new links ---------------    |  [Storage: only metadata (URL, status, timestamp)]    |  [Metrics/Logging/Monitor]\nFrontierfunction offer(url):    if not isValid(url): return    if inmemoryQueue.size &lt; MAX_INMEM: push(inmemoryQueue, url)    else persistQueue.append(url)function refillWorker():    if inmemoryQueue.isEmpty: load up to N from persistQueue\nSeen (BloomFilter + RocksDB)if not bloom.mightContain(url):    bloom.put(url)    if not rocksdb.contains(url):        rocksdb.put(url, meta)        uniqueCount++\nRobots.txtif not robotsCache.has(host):    rules = fetchRobots(host)    robotsCache.put(host, rules)if not rules.isAllowed(userAgent, path):    skipelse    rps = rules.getCrawlDelayOrDefault()    hostRateLimiter.setRate(host, 1/rps)\nRate Limiterlim = hostLimit.computeIfAbsent(host, h -&gt; new TokenBucket(defaultRate, burst))lim.acquire()\nHTTP FetcherHttpRequest req = HttpRequest.newBuilder(uri).GET().build();HttpResponse&lt;InputStream&gt; resp = client.send(req, BodyHandlers.ofInputStream());int status = resp.statusCode();if (status &gt;=200 &amp;&amp; status&lt;300) processBody(resp.body());else if (status &gt;=300 &amp;&amp; status&lt;400) handleRedirect(resp.headers().firstValue(&quot;location&quot;));else if (status==429 || status&gt;=500) retryWithBackoff();else // 4xx -&gt; record and drop\nExponential Backoff + Jittersleep = base * 2^attempt + random(0, jitter)\nfor attempt in 0..max:    try fetch()    if status ok: break    if status is retryable:    sleep = min(maxBackoff, base * 2^attempt)    sleep = random(0, sleep) // full jitter    sleepMillis(sleep)    else break\nCanonicalizationfrom urllib.parse import urlparse, urlunparse, parse_qsl, urlencodedef normalize(u):    p = urlparse(u)    scheme = p.scheme.lower()    host = p.hostname.lower()    port = &#x27;&#x27; if p.port in (80,443,None) else f&quot;:&#123;p.port&#125;&quot;    q = sorted([(k,v) for k,v in parse_qsl(p.query) if not is_tracking_param(k)])    return urlunparse((scheme, host+port, norm_path(p.path), &#x27;&#x27;, urlencode(q), &#x27;&#x27;))\nParserfor link in streamingParse(htmlStream):    abs = resolve(baseUrl, link)    if isSameDomain(abs): offer(abs)\nWorker PoolExecutorService + NIO Queue\nMonitorPrometheus metrics + Grafana dashboards\nQuestion 2: Distributed LLM Inference System DesignDesign a high-throughput system for serving large language model inference requests at scale.\nRequirements:\n\nSupport 15,000+ requests per second\nHandle multiple model variants and sizes\nImplement dynamic load balancing\nAchieve sub0100ms latency for standard requests\nSupport zero-downtime model updates\nInclude comprehensive monitoring and alerting\n\nQuestion 3: High-Performance Document Similarity SearchImplement an efficient algorithm for finding top-k most similar documents from a massive corpus\nRequirements:\n\nHandle 15M+ document corpus\nAverage document length: 1500+ words\nQuery response time: &lt; 30 ms\nSupport real-time document additions\nOptimize memory footprint\n\nQuestion 4: Distributed System Debugging ChallengeDebug a meesage queue system experiencing performance degradation under high load\nCommon Issues:\n\nRace conditions in concurrent message processing\nMemory leaks from unclosed connections\nImproper error handling causing silent failures\nInefficient connection pooling strategies\nDeadlocks in multi-threaded environment\n\nQuestion 5: Real-time Data Processing PipelineDesign a system for processing streaming data with low latency and high reliability.\nRequirements:\n\nProcess 100,000+ events per second\nMaintain exactly-once processing semantics\nSupport complex event transformations\nHandle out-of-order events\nImplement fault tolerance and recovery\n\nQuestion 6: Machine Learning Model Serving OptimizationOptimize an existing ML model serving system for better performance and resource utilization\nFocus Areas:\n\nBatch processing optimization\nGPU memory management\nModel parallelism strategies\nCaching mechanisms for predictions\nAuto-scaling based on demand\n\nQuestion 7: Database Performance TuningAnalyze and optimize a database system experiencing slow query performance.\nCommon Problems:\n\nMissing or inefficient indexes\nPoor query optimization\nLock contention issues\nInadequate connection pooling\nSuboptimal schema design\n\n","categories":["interview-question"],"tags":["interview-question"]},{"title":"Queue","url":"/2025/07/15/leetcode/00-queue/","content":"QueueQueue is a linear data structure that follows the First In First Out (FIFO) principle. Elements are added at the rear (enqueue) and removed from the front (dequeue).\nCircular QueueA circular queue is a linear data structure that follows the FIFO principle but connects the end of the queue back to the front, forming a circle. This allows for efficient use of space by reusing empty slots created by dequeued elements.\npublic class CircularQueue &#123;  private String[] items;  private int n = 0;  private int head = 0;  private int tail = 0;   public CircularQueue(int capacity) &#123;    items = new String[capacity];    n = capacity;  &#125;   public boolean enqueue(String item) &#123;    if ((tail + 1) % n == head) return false;    items[tail] = item;    tail = (tail + 1) % n;    return true;  &#125;   public String dequeue() &#123;    if (head == tail) return null;    String ret = items[head];    head = (head + 1) % n;    return ret;  &#125;&#125;","categories":["Data Structure"],"tags":["Data Structure"]},{"title":"Sort","url":"/2025/06/17/leetcode/00-sort/","content":"SortStable Sort: Stable sort is a sorting algorithm that preserves the relative order of equal elements in the sorted output.Local Sort: Local sort is a sorting algorithm that sorts elements in a small region of the list.\nBubble SortBubble sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.\nStable Sort: YesLocal Sort: Yes\nTime complexity: O(n^2)Space complexity: O(1)\nHere is the implementation of bubble sort in Python:\ndef bubble_sort(arr):    n = len(arr)    for i in range(n):        for j in range(0, n-i-1):            if arr[j] &gt; arr[j+1]:                arr[j], arr[j+1] = arr[j+1], arr[j]    return arr\nSelection SortSelection sort is another simple sorting algorithm that works by repeatedly finding the minimum element from the unsorted part of the list and putting it at the beginning. The pass through the list is repeated until the list is sorted.\nStable Sort: YesLocal Sort: Yes\nTime complexity: O(n^2)Space complexity: O(1)\nHere is the implementation of selection sort in Python:\ndef selection_sort(arr):    n = len(arr)    for i in range(n):        min_idx = i        for j in range(i+1, n):            if arr[j] &lt; arr[min_idx]:                min_idx = j        arr[i], arr[min_idx] = arr[min_idx], arr[i]    return arr\nInsertion SortInsertion sort is a simple sorting algorithm that builds the final sorted list one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort.\nStable Sort: YesLocal Sort: Yes\nTime complexity: O(n^2)Space complexity: O(1)\nHere is the implementation of insertion sort in Python:\ndef insertion_sort(arr):    n = len(arr)    for i in range(1, n):        key = arr[i]        j = i-1        while j &gt;= 0 and key &lt; arr[j]:            arr[j+1] = arr[j]            j -= 1        arr[j+1] = key    return arr\nMerge SortMerge sort is a divide and conquer algorithm that divides the list into two halves, sorts each half recursively, and then merges the two sorted halves back together.\nStable Sort: YesLocal Sort: No\nTime complexity: O(nlogn)Space complexity: O(n)\nHere is the implementation of merge sort in Python:\ndef merge_sort(arr):    if len(arr) &gt; 1:        mid = len(arr) // 2        left_half = arr[:mid]        right_half = arr[mid:]        merge_sort(left_half)        merge_sort(right_half)        i = j = k = 0        while i &lt; len(left_half) and j &lt; len(right_half):            if left_half[i] &lt; right_half[j]:                arr[k] = left_half[i]                i += 1            else:                arr[k] = right_half[j]                j += 1            k += 1        while i &lt; len(left_half):            arr[k] = left_half[i]            i += 1            k += 1        while j &lt; len(right_half):            arr[k] = right_half[j]            j += 1            k += 1    return arr\nQuick SortQuick sort is a divide and conquer algorithm that selects a pivot element and partitions the other elements into two sub-lists, according to whether they are less than or greater than the pivot. The sub-lists are then sorted recursively.\nStable Sort: NoLocal Sort: Yes\nTime complexity: O(nlogn)Space complexity: O(logn)\nHere is the implementation of quick sort in Python:\ndef quick_sort(arr):    if len(arr) &lt;= 1:        return arr    else:        pivot = arr[0]        left = []        right = []        for i in range(1, len(arr)):            if arr[i] &lt; pivot:                left.append(arr[i])            else:                right.append(arr[i])        return quick_sort(left) + [pivot] + quick_sort(right)","categories":["Algorithms"],"tags":["Algorithms"]},{"title":"Leetcode - 1. Two Sum","url":"/2025/07/15/leetcode/01-two-sum/","content":"DescriptionGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\nYou can return the answer in any order.\nExample 1:Input: nums = [2,7,11,15], target = 9Output: [0,1]Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].Example 2:Input: nums = [3,2,4], target = 6Output: [1,2]Example 3:Input: nums = [3,3], target = 6Output: [0,1]\nApproach 1: Brute Forceclass Solution &#123;    public int[] twoSum(int[] nums, int target) &#123;        for(int i=0;i&lt;nums.length;i++)&#123;            for(int j=i+1;j&lt;nums.length;j++)&#123;                if(nums[i]+nums[j]==target)&#123;                    return new int[]&#123;i,j&#125;;                &#125;            &#125;        &#125;        return new int[]&#123;&#125;;    &#125;&#125;\nApproach 2: HashMapclass Solution &#123;    static &#123;        Solution solution = new Solution();        for(int i=0;i&lt;1000;i++)&#123;            solution.twoSum(new int[]&#123;2,7,11,15&#125;, 9);        &#125;    &#125;    public int[] twoSum(int[] nums, int target) &#123;        HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;();        for(int i=0;i&lt;nums.length;i++)&#123;            if(map.containsKey(target-nums[i]))&#123;                return new int[]&#123;map.get(target-nums[i]),i&#125;;            &#125;            map.put(nums[i],i);        &#125;        return new int[]&#123;&#125;;    &#125;&#125;","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 2. Add Two Numbers","url":"/2025/07/15/leetcode/02-add-two-numbers/","content":"DescriptionYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.\nYou may assume the two numbers do not contain any leading zero, except the number 0 itself.\nExample 1:Input: l1 = [2,4,3], l2 = [5,6,4]Output: [7,0,8]Explanation: 342 + 465 = 807.\nExample 2:Input: l1 = [0], l2 = [0]Output: [0]Example 3:Input: l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9]Output: [8,9,9,9,0,0,0,1]\nConstraints:The number of nodes in each linked list is in the range [1, 100].0 &lt;= Node.val &lt;= 9It is guaranteed that the list represents a number that does not have leading zeros.\nApproach/** * Definition for singly-linked list. * public class ListNode &#123; *     int val; *     ListNode next; *     ListNode() &#123;&#125; *     ListNode(int val) &#123; this.val = val; &#125; *     ListNode(int val, ListNode next) &#123; this.val = val; this.next = next; &#125; * &#125; */class Solution &#123;    public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123;        ListNode dummy = new ListNode(0), curr = dummy;        int carry = 0;        while (l1 != null || l2 != null || carry != 0) &#123;            int a = (l1 == null) ? 0 : l1.val;            int b = (l2 == null) ? 0 : l2.val;            int s = a + b + carry;            carry = s / 10;            curr.next = new ListNode(s % 10);            curr = curr.next;            if (l1 != null) l1 = l1.next;            if (l2 != null) l2 = l2.next;        &#125;        return dummy.next;    &#125;&#125;","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 3. Longest Substring Without Repeating Characters","url":"/2025/07/16/leetcode/03-longest-substring/","content":"DescriptionGiven a string s, find the length of the longest substring without duplicate characters.\nExample 1:Input: s = &quot;abcabcbb&quot;Output: 3Explanation: The answer is &quot;abc&quot;, with the length of 3.Example 2:Input: s = &quot;bbbbb&quot;Output: 1Explanation: The answer is &quot;b&quot;, with the length of 1.Example 3:Input: s = &quot;pwwkew&quot;Output: 3Explanation: The answer is &quot;wke&quot;, with the length of 3.Notice that the answer must be a substring, ‚Äúpwke‚Äù is a subsequence and not a substring.\nConstraints:0 &lt;= s.length &lt;= 5 * 104s consists of English letters, digits, symbols and spaces.\nApproach: Sliding Windowclass Solution &#123;    public int lengthOfLongestSubstring(String s) &#123;        int n = s.length();        Set&lt;Character&gt; set = new HashSet&lt;&gt;();        int ans = 0, i = 0, j = 0;        while (i &lt; n &amp;&amp; j &lt; n) &#123;            if (!set.contains(s.charAt(j))) &#123;                set.add(s.charAt(j++));                ans = Math.max(ans, j - i);            &#125; else &#123;                set.remove(s.charAt(i++));            &#125;        &#125;        return ans;    &#125;&#125;\nclass Solution &#123;    public int lengthOfLongestSubstring(String s) &#123;        int start = 0;        int[] lastIndex = new int[128]; // assuming ASCII characters        for(int i=0; i&lt;128; i++)&#123;            lastIndex[i] = -1;  // initialize all characters&#x27; last index as -1        &#125;        int max = 0;        for(int end=0; end&lt;s.length(); end++)&#123;            char c = s.charAt(end);            if(lastIndex[c] &gt;= start) start = lastIndex[c] + 1;            lastIndex[c] = end;            if(end - start + 1 &gt; max) max = end - start + 1;        &#125;        return max;    &#125;&#125;","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 5. Median of Two Sorted Arrays","url":"/2025/07/17/leetcode/04-median-of-two-sorted-arrays/","content":"DescriptionGiven two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays.\nThe overall run time complexity should be O(log (m+n)).\nExample 1:Input: nums1 = [1,3], nums2 = [2]Output: 2.00000Explanation: merged array = [1,2,3] and median is 2.Example 2:Input: nums1 = [1,2], nums2 = [3,4]Output: 2.50000Explanation: merged array = [1,2,3,4] and median is (2 + 3) / 2 = 2.5.\nConstraints:nums1.length == mnums2.length == n0 &lt;= m &lt;= 10000 &lt;= n &lt;= 10001 &lt;= m + n &lt;= 2000-106 &lt;= nums1[i], nums2[i] &lt;= 106\nApproach: Binary Searchnums1: [0 .. i-1] | [i .. m-1]nums2: [0 .. j-1] | [j .. n-1]\nclass Solution &#123;    public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123;        if (nums1.length &gt; nums2.length) &#123;            return findMedianSortedArrays(nums2, nums1);        &#125;        int m = nums1.length;        int n = nums2.length;        int imin = 0, imax = m, halfLen = (m + n + 1) / 2;        while (imin &lt;= imax) &#123;            int i = (imin + imax) / 2;            int j = halfLen - i;            if (i &lt; imax &amp;&amp; nums1[i] &lt; nums2[j - 1]) &#123;                imin = i + 1; // i is too small            &#125; else if (i &gt; imin &amp;&amp; nums1[i - 1] &gt; nums2[j]) &#123;                imax = i - 1; // i is too big            &#125; else &#123; // i is perfect                int maxLeft = 0;                if (i == 0) &#123;                    maxLeft = nums2[j - 1];                &#125; else if (j == 0) &#123;                    maxLeft = nums1[i - 1];                &#125; else &#123;                    maxLeft = Math.max(nums1[i - 1], nums2[j - 1]);                &#125;                if ((m + n) % 2 == 1) &#123;                    return maxLeft;                &#125;                int minRight = 0;                if (i == m) &#123;                    minRight = nums2[j];                &#125; else if (j == n) &#123;                    minRight = nums1[i];                &#125; else &#123;                    minRight = Math.min(nums2[j], nums1[i]);                &#125;                return (maxLeft + minRight) / 2.0;            &#125;        &#125;        return 0.0;    &#125;&#125;\nApproach: Two Pointersclass Solution &#123;    public double findMedianSortedArrays(int[] nums1, int[] nums2) &#123;        if (nums1.length &gt; nums2.length) &#123;            return findMedianSortedArrays(nums2, nums1);        &#125;        int len1 = nums1.length;        int len2 = nums2.length;        int left = 0, right = len1;        while (left &lt;= right) &#123;            int p1 = left + (right - left) / 2;            int p2 = (len1 + len2 + 1) / 2 - p1;            int left1 = p1 == 0 ? Integer.MIN_VALUE : nums1[p1 - 1];            int right1 = p1 &lt; len1 ? nums1[p1] : Integer.MAX_VALUE;            int left2 = p2 == 0 ? Integer.MIN_VALUE : nums2[p2 - 1];            int right2 = p2 &lt; len2 ? nums2[p2] : Integer.MAX_VALUE;            if (left1 &lt;= right2 &amp;&amp; left2 &lt;= right1) &#123;                if ((len1 + len2) % 2 == 0) &#123;                    return (Math.max(left1, left2) +                            Math.min(right1, right2)) / 2.0;                &#125;                return Math.max(left1, left2);            &#125;            if (left1 &gt; right2) &#123;                right = p1 - 1;            &#125; else &#123;                left = p1 + 1;            &#125;        &#125;        return 0.0;    &#125;&#125;","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 6. Zigzag Conversion","url":"/2025/07/18/leetcode/06-zigzag-conversion/","content":"DescriptionThe string &quot;PAYPALISHIRING&quot; is written in a zigzag pattern on a given number of rows like this: (you may want to display this pattern in a fixed font for better legibility)\nP   A   H   NA P L S I I GY   I   R\nWrite the code to convert a string to a zigzag pattern.\nYou should do it in-place.\nExample 1:\nInput: s = &quot;PAYPALISHIRING&quot;, numRows = 3Output: &quot;PAHNAPLSIIGYIR&quot;\nExample 2:  \nInput: s = &quot;PAYPALISHIRING&quot;, numRows = 4Output: &quot;PINALSIGYAHRPI&quot;Explanation:P     I    NA   L S  I GY A   H RP     I\nApproachWe can solve this problem by iterating over the string and keeping track of the current row and direction. We can use two pointers, one for the current row and one for the direction. We can then iterate over the string and add each character to the appropriate row based on the current row and direction.\nclass Solution:    def convert(self, s: str, numRows: int) -&gt; str:        if numRows == 1:            return s        rows = [&#x27;&#x27;] * numRows        row, direction = 0, 1        for char in s:            rows[row] += char            if (row == 0 and direction == -1) or row == numRows - 1:                direction = -direction            row += direction        return &#x27;&#x27;.join(rows)\n","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 6. Longest Palindromic Substring","url":"/2025/07/18/leetcode/05-longest-palindromic-substring/","content":"DescriptionGiven a string s, return the longest palindromic substring in s.\nExample 1:Input: s = &quot;babad&quot;Output: &quot;bab&quot;Explanation: &quot;aba&quot; is also a valid answer.Example 2:Input: s = &quot;cbbd&quot;Output: &quot;bb&quot;Constraints:1 &lt;= s.length &lt;= 1000s consist of only digits and English letters.\nApproach: Expand Around Centerclass Solution:    def longestPalindrome(self, s: str) -&gt; str:        if not s or len(s) == 1:            return s                start, end = 0, 0        def expand(l, r):            while l &gt;= 0 and r &lt; len(s) and s[l] == s[r]:                l -= 1                r += 1            return l + 1, r - 1        for i in range(len(s)):            l1, r1 = expand(i, i)            l2, r2 = expand(i, i + 1)            if r1 - l1 &gt; end - start:                start, end = l1, r1            if r2 - l2 &gt; end - start:                start, end = l2, r2                return s[start:end + 1]\nclass Solution:    def longestPalindrome(self, s: str) -&gt; str:        string = s        longest = &#x27;&#x27;        for i in range( len( string ) * 2 ):            i = i / 2            valid = True            a = int( ( i - 0.5 ) // 1 )            b = int( ( i + 1 ) // 1 )            stringnew = i % 1 == 0 and string[ int( i ) ] or &#x27;&#x27;            length = 0            while valid:                if a &lt; 0 or b &gt;= len( string ):                    break                if string[ a ] != string[ b ]:                    valid = False                    break                stringnew = string[ a ] + stringnew + string[ b ]                a -= 1                b += 1                length += 1                            if len( stringnew ) &gt; len( longest ):                longest = stringnew        return longest__import__(&quot;atexit&quot;).register(lambda: open(&quot;display_runtime.txt&quot;, &#x27;w&#x27;).write(&#x27;0&#x27;))\nApproach: Dynamic Programmingclass Solution:    def longestPalindrome(self, s: str) -&gt; str:        n = len(s)        if n &lt; 2:            return s                dp = [[False] * n for _ in range(n)]        start, max_len = 0, 1        for j in range(1, n):            for i in range(j):                if s[i] == s[j]:                    if j - i &lt; 3:                        dp[i][j] = True                    else:                        dp[i][j] = dp[i + 1][j - 1]                                if dp[i][j] and j - i + 1 &gt; max_len:                    max_len = j - i + 1                    start = i                return s[start:start + max_len]","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 7. Reverse Integer","url":"/2025/07/19/leetcode/07-reverse-integer/","content":"DescriptionGiven a signed 32-bit integer x, return x with its digits reversed. If reversing x causes the value to go outside the signed 32-bit integer range [-2^31, 2^31 - 1], then return 0.\nExample 1:\nInput: x = 123Output: 321\nExample 2:\nInput: x = -123Output: -321\nExample 3:\nInput: x = 120Output: 21\nExample 4:\nInput: x = 0Output: 0\nConstraints:\n\n-2^31 &lt;= x &lt;= 2^31 - 1\n\nApproachclass Solution:    def reverse(self, x: int) -&gt; int:        sign = -1 if x &lt; 0 else 1        x = abs(x)        reversed_x = 0        while x &gt; 0:            reversed_x = reversed_x * 10 + x % 10            x //= 10        reversed_x *= sign        if reversed_x &lt; -2**31 or reversed_x &gt; 2**31 - 1:            return 0        return reversed_x","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 8. String to Integer (atoi)","url":"/2025/07/19/leetcode/08-string-to-integer-atoi/","content":"DescriptionImplement the myAtoi(string s) function, which converts a string to a 32-bit signed integer.\nThe algorithm for myAtoi(string s) is as follows:\n\nWhitespace: Ignore any leading whitespace (‚Äú ‚Äú).\nSignedness: Determine the sign by checking if the next character is ‚Äò-‚Äò or ‚Äò+‚Äô, assuming positivity if neither present.\nConversion: Read the integer by skipping leading zeros until a non-digit character is encountered or the end of the string is reached. If no digits were read, then the result is 0.\nRounding: If the integer is out of the 32-bit signed integer range [-231, 231 - 1], then round the integer to remain in the range. Specifically, integers less than -231 should be rounded to -231, and integers greater than 231 - 1 should be rounded to 231 - 1.\n\nReturn the integer as the final result.\nExample 1:Input: &quot;42&quot;Output: 42\nExample 2:Input: &quot;   -42&quot;Output: -42Explanation: The first non-whitespace character is &#x27;-&#x27;, which indicates an negative sign.             Then take the rest of the string after the sign. &quot;42&quot; is the integer part.\nExample 3:Input: &quot;4193 with words&quot;Output: 4193Explanation: Conversion stops at digit &#x27;3&#x27; as the next character is not a digit.\nExample 4:Input: &quot;words and 987&quot; Output: 0Explanation: The first non-whitespace character is &#x27;w&#x27;, which indicates an invalid input.\nExample 5:Input: &quot;-91283472332&quot;Output: -2147483648Explanation: The number &quot;-91283472332&quot; is out of the range of a 32-bit signed integer.             Thefore, the integer overflows and becomes negative. The function should return -2147483648.\nConstraints:  \n\n0 &lt;= s.length &lt;= 200  \ns consists of English letters (lower-case and upper-case), digits (0-9), ‚Äò ‚Äò, ‚Äò+‚Äô, ‚Äò-‚Äò, and ‚Äò.‚Äô.\n\nApproachclass Solution:    def myAtoi(self, s: str) -&gt; int:        s = s.lstrip()        if not s:            return 0                sign = 1        if s[0] in [&#x27;-&#x27;, &#x27;+&#x27;]:            if s[0] == &#x27;-&#x27;:                sign = -1            s = s[1:]                num = 0        for ch in s:            if not ch.isdigit():                break            num = num * 10 + int(ch)                num *= sign                INT_MIN, INT_MAX = -2**31, 2**31 - 1        if num &lt; INT_MIN:            return INT_MIN        if num &gt; INT_MAX:            return INT_MAX        return num","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 9. Palindrome Number","url":"/2025/07/19/leetcode/09-palindrome-number/","content":"DescriptionGiven an integer x, return true if x is a palindrome, and false otherwise.\nExample 1:Input: x = 121Output: trueExplanation: 121 reads as 121 from left to right and from right to left.Example 2:Input: x = -121Output: falseExplanation: From left to right, it reads -121. From right to left, it becomes 121-. Therefore it is not a palindrome.Example 3:Input: x = 10Output: falseExplanation: Reads 01 from right to left. Therefore it is not a palindrome.\nConstraints:-231 &lt;= x &lt;= 231 - 1\nApproachclass Solution:    def isPalindrome(self, x: int) -&gt; bool:        if x &lt; 0 or (x != 0 and x % 10 == 0):            return False                half = 0        while x &gt; half:            half = x % 10 + half * 10             x //= 10        return half == x or x == half // 10\n","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 10. Regular Expression Matching","url":"/2025/07/19/leetcode/10-regular-expression-matching/","content":"DescriptionGiven an input string s and a pattern p, implement regular expression matching with support for ‚Äò.‚Äô and ‚Äò*‚Äô where:\n‚Äò.‚Äô Matches any single character.‚Äã‚Äã‚Äã‚Äã‚Äò*‚Äô Matches zero or more of the preceding element.The matching should cover the entire input string (not partial).\nExample 1:Input: s = &quot;aa&quot;, p = &quot;a&quot;Output: falseExplanation: &quot;a&quot; does not match the entire string &quot;aa&quot;.Example 2:Input: s = &quot;aa&quot;, p = &quot;a*&quot;Output: trueExplanation: &#x27;*&#x27; means zero or more of the preceding element, &#x27;a&#x27;. Therefore, by repeating &#x27;a&#x27; once, it becomes &quot;aa&quot;.Example 3:Input: s = &quot;ab&quot;, p = &quot;.*&quot;Output: trueExplanation: &quot;.*&quot; means &quot;zero or more (*) of any character (.)&quot;.\nConstraints:1 &lt;= s.length &lt;= 201 &lt;= p.length &lt;= 20s contains only lowercase English letters.p contains only lowercase English letters, &#x27;.&#x27;, and &#x27;*&#x27;.It is guaranteed for each appearance of the character &#x27;*&#x27;, there will be a previous valid character to match.\nApproachclass Solution:    def isMatch(self, s: str, p: str) -&gt; bool:        m, n = len(s), len(p)        dp = [[False] * (n + 1) for _ in range(m + 1)]        dp[0][0] = True        for j in range(2, n + 1):            if p[j - 1] == &#x27;*&#x27;:                dp[0][j] = dp[0][j - 2]        for i in range(1, m + 1):            for j in range(1, n + 1):                if p[j - 1] == &#x27;.&#x27; or p[j - 1] == s[i - 1]:                    dp[i][j] = dp[i - 1][j - 1]                elif p[j - 1] == &#x27;*&#x27;:                    &lt;!-- match zero times --&gt;                    dp[i][j] = dp[i][j - 2]                    if p[j - 2] == &#x27;.&#x27; or p[j - 2] == s[i - 1]:                        dp[i][j] |= dp[i - 1][j]                return dp[m][n]","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 11. Container With Most Water","url":"/2025/07/19/leetcode/11-container-with-most-water/","content":"DescriptionYou are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]).\nFind two lines that together with the x-axis form a container, such that the container contains the most water.\nReturn the maximum amount of water a container can store.\nNotice that you may not slant the container.\nExample 1:Input: height = [1,8,6,2,5,4,8,3,7]Output: 49Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49.\nExample 2:Input: height = [1,1]Output: 1\nConstraints:n == height.length2 &lt;= n &lt;= 1050 &lt;= height[i] &lt;= 104\nApproachclass Solution:    def maxArea(self, height: List[int]) -&gt; int:        l,r=0, len(height)-1        area = 0        while l&lt;r:            area = max((r-l) * min(height[l], height[r]), area)            if height[l] &lt; height[r]:                l+=1            else:                r-=1                return area","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 13. Roman to Integer","url":"/2025/07/21/leetcode/13-roman-to-integer/","content":"DescriptionRoman numerals are represented by seven different symbols: I, V, X, L, C, D and M.Symbol       ValueI             1V             5X             10L             50C             100D             500M             1000For example, 2 is written as II in Roman numeral, just two ones added together. 12 is written as XII, which is simply X + II. The number 27 is written as XXVII, which is XX + V + II.\nRoman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine, which is written as IX. There are six instances where subtraction is used:\nI can be placed before V (5) and X (10) to make 4 and 9.X can be placed before L (50) and C (100) to make 40 and 90.C can be placed before D (500) and M (1000) to make 400 and 900.Given a roman numeral, convert it to an integer.\nExample 1:Input: s = &quot;III&quot;Output: 3Explanation: III = 3.Example 2:Input: s = &quot;LVIII&quot;Output: 58Explanation: L = 50, V= 5, III = 3.Example 3:Input: s = &quot;MCMXCIV&quot;Output: 1994Explanation: M = 1000, CM = 900, XC = 90 and IV = 4.\nConstraints:1 &lt;= s.length &lt;= 15s contains only the characters (&#x27;I&#x27;, &#x27;V&#x27;, &#x27;X&#x27;, &#x27;L&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;M&#x27;).It is guaranteed that s is a valid roman numeral in the range [1, 3999].\nApproachclass Solution:    def romanToInt(self, s: str) -&gt; int:        if len(s) &lt; 1 or len(s) &gt; 15: return 0        d =&#123;            &#x27;I&#x27;:1,            &#x27;IV&#x27;:4,            &#x27;V&#x27;:5,            &#x27;IX&#x27;:9,            &#x27;X&#x27;:10,            &#x27;XL&#x27;:40,            &#x27;L&#x27;:50,            &#x27;XC&#x27;:90,            &#x27;C&#x27;:100,            &#x27;CD&#x27;:400,            &#x27;D&#x27;:500,            &#x27;CM&#x27;:900,            &#x27;M&#x27;:1000,        &#125;        i=0        result=0        while(i&lt;len(s)):            c=s[i]            if c == &#x27;I&#x27;:                if i+1 &gt;= len(s):                    result +=1                    break                else:                    if s[i+1] == &#x27;V&#x27;:                        result += d[&#x27;IV&#x27;]                        i+=2                    elif s[i+1] == &#x27;X&#x27;:                        result += d[&#x27;IX&#x27;]                        i+=2                    else:                        result +=d[&#x27;I&#x27;]                        i+=1            elif c == &#x27;X&#x27;:                if i+1 &gt;= len(s):                    result +=10                    break                else:                    if s[i+1] == &#x27;L&#x27;:                        result += d[&#x27;XL&#x27;]                        i+=2                    elif s[i+1] == &#x27;C&#x27;:                        result += d[&#x27;XC&#x27;]                        i+=2                    else:                        result +=d[&#x27;X&#x27;]                        i+=1                                    elif c == &#x27;C&#x27;:                if i+1 &gt;= len(s):                    result +=100                    break                else:                    if s[i+1] == &#x27;D&#x27;:                        result += d[&#x27;CD&#x27;]                        i+=2                    elif s[i+1] == &#x27;M&#x27;:                        result += d[&#x27;CM&#x27;]                        i+=2                    else:                        result +=d[&#x27;C&#x27;]                        i+=1                                       else:                result +=d[c]                i+=1        return result\nclass Solution:    def romanToInt(self, s: str) -&gt; int:        roman = &#123;            &#x27;I&#x27;: 1,            &#x27;V&#x27;: 5,            &#x27;X&#x27;: 10,            &#x27;L&#x27;: 50,            &#x27;C&#x27;: 100,            &#x27;D&#x27;: 500,            &#x27;M&#x27;: 1000        &#125;        total = 0                for i in range(len(s)):            if i + 1 &lt; len(s) and roman[s[i]] &lt; roman[s[i + 1]]:                total -= roman[s[i]]            else:                total += roman[s[i]]        return total","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 12. Integer to Roman","url":"/2025/07/19/leetcode/12-integer-to-roman/","content":"DescriptionSeven different symbols represent Roman numerals with the following values:\nSymbol    ValueI    1V    5X    10L    50C    100D    500M    1000Roman numerals are formed by appending the conversions of decimal place values from highest to lowest. Converting a decimal place value into a Roman numeral has the following rules:\n\nIf the value does not start with 4 or 9, select the symbol of the maximal value that can be subtracted from the input, append that symbol to the result, subtract its value, and convert the remainder to a Roman numeral.\nIf the value starts with 4 or 9 use the subtractive form representing one symbol subtracted from the following symbol, for example, 4 is 1 (I) less than 5 (V): IV and 9 is 1 (I) less than 10 (X): IX. Only the following subtractive forms are used: 4 (IV), 9 (IX), 40 (XL), 90 (XC), 400 (CD) and 900 (CM).\nOnly powers of 10 (I, X, C, M) can be appended consecutively at most 3 times to represent multiples of 10. You cannot append 5 (V), 50 (L), or 500 (D) multiple times. If you need to append a symbol 4 times use the subtractive form.\n\nGiven an integer, convert it to a Roman numeral.Example 1:Input: num = 3749Output: &quot;MMMDCCXLIX&quot;Explanation:3000 = MMM as 1000 (M) + 1000 (M) + 1000 (M) 700 = DCC as 500 (D) + 100 (C) + 100 (C)  40 = XL as 10 (X) less of 50 (L)   9 = IX as 1 (I) less of 10 (X)Note: 49 is not 1 (I) less of 50 (L) because the conversion is based on decimal placesExample 2:Input: num = 58Output: &quot;LVIII&quot;Explanation:50 = L8 = VIIIExample 3:Input: num = 1994Output: &quot;MCMXCIV&quot;Explanation:1000 = M 900 = CM  90 = XC   4 = IV\nConstraints:1 &lt;= num &lt;= 3999\nApproachclass Solution:    def intToRoman(self, num: int) -&gt; str:        if num &gt; 3999 or num &lt; 1:            return &#x27;&#x27;                arr = [&#x27;I&#x27;,&#x27;V&#x27;,&#x27;X&#x27;,&#x27;L&#x27;,&#x27;C&#x27;,&#x27;D&#x27;,&#x27;M&#x27;]        result = []        value = num         i = 0        while value &gt; 0:            d = value % 10            value //= 10            if d &lt;= 3:                result.extend([arr[i]]*d)            elif d ==4:                result.extend([arr[i + 1], arr[i]])            elif d &lt; 9:                result.extend([arr[i]] * (d - 5))                result.append(arr[i + 1])            elif d == 9:                result.extend([arr[i + 2], arr[i]])            i += 2                return &#x27;&#x27;.join(result[::-1])\nclass Solution:    def intToRoman(self, num: int) -&gt; str:        roman_mapping =&#123;            1000: &quot;M&quot;,            900: &quot;CM&quot;,            500: &quot;D&quot;,            400: &quot;CD&quot;,            100: &quot;C&quot;,            90: &quot;XC&quot;,            50: &quot;L&quot;,            40: &quot;XL&quot;,            10: &quot;X&quot;,            9: &quot;IX&quot;,            5: &quot;V&quot;,            4: &quot;IV&quot;,            1: &quot;I&quot;        &#125;        roman_numeral = &quot;&quot;        for value, symbol in roman_mapping.items():            while num &gt;= value:                roman_numeral += symbol                num -= value        return roman_numeral","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 14. Longest Common Prefix","url":"/2025/07/21/leetcode/14-longest-common-prefix/","content":"DescriptionWrite a function to find the longest common prefix string amongst an array of strings.\nIf there is no common prefix, return an empty string ‚Äú‚Äù.\nExample 1:Input: strs = [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]Output: &quot;fl&quot;\nExample 2:Input: strs = [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]Output: &quot;&quot;Explanation: There is no common prefix among the input strings.\nConstraints:1 &lt;= strs.length &lt;= 2000 &lt;= strs[i].length &lt;= 200strs[i] consists of only lowercase English letters if it is non-empty.\nApproachclass Solution:    def longestCommonPrefix(self, strs: List[str]) -&gt; str:        if not strs:            return &quot;&quot;                if len(strs)==1:            return strs[0]        s1 = strs[0]        result = &quot;&quot;        endFlag = False        for inner in range(len(s1) + 1):            sub = s1[0:inner]            for out in range(1, len(strs)):                if strs[out].startswith(sub):                    if len(result) &lt; len(sub) and out == len(strs)-1:                        result = sub                else:                    endFlag = True                    break            if endFlag:                break        return result\nclass Solution:    def longestCommonPrefix(self, v: List[str]) -&gt; str:        ans=&quot;&quot;        v=sorted(v)        first=v[0]        last=v[-1]        for i in range(min(len(first),len(last))):            if(first[i]!=last[i]):                return ans            ans+=first[i]        return ans ","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 15. 3Sum","url":"/2025/07/22/leetcode/15-3sum/","content":"DescriptionGiven an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.\nNotice that the solution set must not contain duplicate triplets.\nExample 1:Input: nums = [-1,0,1,2,-1,-4]Output: [[-1,-1,2],[-1,0,1]]Explanation: nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0.nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0.nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0.The distinct triplets are [-1,0,1] and [-1,-1,2].Notice that the order of the output and the order of the triplets does not matter.`\nExample 2:Input: nums = [0,1,1]Output: []Explanation: The only possible triplet does not sum up to 0.Example 3:Input: nums = [0,0,0]Output: [[0,0,0]]Explanation: The only possible triplet sums up to 0.\nConstraints:3 &lt;= nums.length &lt;= 3000-105 &lt;= nums[i] &lt;= 105\nApproach__import__(&quot;atexit&quot;).register(lambda: open(&quot;display_runtime.txt&quot;, &quot;w&quot;).write(&quot;0&quot;))class Solution:    def threeSum(self, nums: List[int]) -&gt; List[List[int]]:        nums.sort()        n = len(nums)        result = []        for i in range(n - 2):            # Skip duplicate values for i            if i &gt; 0 and nums[i] == nums[i - 1]:                continue            left, right = i + 1, n - 1            while left &lt; right:                total = nums[i] + nums[left] + nums[right]                if total == 0:                    result.append([nums[i], nums[left], nums[right]])                    # Skip duplicates for left and right                    left += 1                    right -= 1                    while left &lt; right and nums[left] == nums[left - 1]:                        left += 1                    while left &lt; right and nums[right] == nums[right + 1]:                        right -= 1                elif total &lt; 0:                    left += 1                else:                    right -= 1        return result","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 16. 3Sum Closest","url":"/2025/07/22/leetcode/16-3sum-closest/","content":"DescriptionGiven an integer array nums of length n and an integer target, find three integers in nums such that the sum is closest to target.\nReturn the sum of the three integers.\nYou may assume that each input would have exactly one solution.\nExample 1:Input: nums = [-1,2,1,-4], target = 1Output: 2Explanation: The sum that is closest to the target is 2. (-1 + 2 + 1 = 2).\nExample 2:Input: nums = [0,0,0], target = 1Output: 0Explanation: The sum that is closest to the target is 0. (0 + 0 + 0 = 0).\nConstraints:3 &lt;= nums.length &lt;= 500-1000 &lt;= nums[i] &lt;= 1000-104 &lt;= target &lt;= 104\nApproachclass Solution:    def threeSumClosest(self, nums: List[int], target: int) -&gt; int:        nums.sort()        closest_sum = nums[0] + nums[1] + nums[2]        for i in range(len(nums) - 2):            left = i + 1            right = len(nums) - 1            while left &lt; right:                curr_sum = nums[i] + nums[left] + nums[right]                # update if closer                if abs(target - curr_sum) &lt; abs(target - closest_sum):                    closest_sum = curr_sum                # move pointers                if curr_sum &lt; target:                    left += 1                elif curr_sum &gt; target:                    right -= 1                else:                    return curr_sum  # perfect match                return closest_sum__import__(&quot;atexit&quot;).register(lambda: open(&quot;display_runtime.txt&quot;, &quot;w&quot;).write(&quot;0&quot;))","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 17. Letter Combinations of a Phone Number","url":"/2025/07/23/leetcode/17-letter-combinations/","content":"DescriptionGiven a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order.\nA mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.\nExample 1:Input: digits = &quot;23&quot;Output: [&quot;ad&quot;,&quot;ae&quot;,&quot;af&quot;,&quot;bd&quot;,&quot;be&quot;,&quot;bf&quot;,&quot;cd&quot;,&quot;ce&quot;,&quot;cf&quot;]\nExample 2:Input: digits = &quot;2&quot;Output: [&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]\nConstraints:1 &lt;= digits.length &lt;= 4digits[i] is a digit in the range [&#x27;2&#x27;, &#x27;9&#x27;].\nApproachfrom typing import Listclass Solution:    digits_map = &#123;        &quot;2&quot;: &quot;abc&quot;,        &quot;3&quot;: &quot;def&quot;,        &quot;4&quot;: &quot;ghi&quot;,        &quot;5&quot;: &quot;jkl&quot;,        &quot;6&quot;: &quot;mno&quot;,        &quot;7&quot;: &quot;pqrs&quot;,        &quot;8&quot;: &quot;tuv&quot;,        &quot;9&quot;: &quot;wxyz&quot;,    &#125;    def letterCombinations(self, digits: str) -&gt; List[str]:        if not digits:            return []        result = [&quot;&quot;]        for digit in digits:            temp = []            for prefix in result:                for ch in self.digits_map[digit]:                    temp.append(prefix + ch)            result = temp        return result","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 18. 4Sum","url":"/2025/07/23/leetcode/18-4sum/","content":"DescriptionGiven an array nums of n integers, return an array of all the unique quadruplets [nums[a], nums[b], nums[c], nums[d]] such that:\n\n0 &lt;= a, b, c, d &lt; n\na, b, c, and d are distinct.\nnums[a] + nums[b] + nums[c] + nums[d] == target\n\nYou may return the answer in any order.\nExample 1:Input: nums = [1,0,-1,0,-2,2], target = 0Output: [[-2,-1,1,2],[-2,0,0,2],[-1,0,0,1]]Example 2:Input: nums = [2,2,2,2,2], target = 8Output: [[2,2,2,2]]\nConstraints:1 &lt;= nums.length &lt;= 200-109 &lt;= nums[i] &lt;= 109-109 &lt;= target &lt;= 109\nApproachclass Solution:    def fourSum(self, nums: List[int], target: int) -&gt; List[List[int]]:        nums.sort()        n = len(nums)        res = []        for i in range(n - 3):            if i and nums[i] == nums[i-1]:                continue                        for j in range(i + 1, n - 2):                if j &gt; n + 1 and nums[j] == nums[j - 1]:                    continue                                l = j + 1                r = n - 1                while l &lt; r:                    s = nums[i] + nums[j] + nums[l] + nums[r]                    if s &lt; target:                        l += 1                    elif s &gt; target:                        r -= 1                    else:                        if [nums[i], nums[j], nums[l], nums[r]] not in res:                            res.append([nums[i], nums[j], nums[l], nums[r]])                        l += 1                        r -= 1                        while l &lt; r and nums[l] == nums[l - 1]: l += 1                        while l &lt; r and nums[r] == nums[r + 1]: r -= 1        return res ","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 205. Isomorphic Strings","url":"/2025/10/06/leetcode/205-isomorphic-strings/","content":"DescriptionGiven two strings s and t, determine if they are isomorphic.\nTwo strings s and t are isomorphic if the characters in s can be replaced to get t.\nAll occurrences of a character must be replaced with another character while preserving the order of characters. No two characters may map to the same character, but a character may map to itself.\nApproachclass Solution:    def isIsomorphic(self, s: str, t: str) -&gt; bool:        char_index_s = &#123;&#125;        char_index_t = &#123;&#125;        for i in range(len(s)):            if s[i] not in char_index_s:                char_index_s[s[i]] = i            if t[i] not in char_index_t:                char_index_t[t[i]] = i                        if char_index_s[s[i]] != char_index_t[t[i]]:                return False        return True\nclass Solution:    def isIsomorphic(self, s: str, t: str) -&gt; bool:        if len(s) != len(t):            return False                n = [1]*len(s)        m = [1]*len(t)        for i in range(len(s)):            for j in range(i-1,-1,-1):                f = False                if s[j] == s[i]:                    n[i] = n[j]+1                    f = True                if t[j] == t[i]:                    m[i] = m[j]+1                    f = True                if f:                    break        flag = True        for k in range(len(s)):            if n[k] != m[k]:                flag = False        return flag","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 392. Is Subsequence","url":"/2025/10/06/leetcode/392-is-subsequence/","content":"DescriptionGiven two strings s and t, return true if s is a subsequence of t, or false otherwise.A subsequence of a string is a new string that is formed from the original string by deleting some (can be none) of the characters without disturbing the relative positions of the remaining characters. (i.e., ‚Äúace‚Äù is a subsequence of ‚Äúabcde‚Äù while ‚Äúaec‚Äù is not).\nExample 1:\nInput: s = &quot;abc&quot;, t = &quot;ahbgdc&quot;Output: true\nExample 2:\nInput: s = &quot;axc&quot;, t = &quot;ahbgdc&quot;Output: false\nConstraints:\n1 &lt;= s.length &lt;= 1001 &lt;= t.length &lt;= 10^4s and t consist only of lowercase English letters.\nApproachclass Solution:    def isSubsequence(self, s: str, t: str) -&gt; bool:        i=j=0        while i&lt;len(s) and j&lt;len(t):            if s[i]==t[j]:                i+=1            j+=1        return i==len(s)\n","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Leetcode - 812. Largest Triangle Area","url":"/2025/07/15/leetcode/812-largest-triangle-area/","content":"DescriptionGiven an array of points on the X-Y plane points where points[i] = [xi, yi], return the area of the largest triangle that can be formed by any three different points. Answers within 10-5 of the actual answer will be accepted.\nExample 1:\nInput: points = [[0,0],[0,1],[1,0],[0,2],[2,0]]Output: 2.00000Explanation: The five points are shown in the above figure. The red triangle is the largest.\nConstraints:3 &lt;= points.length &lt;= 50-50 &lt;= xi, yi &lt;= 50All the given points are unique.\nAnserformula: $|x_1(y_2-y_3) + x_2(y_3-y_1) + x_3(y_1-y_2)|/2$\nwhere $x_1, x_2, x_3$ are the x-coordinates of the three points, and $y_1, y_2, y_3$ are the y-coordinates of the three points.\nApproach 1Ôºö bruteForceclass Solution &#123;    public double largestTriangleArea(int[][] points) &#123;        int n = points.length;        double maxArea = 0.0;        int[] x = new int[n];        int[] y = new int[n];        for (int i = 0; i &lt; n; i++) &#123;            x[i] = points[i][0];            y[i] = points[i][1];        &#125;        for (int i = 0; i &lt; n; i++) &#123;            for (int j = i + 1; j &lt; n; j++) &#123;                for (int k = j + 1; k &lt; n; k++) &#123;                    double area = Math.abs(                        x[i] * (y[j] - y[k]) +                        x[j] * (y[k] - y[i]) +                        x[k] * (y[i] - y[j])                    ) * 0.5;                    if (area &gt; maxArea) &#123;                        maxArea = area;                    &#125;                &#125;            &#125;        &#125;        return maxArea;    &#125;&#125;\nApproach 2Ôºö convexHullimport java.util.*;class Solution &#123;    public double largestTriangleArea(int[][] points) &#123;        if (points == null || points.length &lt; 3) &#123;            return 0;        &#125;        List&lt;int[]&gt; hull = convexHull(points);        int h = hull.size();        if (h &lt; 3) return 0.0;        double maxArea = 0.0;        for (int i = 0; i &lt; h; i++) &#123;            for (int j = i + 1; j &lt; h; j++) &#123;                int k = (j + 1) % h;                while (true) &#123;                    int nextK = (k + 1) % h;                    double curArea = area(hull.get(i), hull.get(j), hull.get(k));                    double nextArea = area(hull.get(i), hull.get(j), hull.get(nextK));                    if (nextArea &gt; curArea) &#123;                        k = nextK;                    &#125; else &#123;                        break;                    &#125;                &#125;                maxArea = Math.max(maxArea, area(hull.get(i), hull.get(j), hull.get(k)));            &#125;        &#125;        return maxArea;    &#125;    private double area(int[] p1, int[] p2, int[] p3) &#123;        return 0.5 * Math.abs(            p1[0] * (p2[1] - p3[1]) +            p2[0] * (p3[1] - p1[1]) +            p3[0] * (p1[1] - p2[1])        );    &#125;    private List&lt;int[]&gt; convexHull(int[][] points) &#123;        Arrays.sort(points, (a, b) -&gt; a[0] == b[0] ? a[1] - b[1] : a[0] - b[0]);        List&lt;int[]&gt; lower = new ArrayList&lt;&gt;();        for (int[] p : points) &#123;            while (lower.size() &gt;= 2 &amp;&amp; cross(lower.get(lower.size()-2), lower.get(lower.size()-1), p) &lt;= 0) &#123;                lower.remove(lower.size()-1);            &#125;            lower.add(p);        &#125;        List&lt;int[]&gt; upper = new ArrayList&lt;&gt;();        for (int i = points.length - 1; i &gt;= 0; i--) &#123;            int[] p = points[i];            while (upper.size() &gt;= 2 &amp;&amp; cross(upper.get(upper.size()-2), upper.get(upper.size()-1), p) &lt;= 0) &#123;                upper.remove(upper.size()-1);            &#125;            upper.add(p);        &#125;        lower.remove(lower.size()-1);        upper.remove(upper.size()-1);        lower.addAll(upper);        return lower;    &#125;    private long cross(int[] o, int[] a, int[] b) &#123;        return (long)(a[0] - o[0]) * (b[1] - o[1]) - (long)(a[1] - o[1]) * (b[0] - o[0]);    &#125;&#125;","categories":["Leetcode"],"tags":["Leetcode"]},{"title":"Product Management Introduction","url":"/2025/07/15/pmp/01-introduction/","content":"","categories":["pmp"],"tags":["pmp"]},{"title":"Python - Basic Syntax Cheat Sheet","url":"/2025/06/12/python/01-grammar/","content":"Python Basic Syntax Cheat SheetA concise reference for Python beginners and developers coming from languages like JavaScript, Java, or C++.\n1. Variables &amp; Data Typesx = 10              # inty = 3.14            # floatname = &quot;Alice&quot;      # stringis_valid = True     # booleanitems = [1, 2, 3]   # listdata = &#123;&quot;a&quot;: 1&#125;     # dictnums = (1, 2, 3)    # tupleunique = &#123;1, 2, 3&#125;  # set\nType Checkingtype(x)        # &lt;class &#x27;int&#x27;&gt;isinstance(x, int)  # True\n2. Control FlowIf / Elif / Elseif x &gt; 10:    print(&quot;Large&quot;)elif x == 10:    print(&quot;Equal&quot;)else:    print(&quot;Small&quot;)\nTernary Expressionmsg = &quot;even&quot; if x % 2 == 0 else &quot;odd&quot;\n3. LoopsFor Loopfor i in range(5):       # 0,1,2,3,4    print(i)for item in items:        # iterate over list    print(item)\nWhile Loopwhile x &gt; 0:    print(x)    x -= 1\nBreak / Continuefor i in range(5):    if i == 2:        continue    if i == 4:        break    print(i)\n4. Functionsdef greet(name):    return f&quot;Hello, &#123;name&#125;!&quot;print(greet(&quot;Wen&quot;))\nDefault &amp; Keyword Argumentsdef power(base, exp=2):    return base ** expprint(power(3))       # 9print(power(exp=3, base=2))  # 8Lambda (Anonymous Function)\nsquare = lambda x: x ** 2print(square(4))  # 16\n5. Collections (List / Dict / Set / Tuple)Listnums = [1, 2, 3]nums.append(4)nums.extend([5, 6])nums.insert(1, 100)nums.pop()nums.remove(100)print(nums[::-1])     # reverse\nDictionaryperson = &#123;&quot;name&quot;: &quot;Alice&quot;, &quot;age&quot;: 25&#125;print(person[&quot;name&quot;])person[&quot;city&quot;] = &quot;Sydney&quot;for key, value in person.items():    print(key, value)\nSeta = &#123;1, 2, 3&#125;b = &#123;3, 4, 5&#125;print(a | b)  # unionprint(a &amp; b)  # intersection\nTuplepoint = (10, 20)x, y = point  # unpacking\n6. List Comprehensionssquares = [x**2 for x in range(5)]even = [x for x in range(10) if x % 2 == 0]\n7. String Operationss = &quot;Python&quot;print(s.lower())print(s.upper())print(s.replace(&quot;Py&quot;, &quot;My&quot;))print(s[0:3])   # slicingprint(&quot;Hello &quot; + s)print(f&quot;Welcome to &#123;s&#125;!&quot;)\n8. Classes &amp; Objectsclass Person:    def __init__(self, name, age):        self.name = name        self.age = age    def greet(self):        return f&quot;Hi, I&#x27;m &#123;self.name&#125;!&quot;p = Person(&quot;Alice&quot;, 25)print(p.greet())\n9. File I/Owith open(&quot;data.txt&quot;, &quot;w&quot;) as f:    f.write(&quot;Hello, file!&quot;)with open(&quot;data.txt&quot;, &quot;r&quot;) as f:    content = f.read()    print(content)\n10. Common Built-in Functions\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nlen()\nlength\nlen([1,2,3]) ‚Üí 3\n\n\nsum()\nsum of elements\nsum([1,2,3]) ‚Üí 6\n\n\nmax(), min()\nlargest/smallest\nmax([1,5,3]) ‚Üí 5\n\n\nsorted()\nreturn sorted list\nsorted([3,1,2]) ‚Üí [1,2,3]\n\n\nenumerate()\nget index + value\nfor i,v in enumerate(arr)\n\n\nzip()\ncombine iterables\nzip([1,2], [&#39;a&#39;,&#39;b&#39;]) ‚Üí [(1,&#39;a&#39;), (2,&#39;b&#39;)]\n\n\nmap()\napply function\nmap(str, [1,2,3]) ‚Üí [&#39;1&#39;,&#39;2&#39;,&#39;3&#39;]\n\n\nfilter()\nfilter items\nfilter(lambda x: x&gt;0, nums)\n\n\n\n\n11. Error Handlingtry:    x = 10 / 0except ZeroDivisionError as e:    print(&quot;Cannot divide by zero!&quot;)finally:    print(&quot;Done.&quot;)\n12. Imports &amp; Modulesimport mathfrom datetime import datetimeprint(math.sqrt(16))print(datetime.now())\n13. Useful One-liners# Swap valuesa, b = b, a# Read inputname = input(&quot;Enter name: &quot;)# Check membershipif &quot;x&quot; in [1, &quot;x&quot;, 3]:    print(&quot;Found!&quot;)# Conditional expressionstatus = &quot;OK&quot; if success else &quot;FAILED&quot;\n14. Comments &amp; DocstringsSingle-line comment&quot;&quot;&quot;Multi-line comment or docstringUse triple quotes for documentation.&quot;&quot;&quot;\n","categories":["Python"],"tags":["Python"]},{"title":"Python - Basic Usage of Data Analysis","url":"/2025/10/31/python/02-basic-usage-data-analysis/","content":"Basic Usage of Data AnalysisGetting Started with JupyterLabJupyterLab is a web-based development environment supporting numerous programming languages, including, of course, Python.\njupyterlab\nScalar Types in PythonBasic Operations on Data Framesdoc\npre-requisiteimport numpy as npimport pandas as pdpd.set_option(&quot;display.notebook_repr_html&quot;, False)  # disable &quot;rich&quot; output\nAggregatingnp.random.seed(123)d = pd.DataFrame(dict(    u = np.round(np.random.rand(5), 2),    v = np.round(np.random.randn(5), 2),    w = [&quot;spam&quot;, &quot;bacon&quot;, &quot;spam&quot;, &quot;eggs&quot;, &quot;sausage&quot;]), index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;]) # create a sample data framednp.quantile(d.loc[:, &quot;u&quot;], [0, 0.5, 1]) # calculate quantiles of unp.quantile(d.loc[:, &quot;u&quot;], [0, 0.5, 1], interpolation=&quot;nearest&quot;) # calculate quantiles of u with nearest interpolationnp.quantile(d.loc[:, [&quot;u&quot;, &quot;v&quot;]], [0, 0.5, 1], axis=0) # calculate quantiles of u and vnp.mean(d.loc[:, [&quot;u&quot;, &quot;v&quot;]], axis=0) # calculate mean of u and vd.loc[:, [&quot;u&quot;, &quot;v&quot;]].mean(numeric_only=True) # calculate mean of u and v without considering non-numeric columnsd.describe()\nTransformingnp.exp(d.loc[:, &quot;u&quot;]) # apply exponential function to unp.exp(d.loc[:, [&quot;u&quot;, &quot;v&quot;]])u = d.loc[:, &quot;u&quot;](u-np.mean(u))/np.std(u) # standardize ud.loc[:, &quot;u&quot;] &gt; d.loc[:, &quot;v&quot;]uv = d.loc[:, [&quot;u&quot;, &quot;v&quot;]].values # convert to numpy arrayuv = (uv-np.mean(uv, axis=0))/np.std(uv, axis=0) # standardize u and vuv\nFilteringSeriesnp.random.seed(123)b = pd.Series(np.round(np.random.uniform(0,1,10),2))b.index =  np.random.permutation(np.r_[0:10])bc = b.copy()c.index =  list(&quot;abcdefghij&quot;)c\nDataFramenp.random.seed(123)d = pd.DataFrame(dict(    u = np.round(np.random.rand(5), 2),    v = np.round(np.random.randn(5), 2),    w = [&quot;spam&quot;, &quot;bacon&quot;, &quot;spam&quot;, &quot;eggs&quot;, &quot;sausage&quot;],    x = [True, False, True, False, True]), index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;])d.loc[d.loc[:, &quot;u&quot;] &gt; 0.5, &quot;u&quot;:&quot;w&quot;]\nAdding Rows and Columnsd.loc[:, &quot;y&quot;] = d.loc[:, &quot;u&quot;]**2d\nRandom Samplingbody = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/nhanes_adult_female_bmx_2020.csv&quot;,    comment=&quot;#&quot;)body.sample(5, random_state=123)  # 5 random rows without replacementx = body.head(10)  # this is just an examplexnp.random.seed(123)  # reproducibility mattersidx = np.random.permutation(x.shape[0])idxk = int(x.shape[0]*0.8) # 80% of the datax.iloc[idx[:k], :]  # 80% of the data\nHierarchical Indicesnp.random.seed(123)d = pd.DataFrame(dict(    year = sorted([2023, 2024, 2025]*4),    quarter = [&quot;Q1&quot;, &quot;Q2&quot;, &quot;Q3&quot;, &quot;Q4&quot;]*3,    data = np.round(np.random.rand(12), 2))).set_index([&quot;year&quot;, &quot;quarter&quot;])dd.loc[2023, :]d.loc[[(2023, &quot;Q1&quot;), (2024, &quot;Q3&quot;)], :]\nReshaping and Fusing Datasetsimport numpy as npimport pandas as pdpd.set_option(&quot;display.notebook_repr_html&quot;, False)  # disable &quot;rich&quot; outputair = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/air_quality_2018_means.csv&quot;,    comment=&quot;#&quot;)air = (    air.    loc[air.param_id.isin([&quot;BPM2.5&quot;, &quot;NO2&quot;, &quot;CO&quot;]), :].    reset_index(drop=True))air\nSortingair.sort_values(&quot;value&quot;, ascending=False)air.sort_values([&quot;param_id&quot;, &quot;value&quot;], ascending=[True, False])\nStacking and Unstackingair_wide = air.set_index([&quot;sp_name&quot;, &quot;param_id&quot;]).unstack().loc[:, &quot;value&quot;]air_wide.T.rename_axis(index=&quot;location&quot;, columns=&quot;param&quot;).stack().rename(&quot;value&quot;).reset_index()\nSet-Theoretic OperationsA = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/some_birth_dates1.csv&quot;,    comment=&quot;#&quot;)AB = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/some_birth_dates2.csv&quot;,    comment=&quot;#&quot;)BA.loc[A.Name.isin(B.Name), :]A.loc[~A.Name.isin(B.Name), :]pd.concat((A, B.loc[~B.Name.isin(A.Name), :]))\nJoining (Merging)param = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/air_quality_2018_param.csv&quot;,    comment=&quot;#&quot;)param.rename(dict(param_std_unit_of_measure=&quot;unit&quot;), axis=1)A = pd.DataFrame(&#123;    &quot;x&quot;: [&quot;a0&quot;, &quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;],    &quot;y&quot;: [&quot;b0&quot;, &quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;]&#125;)AB = pd.DataFrame(&#123;    &quot;x&quot;: [&quot;a0&quot;, &quot;a2&quot;, &quot;a2&quot;, &quot;a4&quot;],    &quot;z&quot;: [&quot;c0&quot;, &quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;]&#125;)Bpd.merge(A, B, on=&quot;x&quot;)pd.merge(A, B, how=&quot;left&quot;, on=&quot;x&quot;)pd.merge(A, B, how=&quot;right&quot;, on=&quot;x&quot;)pd.merge(A, B, how=&quot;outer&quot;, on=&quot;x&quot;)\nObservation Groupingimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snspd.set_option(&quot;display.notebook_repr_html&quot;, False)  # disable &quot;rich&quot; outputplt.style.use(&quot;seaborn&quot;)trees = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/urban_forest_subset2.csv&quot;,    comment=&quot;#&quot;)trees = trees.rename(&#123;    &quot;Common Name&quot;: &quot;Species&quot;,    &quot;Diameter Breast Height&quot;: &quot;Diameter&quot;,    &quot;Year Planted&quot;: &quot;Year&quot;,    &quot;Located In&quot;: &quot;Location&quot;,    &quot;Age Description&quot;: &quot;Age&quot;,&#125;, axis=1).loc[:, [&quot;Species&quot;, &quot;Location&quot;, &quot;Age&quot;, &quot;Diameter&quot;, &quot;Year&quot;]]trees.head()\nManual Splitting Into Subgroupsnp.split(np.arange(10)*10, [3, 7])trees_srt = trees.sort_values(&quot;Location&quot;, kind=&quot;stable&quot;)levels, where = np.unique(trees_srt.loc[:, &quot;Location&quot;], return_index=True)levels, wheretrees_grp = np.split(trees_srt, where[1:])\nfor i in range(len(levels)):    print(f&quot;level=&#x27;&#123;levels[i]&#125;&#x27;; preview:&quot;)    print(trees_grp[i].iloc[ [0, -1], : ])    print(&quot;&quot;)trees_agg = [    dict(        level=t.loc[:, &quot;Location&quot;].iloc[0],        diam_mean=np.mean(t.loc[:, &quot;Diameter&quot;]),        year_mean=np.mean(t.loc[:, &quot;Year&quot;])    )    for t in trees_grp]trees_agg\nThe groupby Methodstype(trees.groupby(&quot;Location&quot;))type(trees.groupby(&quot;Location&quot;)[&quot;Diameter&quot;])  # or (...).Diametertrees.groupby(&quot;Location&quot;).size()trees.groupby([&quot;Species&quot;, &quot;Location&quot;]).size().rename(&quot;Counts&quot;).reset_index()\nAggregating Data in Groupstrees.groupby(&quot;Location&quot;).mean(numeric_only=True).reset_index()trees.groupby(&quot;Location&quot;)[&quot;Diameter&quot;].describe().reset_index()trees.groupby(&quot;Age&quot;)[[&quot;Diameter&quot;, &quot;Year&quot;]].aggregate([np.mean, np.median]).reset_index()\n(trees.loc[:, [&quot;Age&quot;, &quot;Diameter&quot;, &quot;Year&quot;]]    .groupby(&quot;Age&quot;).aggregate(lambda x: (np.max(x)-np.min(x))/2)    .reset_index())mr = lambda x: (np.max(x)-np.min(x))/2mr.__name__ = &quot;midrange&quot;(trees.loc[:, [&quot;Age&quot;, &quot;Diameter&quot;, &quot;Year&quot;]]    .groupby(&quot;Age&quot;).aggregate([np.mean, mr]).reset_index())\nTransforming Data in Groupsdef standardise(x):    return (x-np.mean(x))/np.std(x)trees[&quot;Diameter_Species_Std&quot;] = (trees.loc[:, [&quot;Species&quot;, &quot;Diameter&quot;]]    .groupby(&quot;Species&quot;).transform(standardise))(trees.loc[:, [&quot;Species&quot;, &quot;Diameter&quot;, &quot;Diameter_Species_Std&quot;]]    .groupby(&quot;Species&quot;).aggregate([np.mean, np.std]))\nPlotting Data in Groups with seabornsns.boxplot(x=&quot;Diameter&quot;, y=&quot;Species&quot;, data=trees)plt.show()sns.barplot(    y=&quot;Counts&quot;,    x=&quot;Species&quot;,    hue=&quot;Location&quot;,    data=trees.groupby([&quot;Species&quot;, &quot;Location&quot;]).size().rename(&quot;Counts&quot;).reset_index())plt.show()\nDatabase Accessimport sqlite3import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snspd.set_option(&quot;display.notebook_repr_html&quot;, False)  # disable &quot;rich&quot; outputplt.style.use(&quot;seaborn&quot;)\nTags = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/travel_stackexchange_com_2017/Tags.csv.gz&quot;,    comment=&quot;#&quot;)Tags.head(3)Users = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/travel_stackexchange_com_2017/Users.csv.gz&quot;,    comment=&quot;#&quot;)Users.head(3)Badges = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/travel_stackexchange_com_2017/Badges.csv.gz&quot;,    comment=&quot;#&quot;)Posts = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/travel_stackexchange_com_2017/Posts.csv.gz&quot;,    comment=&quot;#&quot;)Posts.head(3)Votes = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/travel_stackexchange_com_2017/Votes.csv.gz&quot;,    comment=&quot;#&quot;)Votes.head(3)\nExporting Dataimport tempfile, os.pathdbfile = os.path.join(tempfile.mkdtemp(), &quot;travel.db&quot;)print(dbfile)conn = sqlite3.connect(dbfile)Tags.to_sql(&quot;Tags&quot;, conn, index=False)Users.to_sql(&quot;Users&quot;, conn, index=False)Badges.to_sql(&quot;Badges&quot;, conn, index=False)Posts.to_sql(&quot;Posts&quot;, conn, index=False)Votes.to_sql(&quot;Votes&quot;, conn, index=False)\nExample SQL Queriespd.read_sql_query(&quot;&quot;&quot;    SELECT * FROM Tags LIMIT 3&quot;&quot;&quot;, conn)res1a = pd.read_sql_query(&quot;&quot;&quot;    SELECT * FROM Tags LIMIT 3&quot;&quot;&quot;, conn)res1b = Tags.head(3)res1a.equals(res1b)\nFilteringpd.read_sql_query(&quot;&quot;&quot;    SELECT * FROM Tags WHERE TagName LIKE &#x27;%europe%&#x27;&quot;&quot;&quot;, conn)pd.read_sql_query(&quot;&quot;&quot;    SELECT TagName, Count    FROM Tags    WHERE TagName IN (&#x27;poland&#x27;, &#x27;australia&#x27;, &#x27;china&#x27;)&quot;&quot;&quot;, conn)pd.read_sql_query(&quot;&quot;&quot;    SELECT Title, Score, ViewCount, FavoriteCount    FROM Posts    WHERE PostTypeId=1 AND        ViewCount&gt;=10000 AND        FavoriteCount BETWEEN 35 AND 100&quot;&quot;&quot;, conn)\nOrderingpd.read_sql_query(&quot;&quot;&quot;    SELECT Title, Score    FROM Posts    WHERE ParentId IS NULL AND Title IS NOT NULL    ORDER BY Score DESC    LIMIT 5&quot;&quot;&quot;, conn)pd.read_sql_query(&quot;&quot;&quot;    SELECT DISTINCT Name    FROM Badges    WHERE UserId=23&quot;&quot;&quot;, conn)pd.read_sql_query(&quot;&quot;&quot;    SELECT DISTINCT Name, strftime(&#x27;%Y&#x27;, Date) AS Year    FROM Badges    WHERE UserId=23&quot;&quot;&quot;, conn)\nGrouping and Aggregatingpd.read_sql_query(&quot;&quot;&quot;    SELECT        Name,        COUNT(*) AS Count,        MIN(strftime(&#x27;%Y&#x27;, Date)) AS MinYear,        AVG(strftime(&#x27;%Y&#x27;, Date)) AS MeanYear,        MAX(strftime(&#x27;%Y&#x27;, Date)) AS MaxYear    FROM Badges    WHERE UserId=23    GROUP BY Name    ORDER BY Count DESC    LIMIT 4&quot;&quot;&quot;, conn)pd.read_sql_query(&quot;&quot;&quot;    SELECT        Name,        strftime(&#x27;%Y&#x27;, Date) AS Year,        COUNT(*) AS Count    FROM Badges    WHERE UserId=23    GROUP BY Name, Year    HAVING Count &gt; 1    ORDER BY Count DESC&quot;&quot;&quot;, conn)\nJoiningpd.read_sql_query(&quot;&quot;&quot;    SELECT Tags.TagName, Tags.Count, Posts.OwnerUserId,        Users.Age, Users.Location, Users.DisplayName    FROM Tags    JOIN Posts ON Posts.Id=Tags.WikiPostId    JOIN Users ON Users.AccountId=Posts.OwnerUserId    WHERE OwnerUserId != -1    ORDER BY Tags.Count DESC LIMIT 5&quot;&quot;&quot;, conn)pd.read_sql_query(&quot;&quot;&quot;    SELECT UpVotesTab.*, Posts.Title FROM    (        SELECT PostId, COUNT(*) AS UpVotes        FROM Votes        WHERE VoteTypeId=2        GROUP BY PostId    ) AS UpVotesTab    JOIN Posts ON UpVotesTab.PostId=Posts.Id    WHERE Posts.PostTypeId=1    ORDER BY UpVotesTab.UpVotes DESC LIMIT 5&quot;&quot;&quot;, conn)\nClosing the Database Connectionconn.close()\n","categories":["Python"],"tags":["Python"]},{"title":"Fix \"Unable to locate package docker-compose-plugin\" on Ubuntu","url":"/2025/07/26/ubuntu/01-ubuntu-changeSource/","content":"Install Docker Compose on UbuntuWhen trying to install Docker Compose with the command:\nsudo apt install docker-compose-plugin\nyou might encounter the following error:Reading package lists... DoneBuilding dependency tree... DoneReading state information... DoneE: Unable to locate package docker-compose-pluginThis happens because the default Ubuntu repositories don‚Äôt include the latest Docker packages. To fix it, you need to enable the official Docker repository first.\nSolution\nRemove old Docker Compose (if installed)sudo apt remove docker-compose\nAdd Docker‚Äôs official repositorysudo apt-get updatesudo apt-get install -y ca-certificates curl gnupg lsb-releasesudo mkdir -p /etc/apt/keyringscurl -fsSL https://download.docker.com/linux/ubuntu/gpg \\  | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpgecho \\  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \\  https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; \\  | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nInstall Docker Compose Pluginsudo apt-get updatesudo apt-get install docker-compose-plugin\n\n","categories":["ubuntu","docker","docker-compose"],"tags":["ubuntu"]},{"title":"Installing Docker in Ubuntu","url":"/2025/07/26/ubuntu/02-ubuntu-installDocker/","content":"Installing Docker in UbuntuDocker (https://www.docker.com/) is an open-source technology that allows for applications to be packaged into ‚Äòcontainers‚Äô which include any required libraries and other dependencies. This makes it very easy to deploy applications (especially for large enterprises who need to setup their workstations), and you can run more applications on the same amount of hardware (as containers use less resources than virtual machines hosting the applications), among other benefits. It is becoming widely popular.\nUse Docker to install DWVA (addressed below). The instructions below are derived from the official Docker installation instructions for Ubuntu (https://docs.docker.com/engine/install/ubuntu/).\n\nOpen a Terminal in Ubuntu. Update the Ubuntu package manager before proceeding using the command: sudo apt update\nInstall the prerequisite packages for Docker by entering the command: sudo apt install ca-certificates curl gnupg lsb-release\nAdd Docker‚Äôs official GPG key using the command: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg ‚Äîdearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\nSetup the Docker repository below to the latest stable release by entering the below command in the terminal. Type this on the one line in the terminal: echo ‚Äúdeb [arch=$(dpkg ‚Äîprint-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable‚Äù | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nWith the Docker repository correctly configured with apt, prepare for installation with: sudo apt update\nInstall Docker using the command: sudo apt-get install docker-ce docker-ce-cli containerd.io\nVerify installation was complete by running a simple Docker container called ‚ÄúHello World‚Äù: sudo docker run hello-world\n\n","categories":["ubuntu","docker"],"tags":["ubuntu"]},{"title":"Classification Algorithms","url":"/2025/06/19/ai/algorithm/01-classification-algorithms/","content":"Dataset IntroductionNSL-KDDThe NSL-KDD dataset is a dataset for intrusion detection, which is a type of supervised learning problem. It consists of a large number of network traffic records that are labeled as either normal or malicious. The dataset contains a total of 41,478 network traffic records, which are categorized into 10 different types of attacks, such as DoS, Probe, U2R, R2L, etc. The dataset is publicly available and can be downloaded from the following link: https://www.unb.ca/cic/datasets/nsl.html.\nProcessed Combined IoT datasetThe processed combined IoT dataset is a dataset for anomaly detection, which is a type of unsupervised learning problem. It consists of a large number of IoT sensor data that are labeled as normal or abnormal. The dataset contains a total of 1,000,000 IoT sensor data records, which are categorized into 10 different types of attacks, such as DoS, Probe, U2R, R2L, etc. The dataset is publicly available and can be downloaded from the following link: https://www.kaggle.com/uciml/iot-sensor-dataset.https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9189760 \nClassification AlgorithmsDecision TreeDecision Trees, also known as Classification and Regression Trees (CART), construct a hierarchical tree structure to make predictions. The algorithm splits data at the most optimal points, iteratively refining its predictions. After constructing the tree, pruning techniques are applied to enhance generalization to new data. For this dataset, the Decision Tree algorithm achieves exceptional results, reflecting its ability to handle binary classification tasks effectively and its robustness against overfitting.\nLogistic RegressionLogistic Regression is a binary classification technique that assumes numeric input variables with a Gaussian distribution. Although this assumption is not mandatory, the algorithm performs well even if the data does not adhere to this pattern. Logistic regression calculates coefficients for each input variable, combining them linearly into a regression function and applying a logistic transformation. While it is simple and fast, its effectiveness depends on the characteristics of the dataset. For this dataset, logistic regression may face limitations due to the non-Gaussian distribution of many attributes.\nNaive BayesNa√Øve Bayes is a probabilistic classifier that assumes conditional independence between variables. While this assumption is unrealistic in practice, it simplifies calculations and enables efficient predictions. Na√Øve Bayes calculates the posterior probability for each class and predicts the class with the highest probability. Despite its simplicity, Na√Øve Bayes can be surprisingly effective, particularly for problems with nominal or numerical inputs assuming a specific distribution. Its performance on this dataset might be hindered by the strong interdependence among variables.\nK-Nearest Neighbors (kNN)The k-Nearest Neighbors algorithm is a non-parametric technique that predicts outputs based on the k most similar instances in the training dataset. kNN does not build a model; instead, it relies on the raw dataset and distance-based computations during prediction. This simplicity often results in good performance, particularly for datasets where the similarity between instances is meaningful. However, kNN may be computationally expensive for large datasets and sensitive to noise in the data.\nRandom ForestRandom forest is a type of ensemble learning algorithm that can be used for both classification and regression problems. It is a collection of decision trees that are trained on random subsets of the data. The algorithm combines the predictions of each tree to make a final prediction. The algorithm can handle both categorical and numerical data.\nSupport Vector Machines (SVM)Support Vector Machines are designed primarily for binary classification, though extensions support multi-class classification and regression. SVM finds an optimal hyperplane to separate data into two groups, using support vectors to define the margin. For datasets that are not linearly separable, SVM applies kernel functions such as Polynomial or RBF (Radial Basis Function) to transform the data into higher dimensions. In this dataset, SVM achieves competitive results, particularly with well-chosen kernel functions, demonstrating its ability to handle complex decision boundaries.\nMultilayer PerceptronMultilayer perceptron (MLP) is a type of supervised learning algorithm that can be used for both classification and regression problems. It is a feedforward artificial neural network that consists of multiple layers of nodes. The algorithm trains the network by adjusting the weights of the connections between the nodes. The algorithm can handle both categorical and numerical data.\nEvaluation MetricsAccuracyAccuracy is the most commonly used evaluation metric for classification problems. It is the ratio of the number of correct predictions to the total number of predictions.Accuracyi = (ùëáùëÉùëñ + ùëáùëÅùëñ) /ùëáùëúùë°ùëéùëô ùëÜùëéùëöùëùùëôùëís\nPrecisionPrecision is the ratio of true positives to the total number of true positives and false positives.\nùëÉùëüùëíùëêùëñùë†ùëñùëúùëõùëñ = ùëáùëÉùëñ / (ùëáùëÉi+FPi)\nRecallRecall is the ratio of true positives to the total number of true positives and false negatives.\nRecalli = ùëáùëÉùëñ / (ùëáùëÉi+FNi)\nF-scoreF-score is the harmonic mean of precision and recall.\nF-scorei = 2 * (precisioni * recalli) / (precisioni + recalli)\nFalse AlarmFalse alarm is the ratio of false positives to the total number of false positives and true negatives.\nFalse Alarmi = FPùëñ / (ùëáùëÅi+FPi)\n","categories":["ai","foundation","classification algorithms"],"tags":["ai"]},{"title":"Comparative Analysis of Classification Algorithms","url":"/2025/06/20/ai/algorithm/02-analysis-classification-algorithms/","content":"Comparative Analysis of Classification AlgorithmsAbstractThis report provides a comprehensive analysis of the performance of three popular classification algorithms:Decision Tree, Logistic Regression, Naive Bayes, Random Forest, Support Vector Machines (SVM) and Multilayer Perceptron.The purpose of the study is to evaluate and compare these algorithms based on key performance metrics such as accuracy, precision, recall, F1-score, and false alarm rate (www.evidentlyai.com, n.d.), using two different datasets.The dataset underwent preprocessing steps, including applying multiple scalers, normalization and handling of missing values, to ensure consistency and reliability of the results. The algorithms were trained and tested, with additional validation.Confusion matrices (www.evidentlyai.com, n.d.) were used to visualize classification errors, while performance metrics were calculated for a detailed assessment. \n\nRandom Forest consistently outperformed the other algorithms, achieving the highest accuracy of 95.2% and balanced metrics across classes. \nSVM demonstrated competitive performance but struggled with classes having overlapping distributions. Naive Bayes, while computationally efficient, showed limitations due to its independence assumption, resulting in lower precision for certain classes. \n\nThe results are presented through comparative tables and visualizations, providing actionable insights into the strengths and weaknesses of each algorithm. This analysis serves as a practical guide for selecting classification algorithms based on dataset characteristics and application requirements.\nIntroductionThe objective of this study is to compare various classification algorithms to evaluate their performance across key metrics, such as accuracy, precision, recall, and F1-score.Classification plays a pivotal role in machine learning, enabling systems to categorize data effectively for decision-making tasks.The algorithms explored include Decision Tree, Logistic Regression, Na√Øve Bayes, Random Forest, SVM-SVC, and MLP. The datasets are ‚ÄúNSL-KDD‚Äù AND ‚ÄúProcessed Combined IoT dataset‚Äù. Preprocessing steps included handling missing values, scaling features, and encoding categorical data, preparing the dataset for optimal model performance.\nDataset IntroductionNSL-KDDAccording to Gao(2019), the NSL-KDD dataset is a refined version of the original KDD‚Äô99 dataset, designed for evaluating intrusion detection systems (IDS). It addresses issues like redundant records and class imbalance, providing a balanced and reliable dataset. It includes 41 features per record and categorizes attacks into four types: DoS, Probe, R2L, and U2R. The dataset is divided into training and testing sets, with the testing set containing unseen attack types to evaluate generalization. Widely used for IDS research, feature engineering, and algorithm evaluation, NSL-KDD remains a popular benchmark, though its lack of modern threats highlights the need for complementary dataset.\nProcessed Combined IoT datasetAccording to Alsaedi et al., the TON_IoT dataset is a new data-driven IoT/IIoT dataset. It includes heterogeneous data sources gathered from the Telemetry data of IoT/IIoT services, as well as the Operating Systems logs and Network traffic of IoT network, collected from a realistic representation of a medium-scale network. The dataset has various normal and attack events for different IoT/IIoT services, and includes a combination of physical and simulated IoT/IIoT services . The TON_IoT dataset aims to address the limitations of existing datasets by providing a more representative dataset for evaluating cybersecurity solutions and machine learning methods for IoT/IIoT applications . The dataset can be accessed through the TON-IoT repository.\nAlgorithms OverviewDecision TreeWorking PrincipleA Decision Tree (scikit-learn, 2019) is a supervised learning algorithm used for both classification and regression tasks. It recursively splits the dataset into subsets based on feature values, creating a tree-like structure. Each internal node represents a decision based on a feature, while each leaf node represents the final predicted class or value. The tree construction process uses algorithms like ID3, CART, or C4.5, with the primary goal of reducing impurity at each split. Impurity is measured using metrics such as Gini Impurity or Entropy (for classification). The tree grows by selecting the feature that best separates the data at each node. Decision Trees are easy to understand and interpret, but they tend to overfit if not properly regularized.\nKey Parameters\n\nmax_depth: Limits the depth of the tree to avoid overfitting. A smaller depth prevents the model from capturing too much noise from the training data.\nmin_samples_split: Specifies the minimum number of samples required to split an internal node. Larger values prevent the tree from becoming too specific to the training data.\nmax_features: Defines the number of features to consider when looking for the best split. Reducing this value can speed up the training process and introduce diversity in ensemble methods like Random Forest.\ncriterion: The function used to measure the quality of a split. It can be gini for Gini impurity or entropy for information gain.\n\nLogistic RegressionWorking PrincipleLogistic Regression (scikit-learn, 2014) is a statistical method used for binary classification tasks. It predicts the probability that a given input belongs to a particular class. Unlike linear regression, which predicts continuous values, logistic regression applies a logistic (sigmoid) function to the output of a linear equation to map predictions to a probability between 0 and 1. The model learns by adjusting weights assigned to each feature to minimize the difference between predicted probabilities and actual class labels. Logistic Regression assumes a linear relationship between input features and the log odds of the target variable, making it simple yet effective for many classification problems.\nLogistic Regression is widely used due to its simplicity, interpretability, and effectiveness in problems where the classes are linearly separable.Key Parameters\n\nC: The regularization strength. It controls the trade-off between fitting the data well and preventing overfitting. Smaller values of C apply stronger regularization (higher penalty on complexity).\nsolver: The algorithm used to optimize the weights of the model. Common solvers include liblinear, newton-cg, lbfgs, and saga. The solver choice affects convergence speed and suitability for certain datasets.\nmax_iter: The maximum number of iterations for optimization algorithms. A higher value allows the algorithm to converge more precisely, especially for complex datasets.\npenalty: The regularization technique used. Common penalties include l2 (Ridge regularization), which discourages large coefficients, and l1 (Lasso), which encourages sparsity in the model.\nmulti_class: Defines how multi-class classification is handled. Options include ovr (one-vs-rest) and multinomial. The choice depends on the problem being solved.\n\nNaive BayesWorking PrincipleNaive Bayes (scikit-learn, n.d.) is a probabilistic algorithm based on Bayes‚Äô Theorem, which calculates the posterior probability of a class given the features. It assumes independence among predictors, simplifying computations and making it efficient for high-dimensional data. Despite the naive assumption, it performs well in applications such as text classification and spam filtering. Naive Bayes is fast and computationally efficient, particularly for large datasets with independent features.Key Parameters\n\nvar_smoothing: A small additive constant to stabilize calculations and avoid division by zero when features have zero variance. Adjusting this parameter can help handle datasets with highly varying feature distributions.\n\nRandom ForestWorking PrincipleRandom Forest (scikit-learn, 2018) is an ensemble learning method that builds multiple decision trees during training and combines their predictions to improve classification accuracy and control overfitting. Each tree is trained on a bootstrap sample of the data, and feature selection at each node ensures diversity among the trees. For classification, the output is determined by majority voting among the trees. This method excels at handling large datasets with high dimensionality and is robust against noisy data.Random Forest balances bias and variance effectively, making it suitable for diverse datasets.\nKey Parameters\n\nn_estimators: The number of trees in the forest. Increasing this value improves accuracy but can increase computational cost.\nmax_depth: The maximum depth of each tree. Limiting depth prevents overfitting while maintaining performance.\nmin_samples_split: The minimum number of samples required to split a node. Higher values lead to simpler trees that generalize better.\n\nSupport Vector Machines (SVM)Working PrincipleSVM (scikit-learn, 2019) identifies a hyperplane that separates classes with the maximum margin. It uses kernel functions to transform data into higher dimensions for non-linear separation. By maximizing the margin, SVM minimizes classification error. It is effective for both linear and non-linear problems and is robust to overfitting, especially in highdimensional spaces.SVM is versatile and powerful, particularly for datasets with complex decision boundaries.\nKey Parameters\n\nc: The regularization parameter that controls the trade-off between achieving a low error on the training set and maintaining a large margin.\nkernel: Specifies the type of kernel function (e.g., linear, radial basis function (RBF), or polynomial). The choice depends on the dataset‚Äôs nature.\ngamma: The kernel coefficient for RBF and polynomial kernels, influencing the decision boundary‚Äôs flexibility. Lower values result in smoother boundaries, while higher values allow more complex boundaries.\n\nMultilayer PerceptronWorking PrincipleA Multilayer Perceptron (MLP) (scikit-learn, 2010) is a type of artificial neural network (ANN) composed of multiple layers of nodes (neurons). It consists of an input layer, one or more hidden layers, and an output layer. MLP is used for supervised learning tasks, such as classification and regression. The network learns by adjusting weights based on the errors between predicted and actual outputs using an optimization algorithm, typically backpropagation.\nIn MLP, the input data is passed through each layer of neurons. Each neuron applies a weighted sum of inputs, followed by an activation function to introduce non-linearity into the model. Common activation functions include ReLU (Rectified Linear Unit) for hidden layers and softmax for the output layer in classification tasks. During training, the model uses the backpropagation algorithm to compute the gradient of the loss function and update the weights accordingly. The goal is to minimize the difference between the predicted and actual output (i.e., minimize the loss).\nMLP is a powerful model capable of learning complex patterns, making it well-suited for tasks such as image recognition, speech processing, and classification tasks that involve nonlinear decision boundaries. MLPs are versatile and can handle both simple and complex datasets. By adjusting the key parameters, MLPs can be fine-tuned for a variety of applications, including image recognition, natural language processing, and more.\nKey Parameters\n\nhidden_layer_sizes: Defines the number and size of hidden layers in the network. For example, (100,) specifies a network with one hidden layer containing 100 neurons. The configuration of hidden layers affects the network**s ability to capture complex relationships.\nactivation: Specifies the activation function used in the hidden layers. Common options include:\nrelu: Rectified Linear Unit, a popular activation function that helps with faster training.\ntanh: Hyperbolic tangent, which outputs values between -1 and 1.\nlogistic: Sigmoid function, often used for binary classification tasks.\n\n\nsolver: The algorithm used for weight optimization. The common solvers are:\nadam: A popular optimization algorithm that adapts the learning rate during training.\nsgd: Stochastic Gradient Descent, a basic optimizer that updates weights based on a random subset of data.\nlbfgs: A quasi-Newton method, effective for smaller datasets.\n\n\nlearning_rate: Determines how the learning rate changes during training. Options include:\nconstant: The learning rate remains constant throughout training.\ninvscaling: Gradually decreases the learning rate as training progresses.\nadaptive: The learning rate decreases when the validation score is not improving.\n\n\nmax_iter: The maximum number of iterations for optimization. More iterations generally allow for better convergence, especially on more complex datasets.\nalpha: L2 regularization term that helps prevent overfitting by penalizing large weights. Higher values of alpha make the model simpler by forcing smaller weights.\nbatch_size: The number of samples processed before the model updates its weights. Larger batch sizes lead to more stable but slower updates, while smaller batch sizes can speed up training but result in noisier updates.\n\nEvaluation MetricsAccuracyThe accuracy indicates the proportion of samples correctly classified by the model to the total number of samples.\n\nPrecision_i= \\frac{(TP_i+TN_i)}{(Total Samples)}Among them, $TP_i$ are the true positive examples of class i  , and $FP_i$ are the false positive examples predicted as class i  from other categories.\nPrecisionPrecision is the proportion of correctly positive samples among all the samples predicted as positive. For each category i , the calculation formula for precision is:\n\nPrecision_i= \\frac{TP_i}{(TP_i+FP_i)}Among them, $TP_i$ are the true positive examples of class i  , and $FP_i$ are the false positive examples predicted as class i  from other categories.\nRecallRecall is the proportion of correctly predicted positive samples among all the actual positive samples. For each category i , the calculation formula for precision is:\n\nRecall_i= \\frac{TP_i} {(TP_i+FN_i)}Among them, $TP_i$ are the true positive examples of class i  , and $FN_i$ are the false positive examples predicted as class i  from other categories.\nF-ScoreThe F-Score is the harmonic mean of precision and recall. The calculation formula is:\n\nF_i=\\frac{(2*Precision*Recall)}{(Precision+Recall)}False AlarmFPR (False Positive Rate) is the proportion of negative samples that are incorrectly classified as positive among all actual negative samples. For each category i  , the calculation formula for the false positive rate is:\n\nFPR_i=\\frac{FP_i}{(FP_i+FP_i )}Among them, $FP_i$ are the false positive examples predicted as class i  from other categories, and $FP_i$ are the false positive examples predicted as class i  from other categories.\nResults and AnalysisPerformance MetricsDecision TreeAccording to the API documentation on scikit-learn, parameters such as max_depth, min_samples_split, min_samples_leaf, and criterion were tuned. The optimal parameter values were found to be: max_depth (default), min_samples_split=14, min_samples_leaf=2, and criterion=‚Äôgini‚Äô.\nDataset 1 (NSL-KDD)\nDataset 2 (TON_IOT)\nComparing\n\n\n\nMetric\nNSL-KDD (DT)\nTON-IoT (DT)\n\n\n\n\nAccuracy (%)\n90.846\n86.5\n\n\nPrecision (%)\n83.33\n87.25\n\n\nRecall (%)\n51.274\n84.265\n\n\nF-Score (%)\n53.2\n85.285\n\n\nFalse Alarm (%)\n16.67\n12.75\n\n\n\n\nwe can infer that the TON-IoT dataset generally yields better overall performance with the Decision Tree algorithm due to higher precision, recall, F-Score, and lower false alarm rates. In contrast, the NSL-KDD dataset achieves higher accuracy but struggles with recall and false alarm rates, possibly due to class imbalances or complexity in its features.\nLogistic RegressionLogistic Regression provides several parameters that can be adjusted to change the behaviour of the model: Best parameters: {‚ÄòC‚Äô: 0.1, ‚Äòpenalty‚Äô: ‚Äòl2‚Äô, ‚Äòsolver‚Äô: ‚Äòlbfgs‚Äô}\nDataset 1 (NSL-KDD)\nDataset 2 (TON_IOT)\nComparing\n\n\n\nMetric\nNSL-KDD (DT)\nTON-IoT (DT)\n\n\n\n\nAccuracy (%)\n89.88\n68.78\n\n\nPrecision (%)\n79.54\n76.275\n\n\nRecall (%)\n49.51\n60.605\n\n\nF-Score (%)\n49.19\n58.19\n\n\nFalse Alarm (%)\n20.46\n23.725\n\n\n\n\nNSL-KDD Dataset: Logistic Regression is better at achieving higher accuracy and precision, making it more reliable for applications prioritizing these metrics.TON-IoT Dataset: Logistic Regression is more effective at identifying true positives, as evidenced by its higher recall and F-Score, but it suffers from lower overall accuracy.\nNa√Øve BayesFrom scikit-learn, we know that the performance of GaussianNB is depend on priors and var_smoothing.  Best Parameters: {‚Äòvar_smoothing‚Äô: 1e-05}\nDataset 1 (NSL-KDD)\nDataset 2 (TON_IOT)\nComparing\n\n\n\nMetric\nNSL-KDD (DT)\nTON-IoT (DT)\n\n\n\n\nAccuracy (%)\n86.77\n69.99\n\n\nPrecision (%)\n51.058\n72.59\n\n\nRecall (%)\n52.292\n63.29\n\n\nF-Score (%)\n48.608\n62.68\n\n\nFalse Alarm (%)\n48.942\n27.41\n\n\n\n\nNSL-KDD Dataset: Na√Øve Bayes achieves high accuracy but struggles with false positives and balanced performance (F-Score).TON-IoT Dataset: Na√Øve Bayes provides a more balanced classification with fewer false positives, making it better suited for use cases where precision and recall are critical.\nRandom ForestRandom Forest has several important hyperparameters that we can adjust to improve its performance:Best parameters found:  {‚Äòbootstrap‚Äô: False, ‚Äòmax_depth‚Äô: None, ‚Äòmin_samples_leaf‚Äô: 1, ‚Äòmin_samples_split‚Äô: 4, ‚Äòn_estimators‚Äô: 33}\nDataset 1 (NSL-KDD)\nDataset 2 (TON_IOT)\nComparing\n\n\n\nMetric\nNSL-KDD (DT)\nTON-IoT (DT)\n\n\n\n\nAccuracy (%)\n89.75\n86.96\n\n\nPrecision (%)\n80.72\n87.935\n\n\nRecall (%)\n48.088\n84.64\n\n\nF-Score (%)\n48.424\n85.745\n\n\nFalse Alarm (%)\n19.276\n12.065\n\n\n\n\nNSL-KDD Dataset: Random Forest achieves high accuracy but is less effective at achieving balanced precision and recall, with a moderate false alarm rate.TON-IoT Dataset: Random Forest excels in all metrics, providing robust and balanced performance with minimal false alarms, making it a better-suited dataset for this algorithm.\nSVM-SVCSVM-SVC has two important hyperparameters that we can adjust to improve its performance:Best parameters: {‚ÄòC‚Äô: 1, ‚Äògamma‚Äô: ‚Äòscale‚Äô}\nDataset 1 (NSL-KDD)\nDataset 2 (TON_IOT)Best params: {‚Äòkernel‚Äô: [‚Äòpoly‚Äô], ‚ÄòC‚Äô: [1  10*i for i in range(-3, 11)], ‚Äòdegree‚Äô: range(2, 10), ‚Äòclass_weight‚Äô: [‚Äòbalanced‚Äô], ‚Äòmax_iter‚Äô: [1000]}\n\nComparing\n\n\n\nMetric\nNSL-KDD (DT)\nTON-IoT (DT)\n\n\n\n\nAccuracy (%)\n90.338\n60.51\n\n\nPrecision (%)\n79.56\n45.615\n\n\nRecall (%)\n51.646\n49.735\n\n\nF-Score (%)\n53.84\n38.71\n\n\nFalse Alarm (%)\n20.44\n54.385\n\n\n\n\nNSL-KDD Dataset: SVM-SVC achieves excellent accuracy and precision but struggles with recall and balanced classification.TON-IoT Dataset: SVM-SVC underperforms significantly, with low accuracy, precision, and F-Score, and a very high false alarm rate, suggesting it is not well-suited for this dataset without further tuning or preprocessing.\nMLPMLP has four important hyperparameters that we can adjust to improve its performance:Best Parameters: {‚Äòalpha‚Äô: 0.001, ‚Äòhidden_layer_sizes‚Äô: (100,), ‚Äòlearning_rate_init‚Äô: 0.001, ‚Äòsolver‚Äô: ‚Äòadam‚Äô}\nDataset 1 (NSL-KDD)\nDataset 2 (TON_IOT)\nComparing\n\n\n\nMetric\nNSL-KDD (DT)\nTON-IoT (DT)\n\n\n\n\nAccuracy (%)\n90.22\n80.38\n\n\nPrecision (%)\n72.884\n84.685\n\n\nRecall (%)\n50.408\n75.58\n\n\nF-Score (%)\n52.406\n76.98\n\n\nFalse Alarm (%)\n27.116\n15.315\n\n\n\n\nNSL-KDD Dataset: MLP achieves high accuracy but struggles with recall and false alarms, suggesting potential improvements through feature engineering or class balancing.TON-IoT Dataset: MLP delivers robust performance with high precision, recall, and low false alarm rate, making it a strong candidate for this dataset.\nSummaryDataset 1 (NSL-KDD)\n\n\n\nAlgorithms\nAccuracy (%)\nPrecision (%)\nRecall (%)\nF-Score (%)\nFalse Alarm-FPR (%)\nTimes (s)\n\n\n\n\nDeciTree\n90.846\n83.33\n51.274\n53.2\n16.67\n1\n\n\nLR\n89.88\n79.54\n49.51\n49.19\n20.46\n60\n\n\nNB\n86.77\n51.058\n52.292\n48.608\n48.942\n15\n\n\nRanForest\n89.75\n80.72\n48.088\n48.424\n19.276\n220\n\n\nSVM-SVC\n90.338\n79.56\n51.646\n53.84\n20.44\n582\n\n\nMLP\n90.22\n72.884\n50.408\n52.406\n27.116\n3600\n\n\n\n\n\nDisscussionIn this analysis, we evaluate the performance of six classification algorithms‚ÄîDecision Tree, Logistic Regression (LR), Na√Øve Bayes (NB), Random Forest (RF), SVM-SVC, and Multilayer Perceptron (MLP). The metrics considered include Accuracy, Precision, Recall, F1-Score, False Positive Rate (FPR), and execution time. Below is a summary of trends, dataset impact, and algorithm suitability based on the given dataset.\nPerformance Trends\nAccuracy\nHighest Accuracy: From Picture 25, we can see that Decision Tree (90.846%) and SVM-SVC (90.338%) are the best-performing algorithms.\nLower Accuracy: Naive Bayes (86.77%), indicating it is less reliable for overall prediction correctness.\n\n\nPrecision\nFrom the Picture 25, Decision Tree (83.33%), showing it has the fewest False Positives among all algorithms.\nNa√Øve Bayes showed the weakest precision (51.058%), aligning with its limited performance in complex datasets.    \n\n\nRecall\nNaive Bayes (52.292%) and SVM-SVC (51.646%) perform best in detecting true positives.\nRandom Forest (48.088%), indicating it misses many true positives.\n\n\nF1-Score\nSVM-SVC (53.84%) and Decision Tree (53.2%) provide the most balanced trade-off.\nNaive Bayes (48.608%) again struggles with performance consistency.\n\n\nFalse Positive Rate (FPR)\nDecision Tree (16.67%), making it the most reliable in minimizing False Alarms.\nMulti-Layer Perceptron (27.116%), which is prone to generating more False Alarms.\n\n\n\nImpact of Dataset Characteristics\nDecision Tree (DeciTree) Strengths:\n - Accuracy (90.846%) and Precision (83.33%): Decision trees perform well for problems with simple decision boundaries, leading to good overall results.\n - Execution time (1 seconds): The algorithm is straightforward and efficient, making it suitable for small-scale datasets or simple problems.\n Weaknesses:\n - Low Recall (51.27%) and F1-Score (53.2%): Overfitting might result in poor recall for minority classes.\n - Susceptible to overfitting: When the dataset has noise or high-dimensional features, the model may capture irrelevant details, reducing generalization performance.\n\nLogistic Regression (LR) Strengths:\n - Balanced performance: As a linear model, LR works well for linearly separable data, achieving high precision 89.88%).\n - Efficient runtime (60 seconds): The algorithm is computationally efficient.\n Weaknesses:\n - Low Recall (49.51%) and F1-Score (49.19%): LR struggles with capturing complex nonlinear relationships, leading to lower recall and overall classification performance.\n - Dependency on feature relationships: LR assumes a linear relationship between features and outcomes, limiting its effectiveness on nonlinear datasets.\n\nNa√Øve Bayes (NB) Strengths:\n - Efficient computation: When the independence assumption holds, NB can quickly calculate posterior probabilities, making it suitable for tasks like text classification.\n Weaknesses:\n - Low Accuracy (86.77%) and Precision (51.058%): The model struggles to utilize feature correlations effectively, as the independence assumption often doesn&#39;t hold, reducing its performance.\n - Low Recall (52.292%) and F1-Score (48.608%): Feature interdependence limits the model&#39;s effectiveness on complex datasets.\n\nRandom Forest (RanForest) Strengths: - High Precision (89.75%): By aggregating multiple decision trees, Random Forest mitigates the overfitting issues of single trees.\n - Versatility: It can handle high-dimensional data and capture complex relationships.\n Weaknesses: - High runtime (220 seconds): The complexity of the model significantly increases training and inference time compared to simpler algorithms.\n - Low Recall (48.088%) and F1-Score (48.424%): The model may not handle class imbalance well, requiring parameter tuning (e.g., class weights) to improve recall.\n\nSVM-SVC Strengths: - Adaptability to high-dimensional data: SVM with RBF kernels handles complex, nonlinear decision boundaries effectively.\n - Balanced performance: Accuracy (90.338%) and runtime (582 seconds) are relatively moderate.\n Weaknesses: - Low Precision and Recall (51.646%, 53.84%): The model‚Äôs performance may be impacted by the complexity of data features and relationships.\n - High time complexity: Training time increases significantly for larger datasets.\n\nMultilayer Perceptron (MLP) Strengths: - Relatively stable Accuracy (90.22%) and F1-Score (52.406%): MLP effectively handles complex nonlinear problems.\n - Strong fitting ability: Particularly suitable for large-scale data with complex features.\n Weaknesses: - High runtime (3600 seconds): Training neural networks is computationally expensive, especially for high-dimensional datasets.\n - Low Recall (50.408%): The model may not fully capture minority class characteristics due to insufficient parameter tuning or training data.\n\n\nDataset 2 (TON_IOT)\n\n\n\nAlgorithms\nAccuracy (%)\nPrecision (%)\nRecall (%)\nF-Score (%)\nFalse Alarm-FPR (%)\nTimes (s)\n\n\n\n\nDeciTree\n86.5\n87.25\n84.265\n85.285\n12.75\n1\n\n\nLR\n68.78\n76.275\n60.605\n58.19\n23.725\n319\n\n\nNB\n69.99\n72.59\n63.29\n62.68\n27.41\n1\n\n\nRanForest\n86.96\n87.935\n84.64\n85.745\n12.065\n43\n\n\nSVM-SVC\n60.51\n45.615\n49.735\n38.71\n54.385\n3600\n\n\nMLP\n80.38\n84.685\n75.58\n76.98\n15.315\n2115\n\n\n\n\n\nPerformance Trends\nAccuracy\nBest: Random Forest (86.96%) and Decision Tree (86.5%) are the most accurate.\nWorst: SVM-SVC (60.51%) has the lowest accuracy.\n\n\nPrecision\nBest: Random Forest (87.935%) and Decision Tree (87.25%) again perform best in terms of precision.\nWorst: SVM-SVC (45.615%) significantly underperforms.\n\n\nRecall\nBest: Random Forest (84.64%) and Decision Tree (84.265%) lead in recall.\nWorst: SVM-SVC (49.735%).\n\n\nF-Score\nBest: Random Forest (85.745%) slightly edges out Decision Tree (85.285%).\nWorst: SVM-SVC (38.71%).\n\n\nFalse Alarm (FPR)\nBest: Random Forest (12.065%) and Decision Tree (12.75%) maintain low false positive rates.\nWorst: SVM-SVC (54.385%) is the highest.\n\n\nExecution Time\nFastest: Decision Tree (1 second) and Na√Øve Bayes (1 second).\nSlowest: SVM-SVC (3600 seconds), followed by MLP (2115 seconds).\n\n\n\nImpact of Dataset CharacteristicsDimensionality:TON_IoT likely involves high-dimensional data due to its IoT nature, affecting algorithm performance:\n\nRandom Forest and Decision Tree, with their ability to handle high-dimensional datasets and irrelevant features, perform well.\nSVM-SVC, which struggles with large datasets, reflects this limitation in its performance.\n\nNoise and Outliers:IoT datasets often include noisy data:\n\nRandom Forest and Decision Tree mitigate noise with feature averaging and robust splitting.\nMLP may struggle with noise, requiring substantial preprocessing for optimal results.\nNa√Øve Bayes, assuming feature independence, may be sensitive to outliers, though its simplicity helps in certain cases.\n\nClass Imbalance:IoT datasets often exhibit class imbalance (e.g., rare attack types vs. normal behavior). Algorithms handling imbalance:\n\nRandom Forest and Decision Tree, using ensemble learning and weighted splits, manage imbalance better.\nLogistic Regression and Na√Øve Bayes may underperform in imbalanced scenarios without appropriate resampling techniques.\nSVM-SVC is particularly sensitive to imbalance, resulting in poor metrics.\n\nConclusionIn conclusion, the strengths and weaknesses of each algorithm are influenced by their underlying assumptions, model complexity, and adaptability to the dataset. For simple problems or scenarios requiring high efficiency, Decision Tree or Logistic Regression is recommended. Random Forest and MLP perform better on complex datasets but come with higher computational costs. SVM-SVC is well-suited for high-dimensional, nonlinear data, while Na√Øve Bayes can be effective for specific tasks such as text classification.\nReference\nAlsaedi, Abdullah, et al. ‚ÄúTON_IoT Telemetry Dataset: A New Generation Dataset of IoT and IIoT for Data-Driven Intrusion Detection Systems.‚Äù IEEE Access, 2020, pp. 1‚Äì1, https://doi.org/10.1109/access.2020.3022862. \nGao, X., Shan, C., Hu, C., Niu, Z. and Liu, Z., 2019. An adaptive ensemble machine learning model for intrusion detection. Ieee Access, 7, pp.82512-82521. \nScikit-learn.org. (2019). sklearn.tree.DecisionTreeClassifier ‚Äî scikit-learn 0.22.1 documentation. [online] Available at: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html. \nscikit-learn (2014). sklearn.linear_model.LogisticRegression ‚Äî scikit-learn 0.21.2 documentation. [online] Scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html. \nscikit-learn (n.d.). sklearn.naive_bayes.GaussianNB ‚Äî scikit-learn 0.22.1 documentation. [online] scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html. \nScikit-learn.org. (2018). sklearn.ensemble.RandomForestClassifier ‚Äî scikit-learn 0.20.3 documentation. [online] Available at: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html. \nscikit-learn (2019). sklearn.svm.SVC ‚Äî scikit-learn 0.22 documentation. [online] Scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html. \nscikit-learn (2010). sklearn.neural_network.MLPClassifier ‚Äî scikit-learn 0.20.3 documentation. [online] Scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html.\nSciKit Learn (2019). sklearn.model_selection.GridSearchCV ‚Äî scikit-learn 0.22 Documentation. [online] Scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html. \nwww.evidentlyai.com. (n.d.). How to interpret a confusion matrix for a machine learning model. [online] Available at: https://www.evidentlyai.com/classification-metrics/confusion-matrix.\nwww.evidentlyai.com. (n.d.). Accuracy vs. precision vs. recall in machine learning: what‚Äôs the difference? [online] Available at: https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall#what-is-recall.\n\n","categories":["ai","foundation","classification algorithms"],"tags":["ai"]},{"title":"A Taxonomy and Terminology of Adversarial Machine Learning","url":"/2025/06/21/ai/algorithm/03-adversarial-machine-learning/","content":"BackgroundMachine learning (ML) components are increasingly being deployed in critical applications, form computer vision to cybersecurity. However, the data-driven nature of ML introduces new security challenges compared to traditional knowledge-based AI systems. Adversaries can exploit vulnerabilities in ML models through a variety of adversarial attacks, posing significant risks to the integrity, availability, and confidentiality of these systems.\nKey Attack TypesThe taxonomy of adversarial machine learning (AML) attacks identifies several important attack types, Data Access Attacks, Poisoning Attacks, Evasion Attacks, Oracle Attacks and Membership Inference Attacks. These adversarial attacks on machine learning models can affect the both training and testing stages.\nTraining StageData Access AttacksAdversaries can gain unauthorized access to the training data and manipulate the data to create a substitute model, which can be used to test and develop effective adversarial examples to attack the original target model.\nPoisoning AttacksPoisoning attacks, also know as causative attacks, are a type of adversarial attack against machine learning systems where the adversary alters the training data or model directly or indirectly to degrade the performance of the target model. This can be done through indirect poisoning, where the adversary poisons the data before pre-processing, or direct poisoning, which involves data injection, data manipulation (label or input), or logic corruption. Poisoning attacks target the training phase of the machine learning pipeline, aiming to compromise the integrity of the model by exploiting vulnerabilities in the learning process.\nTesting StageEvasion AttacksEvasion attacks are adversarial attacks that occur during the testing phase of a machine learning system, where inputs are manipulated to evade correct classification by the model. The attacker uses optimization techniques to find small perturbations that cause significant misclassification. Common algorithms for these attacks include L-BFGS, FGSM, and JSMA, which require knowledge of the target or substitute model to compute gradients.\nOracle AttacksIn Oracle Attacks, an adversary uses an API to observe model inputs and outputs, allowing them to train a substitute model similar to the target model. This substitute model can then be used to generate adversarial examples for Evasion Attacks. Oracle Attacks include Extraction Attacks, which extract model parameters, and Inversion Attacks, which reconstruct training data, potentially compromising privacy.\nMembership Inference AttacksThese attacks involve determining whether a particular data point was used to train the machine learning model.\nDefense MechanismsTo mitigate the risks posed by adversarial attacks, researchers have developed several defense mechanisms, Data Encryption and Sanitization, Robust Statistics, Adversarial Training, Gradient Masking, and Differential Privacy. These defense mechanisms aim to improve the overall security and assurance of ML components, making them more robust, resilient, and secure against a range of adversarial attacks.\n\n\n\n\nDefense Mechanisms\nInstructions\n\n\n\n\nData Encryption and Sanitization\nEncrypting training data and sanitizing it to remove potential malicious samples can help protect against data access and poisoning attacks.\n\n\nRobust Statistics\nEmploying robust statistical techniques during model training can improve the model‚Äôs resilience to noisy or adversarial training data.\n\n\nAdversarial Training\nIncorporating adversarial examples into the training process can improve the model‚Äôs robustness to evasion attacks.\n\n\nGradient Masking\nObfuscating the gradients used in the optimization process can make it more difficult for adversaries to craft effective adversarial examples.\n\n\nDifferential Privacy\nApplying differential privacy techniques can help protect the privacy of the training data and mitigate membership inference attacks.\n\n\n\n\nConsequences of Adversarial AttacksThe consequences of successful adversarial attacks on ML models can be severe, leading to integrity, availability, and confidentiality violations:\n\nIntegrity Violations: Adversarial attacks can cause ML models to misclassify inputs, leading to reduced confidence in the model‚Äôs outputs or targeted misclassifications.\nAvailability Violations: Adversarial attacks can disrupt the normal operation of ML models, rendering them unusable or unavailable for their intended purposes.\nConfidentiality Violations: Adversarial attacks can lead to the extraction of sensitive information about the target ML model, such as its architecture, parameters, or training data, potentially resulting in privacy breaches.These consequences can have significant impacts on the applications and systems that rely on the security and trustworthiness of ML components, underscoring the importance of developing robust defense mechanisms to mitigate the risks of adversarial attacks.\n\n","categories":["ai","foundation","adversarial-machine-learning"],"tags":["ai"]},{"title":"Physical layer","url":"/2025/07/11/computer_basic/network/01-physical-layer/","content":"Physical LayerThe physical layer is the lowest layer in the OSI reference model. It is responsible for the transmission of raw bits over a physical medium, defining the electrical, optical, or wireless signaling that carries the data. Its functions include bit-level transmission, modulation, synchronization, and dealing with noise and interference.\n1. Basic Concepts of the Physical Layer\nDefines the hardware means of sending and receiving data (cables, switches, radios, etc.).  \nTransmits data in the form of electrical signals, light pulses, or radio waves.  \nProvides data rate, bit representation, and signal transmission mode (simplex, half-duplex, full-duplex).  \n\n2. Fundamentals of Data CommunicationData communication at the physical layer is based on converting digital data into signals that can travel over the medium.Key concepts include:  \n\nAnalog vs Digital Signals: Continuous vs discrete representation of data.  \nBandwidth: The capacity of a channel, usually measured in Hz.  \nNoise: Unwanted signals that affect transmission quality.  \nData Rate: The speed at which data is transmitted, usually measured in bps (bits per second).  \n\n3. Nyquist Theorem and Shannon Capacity\nNyquist Theorem: In a noiseless channel with bandwidth B, the maximum data rate is 2B log2(M) bits per second, where M is the number of discrete signal levels.  \nShannon Capacity: In a channel with bandwidth B and signal-to-noise ratio (SNR), the maximum data rate is C = B log2(1 + SNR).These formulas establish the theoretical limits of data transmission.  \n\n4. Transmission MediaPhysical media can be categorized into guided and unguided transmission:  \n\nGuided Media  \n\nTwisted Pair Cable: Inexpensive, widely used in LANs.  \nCoaxial Cable: Better shielding, used for cable networks.  \nOptical Fiber: High bandwidth, long-distance, immune to EMI.  \n\n\nUnguided Media  \n\nRadio Frequency (RF): Common in Wi-Fi and Bluetooth.  \nMicrowave: Point-to-point communication, used in backbone links.  \nSatellite Communication: Long-range global coverage.  \n\n\n\nPhysical Layer Protocols\nElectrical Medium Access Control (MAC): Manages reliable physical transmission.  \nDigital Signal Processing (DSP): Converts bits into signals and vice versa.  \nOptical Fiber Communications (OFC): Transmission over fiber optics.  \nRadio Frequency Communications (RF): Wireless data transmission.  \n\nPhysical Layer Standards\nIEEE 802.3 (Ethernet)  \nIEEE 802.11 (Wi-Fi)  \nIEEE 802.15.1 (Bluetooth)  \n\nPhysical Layer Technologies\nFrequency-Division Multiplexing (FDM)  \nTime-Division Multiplexing (TDM)  \nCode-Division Multiple Access (CDMA)  \nOrthogonal Frequency-Division Multiplexing (OFDM)  \nSingle-Carrier Frequency-Division Multiplexing (SC-FDM)  \n\n","categories":["computer-fundation","network"],"tags":["network"]},{"title":"Physical layer","url":"/2025/07/11/computer_basic/network/02-data-link-layer/","content":"Physical Layer OverviewThe physical layer is the lowest layer in the OSI model. It is responsible for the actual transmission of raw bits across the physical medium. It defines hardware specifications, signaling methods, transmission rates, and physical connections. In short, the physical layer converts digital data into signals suitable for the transmission medium.\nBasic Concepts of the Network LayerAlthough the physical layer is the foundation, it works closely with the network layer above it. The physical layer ensures the integrity of raw data transfer, while the network layer focuses on logical addressing and routing. Together, they enable end-to-end communication.\nFundamentals of Data CommunicationData communication involves five key components:  \n\nSender ‚Äì the source of data.  \nMessage ‚Äì the information to be transmitted.  \nTransmission Medium ‚Äì the physical path (cable, fiber, wireless).  \nReceiver ‚Äì the destination of the message.  \nProtocol ‚Äì the rules governing communication.  \n\nPerformance is evaluated by bandwidth, latency, throughput, and error rate.\nTransmission TheoriesNyquist TheoremThe Nyquist criterion states that a noiseless channel with bandwidth B Hz can transmit at most 2B symbols per second. If each symbol carries k bits, then maximum data rate is:[R = 2B \\log_2(M)]where M is the number of discrete signal levels.\nShannon Capacity FormulaThe Shannon formula gives the maximum channel capacity considering noise:[C = B \\log_2 (1 + \\text{SNR})]where B is bandwidth and SNR is signal-to-noise ratio.This provides the theoretical upper bound for reliable communication.\nPhysical Transmission Media\nTwisted Pair Cable: Cheap, widely used, but limited in bandwidth and distance.  \nCoaxial Cable: Better shielding, supports higher frequencies.  \nOptical Fiber: High bandwidth, immune to electromagnetic interference, long-distance capability.  \nWireless Transmission: Uses radio, microwave, or infrared signals. Suitable for mobility.  \n\nMultiplexing TechniquesFrequency-Division Multiplexing (FDM)Divides the available frequency spectrum into non-overlapping bands, each carrying a separate signal. Used in analog broadcasting and cable TV.\nTime-Division Multiplexing (TDM)Allocates different time slots to multiple signals on the same channel. Widely used in digital telephony.\nCode-Division Multiplexing (CDM / CDMA)Assigns unique codes to each signal. All signals share the same frequency spectrum but remain distinguishable via orthogonal codes.\nDigital Transmission SystemsDigital transmission converts data into discrete signals. It includes:  \n\nBaseband Transmission: Direct transmission of digital signals.  \nBroadband Transmission: Modulated signals using carriers (e.g., DSL).  \nLine Coding: Methods such as NRZ, Manchester, and Differential Manchester.  \n\nBroadband Access TechnologiesBroadband access provides high-speed Internet connectivity. Common methods include:  \n\nDSL (Digital Subscriber Line)  \nCable Modem  \nFTTH (Fiber to the Home)  \nWireless Broadband (WiMAX, LTE, 5G)  \n\nEach technology balances speed, coverage, and cost.\nPhysical Layer Standards\nIEEE 802.3 (Ethernet): Defines wired LAN physical layer.  \nIEEE 802.11 (Wi-Fi): Defines wireless LAN.  \nIEEE 802.15.1 (Bluetooth): Defines short-range wireless links.  \n\n","categories":["computer-fundation","network"],"tags":["network"]},{"title":"Network layer","url":"/2025/07/19/computer_basic/network/03-network-layer/","content":"Network layerThe network layer is responsible for delivering packets from the source host to the destination host across multiple networks. Its main functions include logical addressing, routing, forwarding, and error handling. It also supports different service models such as virtual circuit and datagram service.\nNetwork ServicesVirtual Circuit vs Datagram Service\nVirtual Circuit: A pre-established path between source and destination. It provides reliability and order but requires setup overhead.  \nDatagram Service: Each packet is routed independently, without setup. It is flexible and scalable but may cause out-of-order delivery.\n\nVirtual NetworkA virtual network abstracts the underlying physical network into a logical topology. Technologies like VLAN, VPN, and overlay networks enable segmentation, isolation, and flexible resource allocation.\nIP AddressingIP AddressAn IP address uniquely identifies a host in a network. IPv4 addresses are 32-bit, typically written in dotted-decimal notation.\nClasses of IPv4\nClass A: 1.0.0.0 ‚Äì 126.255.255.255 (Default mask: 255.0.0.0)  \nClass B: 128.0.0.0 ‚Äì 191.255.255.255 (Default mask: 255.255.0.0)  \nClass C: 192.0.0.0 ‚Äì 223.255.255.255 (Default mask: 255.255.255.0)  \nClass D: 224.0.0.0 ‚Äì 239.255.255.255 (Multicast)  \nClass E: 240.0.0.0 ‚Äì 255.255.255.255 (Reserved for research)\n\nSubnettingSubnetting divides a network into smaller networks by borrowing bits from the host portion. It improves address utilization and enables better network management.\nSupernettingSupernetting (CIDR aggregation) combines multiple contiguous subnets into a larger block, reducing routing table entries.\nPacket HandlingPacket ForwardingRouters use destination IP addresses and routing tables to forward packets. This involves:  \n\nChecking the routing table  \nDetermining the next hop  \nUpdating headers and sending the packet  \n\nARP (Address Resolution Protocol)\nWorking: ARP maps IP addresses to MAC addresses within a local network.  \nARP Spoofing: An attacker forges ARP messages to intercept or redirect traffic.\n\nPacket AnalysisPackets consist of headers and payloads. The IP header includes source/destination IP, TTL, protocol type, and checksum for error detection.\nRoutingStatic RoutingManually configured routes. Simple but lacks scalability.\nGatewaysGateways connect networks with different protocols or architectures, enabling interoperability.\nDynamic Routing\nRIP (Routing Information Protocol): Distance-vector protocol, uses hop count as metric.  \nOSPF (Open Shortest Path First): Link-state protocol, uses Dijkstra‚Äôs algorithm.  \nBGP (Border Gateway Protocol): Path-vector protocol, used for inter-domain routing across the Internet.\n\nNetwork FunctionsLoad BalancingDistributes network traffic across multiple servers or links to improve reliability and performance.\nICMPThe Internet Control Message Protocol supports error reporting and diagnostics.  \n\nPing: Tests connectivity by sending ICMP Echo requests.  \nPathping / Traceroute: Tracks the path and delay of packets across routers.\n\nProxy Control via MACAccess control can be enforced at the proxy server by binding MAC addresses, preventing unauthorized clients.\nNetwork Address Translation\nNAT (Network Address Translation): Translates private IP addresses to public ones.  \nPAT (Port Address Translation): Maps multiple private addresses to a single public IP using different port numbers.  \n\nVPN (Virtual Private Network)A VPN establishes an encrypted tunnel over the Internet, ensuring confidentiality and secure access to private networks.\n","categories":["computer-fundation","network"],"tags":["network"]},{"title":"Kubernetes Basic Info","url":"/2025/06/03/container/k8s/01-basic-info/","content":"Installationinstallation\nConceptsconcepts\n","categories":["k8s","basic"],"tags":["k8s"]},{"title":"Port-Forward, Envoy Proxy, and Ingress","url":"/2025/06/03/container/k8s/02-networking/","content":"Understanding Kubernetes Networking: Port-Forward, Envoy Proxy, and IngressKubernetes (K8s) has revolutionized how we deploy, manage, and scale applications. However, understanding how network traffic flows inside and outside the cluster can still be challenging for many developers.In this post, we‚Äôll explore three important concepts in Kubernetes networking ‚Äî Port-Forward, Envoy Proxy, and Ingress ‚Äî and understand how they fit together to enable secure and efficient communication in a Kubernetes environment.\nPort-Forward ‚Äî Quick Access to Pods for DebuggingPort forwarding is the simplest way to access an application running inside a Kubernetes cluster from your local machine.It allows you to temporarily map a local port to a port inside a Pod, bypassing the need for complex networking configurations.\nExamplekubectl port-forward pod/my-app-pod 8080:80\nThis command forwards traffic from your local port 8080 to port 80 of the my-app-pod.You can now access the application locally using:http://localhost:8080\nWhen to Use\nDebugging or testing a service inside the cluster.\nAccessing a Pod that doesn‚Äôt expose a Service or Ingress.\nTemporary local development access.\n\nLimitations\nPort-forward is not suitable for production.\nIt only works for local, one-off connections.\nIt can become inefficient when multiple Pods or replicas are involved.\n\nEnvoy Proxy ‚Äî The Modern Cloud-Native ProxyEnvoy is a high-performance, open-source edge and service proxy originally developed by Lyft.It plays a critical role in many service mesh architectures, such as Istio, and is designed to handle observability, load balancing, and security between microservices.\nKey Features\nLayer 7 (HTTP) load balancing\nTraffic routing and retries\nCircuit breaking and rate limiting\nmTLS (mutual TLS) encryption\nDetailed metrics and tracing\n\nIn Kubernetes, Envoy typically runs as a sidecar container alongside your application Pod.This pattern intercepts all inbound and outbound traffic, enabling fine-grained control over how services communicate.\nExample (Sidecar Setup)A Pod definition with Envoy sidecar might look like this:apiVersion: v1kind: Podmetadata:  name: my-custom-websitespec:  containers:  - name: my-custom-pod    image: localhost:5000/node-web    ports:    - name: http      containerPort: 8080  - name: envoy    image: luksa/kubia-ssl-proxy:1.0    ports:    - name: https      containerPort: 8443    - name: admin      containerPort: 9901\nUse Cases\nImplementing a service mesh (e.g., Istio, Consul, or Linkerd).\nSecuring inter-service communication with mTLS.\nRouting and observability in microservice architectures.\n\nIngress ‚Äî Managing External Access to Your ServicesWhile Port-Forward and Envoy handle internal traffic, Ingress is how external clients reach your Kubernetes services.An Ingress is an API object that manages external HTTP and HTTPS access to services inside a cluster.It acts as an entry point, usually backed by an Ingress Controller such as NGINX, Traefik, or Envoy Gateway.\nExampleapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: my-app-ingressspec:  rules:  - host: myapp.example.com    http:      paths:      - path: /        pathType: Prefix        backend:          service:            name: my-app-service            port:              number: 80\nWith this configuration, any traffic to http://myapp.example.com will be routed to the Kubernetes Service my-app-service on port 80.\nBenefits\nCentralized control for routing and SSL termination.\nSimplifies managing multiple services under one domain.\nCan integrate with external load balancers and DNS.\n\nThings to Note\nYou must install an Ingress Controller (the Ingress resource alone doesn‚Äôt route traffic).\nConfiguration and annotations vary between controllers.\n\nComparison\n\n\n\nComponent\nPurpose\nTypical Use Case\n\n\n\n\nPort-Forward\nLocal access to a Pod\nDebugging, testing\n\n\nEnvoy Proxy\nService-to-service communication\nService mesh, mTLS, observability\n\n\nIngress\nExternal access routing\nExpose apps to the internet\n\n\n\n\nEach of these components operates at a different layer of the Kubernetes networking stack:\n\nPort-Forward ‚Üí developer convenience\nEnvoy ‚Üí internal microservice communication\nIngress ‚Üí external user access\n\nTogether, they form the foundation of secure and efficient communication in modern Kubernetes deployments.\n","categories":["k8s","networking"],"tags":["k8s"]},{"title":"MongoDB Basic Information","url":"/2025/06/04/database/mongodb/01-install/","content":"MongoDB Basic InformationMongoDB is a popular NoSQL database known for its flexibility, scalability, and ease of use. It stores data in a JSON-like format called BSON (Binary JSON), which allows for dynamic schemas and makes it easy to work with complex data structures.\nKey Features of MongoDB\nDocument-Oriented Storage: MongoDB stores data in collections of documents, which can have varying structures. This allows for greater flexibility compared to traditional relational databases.\nScalability: MongoDB supports horizontal scaling through sharding, allowing it to handle large volumes of data and high traffic loads.\nRich Query Language: MongoDB provides a powerful query language that supports a wide range of operations, including filtering, sorting, and aggregating data.\nIndexing: MongoDB supports various types of indexes to improve query performance, including single-field, compound, geospatial, and text indexes.\nReplication: MongoDB supports replica sets, which provide high availability and data redundancy by maintaining multiple copies of data across different servers.\nAggregation Framework: MongoDB includes a robust aggregation framework that allows for complex data processing and transformation operations.\nFlexible Schema: MongoDB‚Äôs schema-less design allows for easy modifications to the data structure without requiring downtime or complex migrations.\n\nDocumentationOfficial Documentation\nInstallationInstallation\n","categories":["database","mongodb"],"tags":["mongodb"]},{"title":"Replication Support in MongoDB","url":"/2025/06/04/database/mongodb/02-replication/","content":"OverviewMongoDB is a document-oriented NoSQL database that stores data in flexible, JSON-like documents (Wikipedia Contributors 2019). Unlike traditional relational databases, MongoDB does not require a predefined schema, allowing for dynamic and hierarchical data structures. This flexibility makes it particularly suitable for applications that handle diverse and evolving data formats, such as content management systems, real-time analytics platforms, and Internet of Things (IoT) applications. Its scalability and performance are further enhanced by features like horizontal scaling and high availability.\nReplication SupportMongoDB implements replication through a feature called replica sets. A replica set consists of multiple MongoDB instances (nodes) that maintain the same dataset, providing redundancy and high availability (Team 2025). In a typical replica set, one node is designated as the primary, while the others serve as secondaries. The primary node handles all write operations, and the secondaries replicate the primary‚Äôs data asynchronously. If the primary node fails, an automatic election process promotes one of the secondaries to become the new primary, ensuring continuous availability.\nTo set up replication, MongoDB requires a minimum of three nodes: one primary and two secondaries (ActiveLobby Team 2021). Details in following picture. This configuration ensures fault tolerance and allows for automatic failover. Additionally, MongoDB supports the use of arbiters‚Äînodes that participate in elections but do not hold data‚Äîto maintain an odd number of voting members, which is essential for achieving consensus during elections.\nImplications for Application DevelopmentWhen developing applications that interact with a replicated MongoDB setup, several considerations come into play:\n\nRead and Write Operations: By default, all write operations are directed to the primary node to maintain data consistency (Ishita Srivastava 2024). However, applications can be configured to read from secondary nodes to distribute the read load and improve performance. This approach, while beneficial for scalability, may introduce eventual consistency, as secondaries replicate data asynchronously.\nHandling Failover: Applications should be designed to handle automatic failovers gracefully. This involves configuring the MongoDB drivers to detect changes in the primary node and reroute operations accordingly. Proper error handling and retry mechanisms are essential to ensure seamless operation during node transitions.\nData Consistency: Developers must be aware of the consistency model of MongoDB. While reading from the primary ensures strong consistency, reading from secondaries may result in stale data due to replication lag. Applications that require up-to-date information should enforce read preferences accordingly.\nPerformance Optimization: Leveraging secondary nodes for read operations can enhance performance, especially in read-heavy applications. Additionally, implementing caching strategies can reduce the load on the database and improve response times.\n\nIn general, while MongoDB‚Äôs replication features offer significant benefits in terms of availability and scalability, they also introduce complexities that developers must address. Understanding the replication mechanics and designing applications with these considerations in mind is crucial for building robust and efficient systems.\nDeploy MongoDB Replica Set In KubernetesTo deploy a MongoDB replica set in a Kubernetes environment, you can follow these general steps:\n\nCreate ConfigMaps and Secrets: Store MongoDB configuration files and sensitive information like passwords using ConfigMaps and Secrets.\nDefine Persistent Volume Claims (PVCs): Set up PVCs to ensure data persistence for each MongoDB pod.\nCreate a StatefulSet: Use a StatefulSet to manage the MongoDB pods, ensuring that each pod has a stable network identity and persistent storage.\n\nPrerequisites:\n\nCreate openssl-cert secret for internal authentication between replica set members.\n\nsudo bash -c &quot;openssl rand -base64 756 &gt; mongodb-keyfile&quot;sudo chmod 400 mongodb-keyfilekubectl create secret generic keyfile --from-file=mongodb-keyfile\nrepositorykubectl apply -f storageclass.yamlkubectl apply -f db-configMap.yamlkubectl apply -f db-secret.yamlkubectl apply -f db-service.yamlkubectl apply -f db-statefulSet.yaml\nInitial ReplicaSetkubectl exec -it mongodb-rs-0 -- mongosh\nuse adminrs.initiate(&#123;  _id: &quot;rs0&quot;,  members: [    &#123; _id: 0, host: &quot;mongodb-0.mongodb-service:27017&quot;, priority: 2 &#125;,    &#123; _id: 1, host: &quot;mongodb-1.mongodb-service:27017&quot;, priority: 1 &#125;,    &#123; _id: 2, host: &quot;mongodb-2.mongodb-service:27017&quot;, priority: 1 &#125;  ]&#125;)\nReferencing[1] ActiveLobby Team 2021, 5 Steps to Configure MongoDB Replication: A Complete Guide, Activelobby Support, viewed 4 June 2025, available: https://blog.supportlobby.com/yourdomain-com-mongodb-replication-guide/ , accessed 4 June 2025.[2] Ishita Srivastava 2024, Understanding Replication and High Availability in MongoDB | Code by Zeba Academy, Code by Zeba Academy, available: https://code.zeba.academy/replication-availability-mongodb/, accessed 4 June 2025.[3] Team, MD 2025, Replication, Mongodb.com, viewed 4 June 2025, available: https://www.mongodb.com/docs/v7.0/replication. accessed 4 June 2025.[4] WebHi 2025, MongoDB sharding and replication guide - Tutorial &amp; Documentation, Tutorial &amp; Documentation, viewed 4 June 2025, available: https://www.webhi.com/how-to/mongodb-sharding-and-replication-guide/, accessed 4 June 2025.[5] Wikipedia Contributors 2019, MongoDB, Wikipedia, Wikimedia Foundation, available: https://en.wikipedia.org/wiki/MongoDB, accessed 4 June 2025.\n","categories":["database","mongodb"],"tags":["mongodb"]},{"title":"Hello Prisma","url":"/2025/10/21/nodejs/prisma/01-hello-prisma/","content":"Connecting Prisma application to Supabase PostgresCreste a custom user fro Prismacreate a Prisma DB user with full privileges on the public schema-- Create custom usercreate user &quot;prisma&quot; with password &#x27;custom_password&#x27; bypassrls createdb;-- extend prisma&#x27;s privileges to postgres (necessary to view changes in Dashboard)grant &quot;prisma&quot; to &quot;postgres&quot;;-- Grant it necessary permissions over the relevant schemas (public)grant usage on schema public to prisma;grant create on schema public to prisma;grant all on all tables in schema public to prisma;grant all on all routines in schema public to prisma;grant all on all sequences in schema public to prisma;alter default privileges for role postgres in schema public grant all on tables to prisma;alter default privileges for role postgres in schema public grant all on routines to prisma;alter default privileges for role postgres in schema public grant all on sequences to prisma;check privilegesSELECT usename, usecreatedb, usebypassrls FROM pg_user;\nalter prisma password if neededalter user &quot;prisma&quot; with password &#x27;new_password&#x27;;\nCreate a Prisma Projectmkdir hello-prismacd hello-prisma\nInitiate a new Prisma projectmkdir hello-prismacd hello-prismanpm init -ynpm install prisma typescript ts-node @types/node --save-devnpx tsc --initnpx prisma init\nAdd connection information to .env file\nDIRECT_URL is used for migrationsDATABASE_URL is used for connection pooling\nCreate migrationcreate new tables in prisma.shcema filemodel Post &#123;  id        Int     @id @default(autoincrement())  title     String  content   String?  published Boolean @default(false)  author    User?   @relation(fields: [authorId], references: [id])  authorId  Int?&#125;model User &#123;  id    Int     @id @default(autoincrement())  email String  @unique  name  String?  posts Post[]&#125;commit migrationnpx prisma migrate dev --name first_prisma_migration\nInstall the prisma clientnpm install @prisma/clientnpx prisma generate\n\nset schema.prismagenerator client &#123;  provider = &quot;prisma-client-js&quot;&#125;datasource db &#123;  provider = &quot;postgresql&quot;  url      = env(&quot;DATABASE_URL&quot;)&#125;model Post &#123;  id        Int     @id @default(autoincrement())  title     String  content   String?  published Boolean @default(false)  author    User?   @relation(fields: [authorId], references: [id])  authorId  Int?&#125;model User &#123;  id    Int     @id @default(autoincrement())  email String  @unique  name  String?  posts Post[]&#125;\nset prisma.config.tsimport &#123; defineConfig, env &#125; from &quot;prisma/config&quot;;import &quot;dotenv/config&quot;;export default defineConfig(&#123;  schema: &quot;prisma/schema.prisma&quot;,  migrations: &#123;    path: &quot;prisma/migrations&quot;,  &#125;,  engine: &quot;classic&quot;,  datasource: &#123;    url: env(&quot;DATABASE_URL&quot;),  &#125;,&#125;);\nset index.tsimport &#123; PrismaClient &#125; from &#x27;@prisma/client&#x27;;const prisma = new PrismaClient();async function main() &#123;    const val = await prisma.user.findMany(&#123;        take: 10,    &#125;);    console.log(val);&#125;main()    .then(async () =&gt; &#123;        await prisma.$disconnect();    &#125;)    .catch(async (e) =&gt; &#123;        console.error(e);        await prisma.$disconnect();        process.exit(1);    &#125;)\nInsert test data\nTest index.ts\nReferencereposupabase doc\n","categories":["nodejs","orm","prisma"],"tags":["prisma, supabase"]},{"title":"Github Action with Azure","url":"/2025/07/15/github/action/01-gaction-azure/","content":"Github Action with AzureIn this post, I will demonstrate a pipeline that is triggered only when code is pushed to the testing or main branch. The pipeline includes static code analysis, unit testing, and an image vulnerability scan. After these checks, the container image is deployed to Azure Kubernetes Service (AKS).\nProccess\nCode RepoCode Repo\nFeatures\nDev Backend Code checks - Code Analysis, Test\nUAT Backend CI - Test, Build, and Deploy to UAT\nPROD Backend CD - Deploy Backend Services to AKS\nPROD Frontend CI &amp; CD\n\nPrerequirements\nAn Azure account with a credit limit\nInstall Terraform\n\nStep 1: Initialize Production Env by Terraformcd terraform/production/terraform initterraform planterraform apply\n\nStep 2: Collect relevant informationGet Storage Account keyaz storage account keys list --resource-group chriswen430-rg --account-name chriswen430stg202507 --output table\nGet container registry infoaz acr show --name chriswen430acr202507 --resource-group chriswen430-rg --query id --output tsvaz acr show --name chriswen430acr202507 --resource-group chriswen430-rg  --output tableaz acr credential show --name chriswen430acr202507 --resource-group chriswen430-rg --query &quot;username&quot; -o tsv az acr credential show --name chriswen430acr202507 --resource-group chriswen430-rg --query &quot;passwords[0].value&quot; -o tsv\n\nCreate Service Principal for GitHub Actionaz ad sp create-for-rbac --name &quot;github-action-sp&quot; --role Owner  --scope /subscriptions/e43f0850-53ec-498f-a66c-b4178b6d37f6/resourceGroups/chriswen430-rg --sdk-auth\nStep 3: Configure GitHub Secrets\nStep 5: Test PipelineDev Backend Code checks - Code Analysis, Test\nFile Path: .github/workflows/dev-backend.yml\nMain Steps:\nStep 1: Code Quality Checks and Unit Tests\nStatic Code Analysis by flake8\nUnit Test\n\n\nStep 2: Build Image and Scan Vulnerabilities\nBuild Docker Image\nScan Image Vulnerabilities by Trivy\n\n\n\n\n\nUAT Backend CI - Test, Build, and Deploy to UAT\nFile Path: .github/workflows/uat-backend-ci.yml\nMain Steps:\nStep 1: Code Quality Checks and Unit Tests\nStep 2: Build Image and Scan Vulnerabilities\nStep 3: Create Simulated Env to Test Deployment\n\n\n\nPROD Backend CD - Deploy Backend Services to AKS\nFile Path: .github/workflows/prod-backend-cd.yml\nMain Steps:\nStep 1: Build and Push Image to ACR\nStep 2: Update Kubernetes Configuration with New Image\nStep 3: Deploy Backend Infrustructure\nStep 4: Deploy Backend Services\n\n\n\nPROD Frontend CI &amp; CD\nFile Path: .github/workflows/prod-frontend-ci&amp;cd.yml\nMain Steps:\nStep 1: Build and Push Image to ACR\nStep 2: Update Kubernetes Configuration with New Image\nStep 3: Deploy Frontend Service\n\n\n\nStep 6: Create Promethus and Grafana Dashboardsprerequisties: connect to AKS cluster\ncreate promethus and grafanakubectl apply -f prometheus-configmap.yaml -f prometheus-deployment.yaml -f prometheus-rbac.yaml kubectl apply -f grafana-deployment.yaml  \nSet Connections \nSet Metrics\n","categories":["github","github-action","azure"],"tags":["github-action"]},{"title":"Malware Review","url":"/2025/07/01/computer_basic/network/malware/01-malware-review-zeus&Fareit/","content":"Malware ReviewThis article introduces two major malware families, Zeus and Fareit. Zeus is a banking trojanthat steals online financial credentials through techniques such as keylogging and formgrabbing. Fareit is a credential-stealing trojan that collects passwords, usernames, and walletdata, often delivering other malware as well. The study outlines their impacts on target systemsand highlights key traffic patterns, providing insight into how these threats can be identifiedthrough network analysis.\nZeusZeus, also known as Zbot, Wsnpoem, Kneber botnet.\nImpactsZeus is a banking trojan designed to steal online banking credentials, credit card data, andlogin information by keylogging and man-in-the-browser attacks [1]. It can also downloadadditional malware, spread via phishing emails, and build botnets for cybercrime campaigns.\nDetection PatternsIndicators include unusual encrypted HTTP traffic to command-and-control (C2) servers,domain-generation algorithm (DGA) domains, suspicious form-grabbing behaviour in webtraffic, and registry changes to maintain persistence. Network captures often show repeatedPOST requests with encoded payloads.\n\nRepeated POST requests: Frequent small POST packets sent to the same IP/domain,possibly containing encrypted credentials.\nAbnormal User-Agent: For example, uncommon browser identifiers or missing standardfields.\nPlaintext credential leakage: HTTP POST requests containing values likeusername=xxx&amp;password=xxx.\nDGA domains: Domains consisting of random-looking strings, such as abdh12kq.xyz.\n\nFareitFareit, also called Pony or Pony Loader.\nImpactsFareit is a credential-stealing trojan that harvests usernames, passwords, and cryptocurrencywallet data from infected systems [2]. It is often used to install other malware, such asransomware or banking trojans. Attackers use it to exfiltrate data to remote C2 servers,enabling identity theft and financial fraud.\nDetection PatternsIndicators include large volumes of outbound POST requests with encoded data, attempts toconnect to hardcoded C2 IPs/domains, and sudden access to browser-stored credentials orWindows credential stores. In packet captures, Fareit traffic may stand out as frequent smallPOST requests containing credential dumps.\nReference[1] Wikipedia Contributors, ‚ÄúZeus (malware),‚Äù Wikipedia, Nov. 07, 2019.https://en.wikipedia.org/wiki/Zeus_(malware). Accessed July 7, 2025.[2] REXor, ‚ÄúPony | Fareit,‚Äù RexorVc0, 2024. https://rexorvc0.com/2024/02/04/Pony_Fareit/.Accessed July 7, 2025.\n","categories":["computer-fundation","network","malware"],"tags":["network"]},{"title":"DHCP Protocol & DHCP Spoofing","url":"/2025/07/02/computer_basic/network/malware/02-dhcp-protocol&dhcp-spoofing/","content":"DHCP Protocol &amp; DHCP SpoofingDHCP Protocol OverviewThe Dynamic Host Configuration Protocol (DHCP) is a network management protocol used onIP networks to automatically assign IP addresses and other configuration parameters to devicesusing a client-server model [1]. \nLayer: DHCP operates at the Application layer in the TCP/IP model and uses UDP as itstransport protocol [2].Ports: The server uses UDP port 67, and the client uses UDP port 68 [2].\nPrimary Function\n\nAutomatic IP Configuration: Assigns clients IP address, subnet mask, default gateway, DNS,lease duration. [2]\nCentralized Management: Simplifies network administration‚Äîclients need no manualnetwork setup. [2]\nLease Renewal: Clients automatically renew or release leases before expiration. [2]\n\nNormal DHCP Operation\nDHCP DISCOVER: Client broadcasts DHCPDISCOVER to 255.255.255.255:67.\nDHCP OFFER (Unicast/Broadcast): DHCP server(s) reply with DHCPOFFER containing anavailable IP and configuration parameters.\nDHCP REQUEST (Broadcast): Client selects one offer, broadcasts DHCPREQUEST to formallyrequest it.\nDHCP ACK (Unicast): Server sends DHCPACK confirming the lease and providing fullnetwork settings.\n\nDHCP Vulnerabilities Enabling SpoofingDHCP Spoofing is a cyberattack in which a malicious actor sets up a rogue DHCP server withina network to respond to DHCP requests with malicious or misleading configurations.\nThis is possible because the DHCP protocol lacks authentication mechanisms, trusting anyserver that replies to a DHCPDISCOVER message\nHow does DHCP Spoofing work?\nDeploy: The attacker connects a rogue DHCP server (e.g., a laptop, Raspberry Pi, orcompromised IoT device) to the local network.\nRace: When a client broadcasts a DHCPDISCOVER, both the legitimate and rogue serversrespond. The rogue server may win by responding faster or sending more aggressive offers.\nPoison: The rogue server provides malicious network settings in its DHCPOFFER, such as:a. Attacker‚Äôs IP as the default gatewayb. Malicious DNS server addressc. Invalid subnet or routing configuration\n\nImpactBy hijacking DHCP communications, the attacker can reroute network traffic through their owndevice, giving them full visibility into‚Äîand control over‚Äîthe data flow. They can also mount aDenial of Service by bombarding the legitimate DHCP server with bogus requests, depleting itspool of available IP addresses and blocking service to genuine clients. By tampering with DHCPmessages, the attacker effectively positions themselves as a man-in-the-middle, disruptingnormal traffic paths and paving the way for further malicious actions.\nPotential RiskUnderstanding the potential Risks of DHCP Spoofing is crucial for maintaining network security.Here are some risks associated with DHCP Spoofing:\n\nData Interception: Attackers can intercept sensitive data by positioning themselves as thedefault gateway or DNS server, allowing them to monitor and manipulate network traffic.[3]\nUnauthorized Network Access: Rogue DHCP servers can provide incorrect networkconfigurations, granting attackers unauthorized access to network resources and sensitiveinformation. [3]\nNetwork Disruption: DHCP Starvation attacks can deplete the DHCP server‚Äôs pool of IPaddresses, preventing legitimate devices from connecting to the network and causingsignificant disruption.[3]\n\nProtection Against DHCP SpoofingTo prevent DHCP spoofing attacks, networks can implement the following security measures:\n\nDHCP Snooping: Configure network switches to only allow DHCP responses fromtrusted ports.\nPort Security: Limit the number of MAC addresses per switch port and bind specificdevices to specific ports.\nVLAN Segmentation: Separate sensitive devices and services into different VLANs toisolate DHCP traffic.\nIP Source Guard: Used with DHCP snooping to verify IP-to-MAC bindings and blockspoofed packets.\n\nReference[1] Wikipedia Contributors, ‚ÄúDynamic Host Configuration Protocol,‚Äù Wikipedia, Oct. 07, 2019.[online]. https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol. (accessed July 2,2025)[2] GeeksforGeeks, ‚ÄúDynamic Host Configuration Protocol (DHCP),‚Äù GeeksforGeeks, Jan. 03,\n\n[online]. https://www.geeksforgeeks.org/computer-networks/dynamic-hostconfiguration-protocol-dhcp/. (accessed July 2, 2025)[3] Twingate, ‚ÄúWhat is DHCP Spoofing? How It Works &amp; Examples | Twingate,‚Äù Twingate.com,\n[online]. https://www.twingate.com/blog/glossary/dhcp%20spoofing. (accessed July 2,2025)\n\n","categories":["computer-fundation","network","malware"],"tags":["network"]},{"title":"DHCP Protocol & DHCP Spoofing Experiment","url":"/2025/07/02/computer_basic/network/malware/03-dhcp-experiment/","content":"DHCP Protocol &amp; DHCP SpoofingObjectiveThis experiment aims to observe abnormal behavior in the DHCP protocol by building a simpleLAN topology using Mininet and launching a DHCP spoofing attack with DHCPig [1].\nEnvironment\n\n\n\nComponent Name\nVersion\n\n\n\n\nUbuntu\n18.04.1\n\n\nMininet\n2.3.1b4\n\n\nDHCPig\n-\n\n\nisc-dhcp-server[2]\n-\n\n\nPython3\n3.6.9\n\n\n\n\nNetwork Topology\nIllustration:\n\nThe host 1 will act as the DHCP server;\nThe host 2 will act as the client;\nThe host 3 will act as the attacker.\n\nExperimental Steps\nInstall dependency.\nsudo apt install isc-dhcp-servergit clone https://github.com/kamorin/DHCPig.git\n\n\nStart miniedit via sudo python3 miniedit/examples/miniedit.py.\n\nCreate topology and click Run icon.Host 1Host 2Host 3\n\nConfigure DHCP server in h1 terminalUpdate /etc/dhcp/dhcpd.conf file, set IP range 10.0.0.100-10.0.0.200Update /etc/default/isc-dhcp-server file, assigning the interface h1-eth0\n\nStart the DHCP server via sudo dhcpd -4 -cf /etc/dhcp/dhcpd.conf h1-eth0\n\nIn h2 terminal, check h2-eth0 and request a new IP via dhclient h2-eth0Check IP info in h2 terminalRequest a new IP\n\nWe can see that h2-eth0 have two IP: 10.0.0.2/8 and 10.0.0.100/24 (this IP is assigned from DHCP server)\n\n\n\n\n\n\n\n\nClear IP information of h2-eth0 via dhclient -r h2-eth0.We can see that all the IP information has been cleared.\n\n\nLaunch the DHCP spoofing attack in the h3 terminal and wait around 10 seconds, or until all available IP addresses are exhausted.sudo python3 pig.py h3-eth0We can see that h3 (10.0.0.3) continuously sends DHCP Discover messages to h1 (10.0.0.1) and receives IP addresses in response.Finally, the DHCP pool exhausted.\n\nRepeat the step 6Although the DHCP server is running, but h2 can not get any response from the DHCP server.\n\n\nResults and AnalysisDHCP protocolBy capturing the DHCP traffic, we can observe the standard four-step DHCP handshake between h2 (client) and h1 (server).First, h2 broadcasts a DHCP Discover message to locate available DHCP servers on the network.In response, h1 sends a DHCP Offer message containing an available IP address and configuration options (such as subnet mask and lease time).Next, h2 replies with a DHCP Request message, indicating its intent to accept the offered IP and explicitly requesting it. This message also includes the identifier of the DHCP server.Finally, h1 responds with a DHCP ACK, confirming the IP lease and providing final configuration parameters (such as router, DNS server, and lease duration).\n\nDHCP SpoofingAfter conducting this experiment, we observed that before h3 initiates the DHCP spoofing attack, h2 is able to obtain IP address information from the DHCP server running on h1. However, once h3 begins the spoofing, it continuously sends DHCP Discover messages to h1 and receives available IP addresses in response. As a result, h2 can no longer obtain an IP address from the server. This causes serious disruption to the network, potentially leading to service unavailability, denial of network access, and overall degradation of network performance.\nConclusionThis experiment successfully demonstrated the DHCP Spoofing attack within a simulated network environment using Mininet. By configuring both a legitimate DHCP server and a rogue DHCP server, we observed how the lack of authentication in the DHCP protocol can be exploited by attackers to deliver malicious IP configurations. The attacker-controlled server was able to respond to DHCPDISCOVER messages faster than the legitimate server, thereby redirecting client traffic and enabling potential man-in-the-middle attacks or denial-of-service conditions.Through this setup, we gained hands-on understanding of:\n\nThe operation of the DHCP protocol within the TCP/IP application layer.\nThe vulnerability of unauthenticated DHCP message exchanges\nThe impact of rogue DHCP servers on network reliability and security.\n\nThis experiment highlights the importance of securing network infrastructure through techniques such as DHCP Snooping, port security, and VLAN segmentation to mitigate DHCP-based attacks in real-world networks.\nReference[1] K. Amorin, ‚Äúkamorin/DHCPig,‚Äù GitHub, May 18, 2024. [online]. https://github.com/kamorin/DHCPig.  Accessed July 2, 2025[2] I. S. Consortium, ‚ÄúISC DHCP,‚Äù [online]. www.isc.org. https://www.isc.org/dhcp/. Accessed July 2, 2025\n","categories":["computer-fundation","network","malware"],"tags":["network"]},{"title":"mininet","url":"/2025/07/16/computer_basic/network/mininet/01-mininet/","content":"What is Mininet?Mininet is a lightweight network emulator that creates a virtual network with hosts, switches, links, and controllers all on a single machine. It‚Äôs widely used in Software-Defined Networking (SDN) research and education.mininet\nKey Features\nLightweight and runs real code (like real Linux hosts)\nAllows fast prototyping of large networks\nIntegrates well with OpenFlow and SDN controllers like POX, Ryu, and ONOS\n\nInstallationmininet download\nBasic Concepts\nHost: virtual end device (eg, h1, h2)\nSwitch: OpenFlow-enabled switch (eg, s1)\nController: Manages forwarding rules (eg, POX, Ryu)\nLink: Virtual cable between nodes\n\nSimple CommandMininet Command StructureMininet commands fall into three categories:\n\nNetwork Startup Parameters ‚Äì ‚Äîtopo, ‚Äîcustom, ‚Äîswitch, ‚Äîcontroller, ‚Äîmac, etc.\nInteractive CLI Commands ‚Äì net, nodes, pingall, dump, iperf, etc.\nExternal Parameters ‚Äì -c (cleanup), -h (help), etc.\n\ncreate simple topologysudo mnThis launches the default topology (2 hosts, 1 switch, 1 controller)\nSingle Switch, Multiple Hostssudo mn --topo=single,5\nLinear Topology (Switches connected in a line)sudo mn --topo=linear,3sudo mn --topo=linear,3,2\nTree Topology (Hierarchical switches)sudo mn --topo=tree,depth=3,fanout=2\n\ndepth: Number of switch levelsfanout: Number of child switches/hosts per parent\n\nlist all nodesnodes\nshow connectionsnet\ntest connectivitypingall\nshow host IPh1 ifconfig\ntest ping between hostsh1 ping h2\nexit &amp; clean upsudo mn -c\nCustom Topology Example (Python Script)from mininet.topo import Topofrom mininet.net import Mininetfrom mininet.node import Controllerfrom mininet.cli import CLIfrom mininet.link import TCLinkclass MyTopo(Topo):    def build(self):        h1 = self.addHost(&#x27;h1&#x27;)        h2 = self.addHost(&#x27;h2&#x27;)        s1 = self.addSwitch(&#x27;s1&#x27;)        self.addLink(h1, s1)        self.addLink(h2, s1)if __name__ == &#x27;__main__&#x27;:    topo = MyTopo()    net = Mininet(topo=topo, controller=Controller)    net.start()    CLI(net)    net.stop()\nsudo python3 mytopo.py\n\n\nmore info: mininet\n\nConclusionMininet is an essential tool for learning and experimenting with SDN and virtual networking. It supports:\n\nReal Linux networking code\nPython scripting for custom topologies\nIntegration with external SDN controllers\nFast and scalable virtual testing\n\n","categories":["computer-fundation","network","mininet"],"tags":["network","mininet"]},{"title":"Descriptive Statistic for Continuous Data","url":"/2025/11/09/big_data/da/01-des-statistics/","content":"Descriptive Statistic fopr Continuous DataHistograms are based on binned data and hence provide us with snapshots of how much probability mass is allocated in diferent parts of the data domain.\nimport numpy as npincome = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/uk_income_simulated_2020.txt&quot;)b = [0, 10000, 20000, 30000, 40000, 50000, 60000, 80000, np.inf]  # bin boundsc = np.histogram(income, bins=b)[0]  # countsfor i in range(len(c)):    print(f&quot;&#123;b[i]:5&#125;-&#123;b[i+1]:5&#125;: &#123;c[i]:4&#125;&quot;)\nimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use(&quot;seaborn&quot;)sns.histplot(income)plt.show()\nMathematical NotationAssume that a sample to be aggregated with x\n\nx=(x1,x2,‚Ä¶,xn)we shall write \n\nx‚ààRnUsing the programming syntax,n corresponds to len(income) or equivalently income.shape[0] and  xi   is income[i-1] (because in Python the first element is at index 0).\nincome_sorted = np.sort(income)income_sorted[0], income_sorted[-1]  # the minimum and the maximum\nheights = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/nhanes_adult_female_height_2020.txt&quot;)heights_sorted = np.sort(heights)heights_sorted[0], heights_sorted[-1]sns.histplot(heights)plt.show()\nMeasures of LocationTwo main measures of central tendency are:\n\nthe arithmetic mean (sometimes for simplicity called the mean or average), is defined as the sum of all observations divided by the sample size:\n\n\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\nthe median, being the middle value in a sorted version of the sample if its length is odd, or the arithmetic mean of the two middle values otherwise:\n\nm =\n\\begin{cases}\nx_{\\frac{n+1}{2}}, & \\text{if } n \\text{ is odd} \\\\[6pt]\n\\dfrac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2}, & \\text{if } n \\text{ is even}\n\\end{cases}\n\nnp.mean(heights), np.median(heights)\nnp.mean(income), np.median(income)# The arithmetic mean is strongly influenced by very large or very small observationsincome2 = np.append(income, [1_000_000_000])np.mean(income2)\nFor symmetrical distributions with no outliers, the mean will be better as it uses all data (and its efficiency can be proven for certain statistical models).For skewed distributions, the median has a nice interpretation.\nnp.mean(heights), heights.mean(), np.sum(heights)/len(heights), heights.sum()/heights.shape[0]\nQuantilesQuantiles generalise the notion of the sample median. For any p between 0 and 1, a p-quantile, denoted  $q_p$  is a value dividing the sample in such a way that:\n\n100p% of observations are not greater than  $q_p$,\nthe remaining 100(1-p)% are not less than  $q_p$.\n\nQuantiles appear under many different names, but they all refer to the same concept. In particular, we can speak about 100p-th percentiles, e.g., the 0.5-quantile is the same as the 50th percentile.\n\n0-quantile ($q_0$) = the minimum (also: numpy.min),\n0.25-quantile ($q_0.25$) = the 1st quartile (denoted  $Q_1$),\n0.5-quantile ($q_0.5$) = the 2nd quartile a.k.a. median,\n0.75-quantile ($q_0.75$) = the 3rd quartile (denoted  $Q_3$),\n1-quantile ( $q_1$ ) = the maximum (also: numpy.max).\n\nnp.quantile(income, [0, 0.25, 0.5, 0.75, 1])np.quantile(heights, [0, 0.25, 0.5, 0.75, 1])\nMeasures of DispersionMeasures of central tendency quantify the location of the most typical value.\n\nthe standard deviation: being the average distance to the arithmetic mean\n\ns = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 }.\nthe interquartile range (IQR), being the difference between the 3rd and the 1st quartile:\n\nIQR = q_0.75 - q_0.25 = Q3 - Q1\n\nnp.std(income), np.quantile(income, 0.75)-np.quantile(income, 0.25)np.std(heights), np.quantile(heights, 0.75)-np.quantile(heights, 0.25)\nThe IQR has an appealing interpretation, because we may say that this is the range comprised of the 50% most typical values.\nThe standard deviation measures the average degree of spread around the arithmetic mean. Thus, it makes the most sense for data distributions that are symmetric around the mean. This measure is useful overall for making comparisons across different samples. However, without further assumptions, it‚Äôs quite difficult to express the meaning of a particular value of s \nBox (and Whisker) PlotsThe box and whisker plot (or the box plot for short) depicts some of the most noteworthy features of a data sample.\nplt.subplot(211)  # 2 rows, 1 column, 1st subplotsns.boxplot(data=income, orient=&quot;h&quot;)plt.title(&quot;income&quot;)plt.subplot(212)  # 2 rows, 1 column, 2nd subplotsns.boxplot(data=heights, orient=&quot;h&quot;)plt.title(&quot;heights&quot;)plt.show()\nEach box plot consists of:\n\nthe box, which spans between the 1st and the 3rd quartile:\nthe median is clearly marked by a vertical bar inside the box;\nnote that the width of the box corresponds to the IQR;\n\n\nthe whiskers, which span between\nthe smallest observation (the minimum) or  $Q_1 - 1.5IQR$ (the left side of the box minus 3/2 of its width), whichever is larger,\nthe largest observation (the maximum) or  $Q_3 + 1.5IQR$ (the right side of the box plus 3/2 of its width), whichever is smaller.\n\n\n\nAdditionally, all observations that are less than  $Q_1 - 1.5IQR$  (if any) or greater than  $Q_3 + 1.5IQR$  (if any) are separately marked.\nkernel density estimatorsns.violinplot(data=income, orient=&quot;h&quot;)plt.show()\nMeasures of Shape (*)\ng =\n\\frac{\n\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^{3}\n}{\n\\left( \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^{2} } \\right)^{3}\n}import scipy.statsscipy.stats.skew(heights)scipy.stats.skew(income)\n","categories":["da"],"tags":["da"]},{"title":"Inspecting the Distribution of Numberic Data","url":"/2025/11/10/big_data/da/02-des-statistics/","content":"Inspecting the Distribution of Numberic Dataimport numpy as npheights = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/nhanes_adult_female_height_2020.txt&quot;)np.random.choice(heights, 24, replace=False)\nHistogramsimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use(&quot;seaborn&quot;)sns.__version__  # FYIsns.histplot(heights, bins=11)plt.show()\nincome = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/uk_income_simulated_2020.txt&quot;)sns.histplot(income, stat=&quot;percent&quot;, bins=20)plt.show()\nmarathon = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/37_pzu_warsaw_marathon_mins.txt&quot;)marathon[:5]  # preview top 5 (data are sorted increasingly)sns.histplot(marathon[marathon &lt; 180])plt.show()\nBinningplt.subplot(121)  # 1 row, 2 columns, 1st plotsns.histplot(income, bins=5)plt.subplot(122)  # 1 row, 2 columns, 2nd plotsns.histplot(income, bins=200)plt.ylabel(None)plt.show()\npeds = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/southern_cross_station_peds_2019_dec.txt&quot;)pedsplt.bar(np.arange(0, 24), width=1, height=peds, edgecolor=&quot;black&quot;)plt.show()\nmatura = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/matura_2019_polish.txt&quot;)plt.bar(np.arange(0, 71), width=1, height=matura, edgecolor=&quot;black&quot;)plt.show()\nCumulative Countssns.histplot(heights, stat=&quot;percent&quot;, cumulative=True)plt.show()\nn = len(heights)heights_sorted = np.sort(heights)plt.plot(heights_sorted, np.arange(1, n+1)/n, drawstyle=&quot;steps-post&quot;)plt.xlabel(&quot;$x$&quot;)plt.ylabel(&quot;$\\\\hat&#123;F&#125;_n(x)$, i.e., Prob(height $\\\\leq$ x)&quot;)plt.show()\nLog-scalecities = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/other/us_cities_2000.txt&quot;)large_cities = cities[cities &gt;= 10000]large_cities[-5:]  # data are sorted\nsns.histplot(large_cities, bins=20)plt.show()sns.histplot(large_cities, bins=20, log_scale=True)plt.show()","categories":["da"],"tags":["da"]},{"title":"Visualising Multidimensional Data and Measuring Correlation","url":"/2025/11/10/big_data/da/04-des-statistics/","content":"Visualising Multidimensional Data and Measuring Correlationimport numpy as npimport pandas as pdbody = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/nhanes_adult_female_bmx_2020.csv&quot;,    comment=&quot;#&quot;)body = body.to_numpy()  # data frames will be covered laterbody.shapebody[:6, :]  # 6 first rows, all columns\nScatterplots2D Dataimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use(&quot;seaborn&quot;)plt.scatter(body[:, 1], body[:, 3])  # x=body[:, 1], y=body[:, 3]plt.xlabel(&quot;standing height (cm)&quot;)plt.ylabel(&quot;upper leg length (cm)&quot;)plt.show()body[np.argmin(body[:, 1]), [1, 3]]body[np.argmax(body[:, 3]), [1, 3]]\nplt.scatter(body[:, 1], body[:, 3], c=&quot;#00000011&quot;)plt.plot(np.mean(body[:, 1]), np.mean(body[:, 3]), &quot;rX&quot;)  # the centroidplt.xlabel(&quot;standing height (cm)&quot;)plt.ylabel(&quot;upper leg length (cm)&quot;)plt.axis(&quot;equal&quot;)plt.show()\n3D Data and Beyondfig = plt.figure()ax = fig.add_subplot(projection=&quot;3d&quot;)ax.scatter(body[:, 1], body[:, 3], body[:, 0], color=&quot;#00000011&quot;)ax.view_init(elev=30, azim=20, vertical_axis=&quot;y&quot;)ax.set_xlabel(&quot;standing height (cm)&quot;)ax.set_ylabel(&quot;upper leg length (cm)&quot;)ax.set_zlabel(&quot;weight (kg)&quot;)plt.show()\nfrom matplotlib import cmplt.scatter(    body[:, 4],   # x    body[:, 5],   # y    c=body[:, 0], # &quot;z&quot; - colours    cmap=cm.get_cmap(&quot;copper&quot;),  # colour map    alpha=0.5  # opaqueness level between 0 and 1)plt.xlabel(&quot;arm circumference (cm)&quot;)plt.ylabel(&quot;hip circumference (cm)&quot;)plt.axis(&quot;equal&quot;)plt.rcParams[&quot;axes.grid&quot;] = Falsecbar = plt.colorbar()plt.rcParams[&quot;axes.grid&quot;] = Truecbar.set_label(&quot;weight (kg)&quot;)plt.show()\nScatterplot Matrixsns.pairplot(data=pd.DataFrame(    body[:, [0, 1, 4, 5]],    columns=[        &quot;weight (kg)&quot;, &quot;standing height (cm)&quot;,        &quot;arm circumference (cm)&quot;, &quot;hip circumference (cm)&quot;    ]))plt.show()\nMeasuring CorrelationPearson‚Äôs Linear Correlation Coefficient\n\nr(x, y) =\\frac{1}{\\,n - 1\\,}\\sum_{i=1}^{n}\\left(\\frac{x_i - \\bar{x}}{s_x}\\right)\\left(\\frac{y_i - \\bar{y}}{s_y}\\right),with $s_x$, $s_y$  denoting the standard deviations and  $\\bar{x}$, $\\bar{y}$ being the means of  $x = (x_1, \\cdots, x_n)$  and  $y=(y_1, \\cdots, y_n)$, respectively.\nx = body[:, 4]  # arm circumferencey = body[:, 5]  # hip circumferencex_std = (x-np.mean(x))/np.std(x, ddof=1)y_std = (y-np.mean(y))/np.std(y, ddof=1)np.sum(x_std*y_std)/(len(x)-1)\nimport scipy.statsscipy.stats.pearsonr(x, y)[0]  # returns more than we ask for\ndef plot_corr(x, y):    r = scipy.stats.pearsonr(x, y)[0]    œÅ = scipy.stats.spearmanr(x, y)[0]    plt.scatter(x, y, label=f&quot;r = &#123;r:.3&#125;\\nœÅ = &#123;œÅ:.3&#125;&quot;)    plt.legend()\nnp.random.seed(123)x = np.random.rand(100)plt.subplot(121)plot_corr(x, -0.5*x+3)  # negative slopeplt.axis(&quot;equal&quot;)plt.subplot(122)plot_corr(x, 3*x+10)  # positive slopeplt.axis(&quot;equal&quot;)plt.show()\nx = np.random.rand(100)y = 0.5*xe = np.random.randn(len(x))  # random white noise (of mean 0)plt.subplot(221)plot_corr(x, y)plt.subplot(222)plot_corr(x, y+0.05*e)  # add some noiseplt.subplot(223)plot_corr(x, y+0.1*e)   # more noiseplt.subplot(224)plot_corr(x, y+0.25*e)  # even more noiseplt.show()\nplt.subplot(221)plot_corr(x, np.random.rand(100))  # independent (not correlated)plt.subplot(222)plot_corr(x, (2*x-1)**2-1)  # quadratic dependenceplt.subplot(223)plot_corr(x, np.abs(2*x-1))  # another form of dependenceplt.subplot(224)plot_corr(x, 0.25*np.sin(8*np.pi*x))  # anotherplt.show()\nplt.subplot(221)plot_corr(x, 0.7*np.sin(2*np.pi*x))plt.subplot(222)plot_corr(x, np.log(x+1))plt.subplot(223)plot_corr(x, np.exp(x**2))plt.subplot(224)plot_corr(x, 1/x)plt.show()\nCorrelation Heatmaporder = [4, 5, 6, 0, 2, 1, 3]cols = np.array([&quot;weight&quot;, &quot;height&quot;, &quot;arm len&quot;,    &quot;leg len&quot;, &quot;arm circ&quot;, &quot;hip circ&quot;, &quot;waist circ&quot;])C = np.corrcoef(body.T)sns.heatmap(    C[np.ix_(order, order)],    xticklabels=cols[order],    yticklabels=cols[order],    annot=True, fmt=&quot;.2f&quot;, cmap=cm.get_cmap(&quot;copper&quot;))plt.show()\nLinear Correlation Coefficients on Transformed Dataworld = pd.read_csv(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/world_factbook_2020_subset1.csv&quot;,    comment=&quot;#&quot;)world = world.to_numpy()world[:6, :]  # previewscipy.stats.pearsonr(world[:, 0], world[:, 1])[0]scipy.stats.pearsonr(np.log(world[:, 0]), world[:, 1])[0]plt.subplot(121)plot_corr(world[:, 0], world[:, 1])plt.subplot(122)plot_corr(np.log(world[:, 0]), world[:, 1])plt.show()\nSpearman‚Äôs Rank Correlation CoefficientSpearman‚Äôs Rank Correlation Coefficientscipy.stats.spearmanr(world[:, 0], world[:, 1])[0]scipy.stats.pearsonr(    scipy.stats.rankdata(world[:, 0]),    scipy.stats.rankdata(world[:, 1]))[0]scipy.stats.spearmanr(np.log(world[:, 0]), -np.sqrt(world[:, 1]))[0]\n","categories":["da"],"tags":["da"]},{"title":"Handling Categorical Data","url":"/2025/11/10/big_data/da/03-des-statistics/","content":"Handling Categorical DataRepresenting Categorical DataTwo common ways to represent a categorical variable with k distinct levels is by storing it as:\n\na vector of strings,\na vector of integers between 0 (inclusive) and k (exclusive).\n\nimport numpy as npcountries = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/37_pzu_warsaw_marathon_country.txt&quot;,    dtype=&quot;str&quot;)x = countries[:16]xnp.unique(x)\nEncoding and Decoding Factorscategories, codes = np.unique(x, return_inverse=True)categories, codescategories[codes]np.array([&quot;Ethiopia&quot;, &quot;Israel&quot;, &quot;Kenya&quot;, &quot;Morocco&quot;, &quot;Poland&quot;])[codes]\nBinning Numeric Datamarathon = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/37_pzu_warsaw_marathon_mins.txt&quot;)t = marathon[:16]tbins = [130, 140, 150]codes = np.searchsorted(bins, t)codesbins2 = np.r_[-np.inf, bins, np.inf]categories = np.array(    [f&quot;(&#123;bins2[i-1]&#125;, &#123;bins2[i]&#125;]&quot; for i in range(1, len(bins2))])categoriescategories[codes]\nGenerating Pseudorandom Labelsnp.random.seed(123)np.random.choice(    [&quot;spam&quot;, &quot;bacon&quot;, &quot;eggs&quot;, &quot;tempeh&quot;], p=[0.7, 0.1, 0.15, 0.05],    replace=True, size=16)\nFrequency DistributionsCountingx = countries[:16]np.unique(x, return_counts=True)categories, codes = np.unique(x, return_inverse=True)counts = np.bincount(codes)counts\nfor category, count in zip(categories, counts):    print(f&quot;&#123;category:4&#125;: &#123;count:5&#125;&quot;)counts/np.sum(counts)\nVisualisingBar Plotsimport matplotlib.pyplot as pltplt.style.use(&quot;seaborn&quot;)ind = np.arange(len(categories))plt.bar(ind, height=counts)plt.xticks(ind, categories)plt.show()\nplt.bar([1, 2], height=[51.03, 48.97])plt.xticks([1, 2], [&quot;Duda&quot;, &quot;Trzaskowski&quot;])plt.ylabel(&quot;%&quot;)plt.ylim(48.9, 51.1)plt.show()\nplt.bar([1, 2], height=[51.03, 48.97])plt.xticks([1, 2], [&quot;Duda&quot;, &quot;Trzaskowski&quot;])plt.ylabel(&quot;%&quot;)plt.ylim(0, 250)plt.yticks([0, 100])plt.show()\nError Barsc, n = 516, 1017plt.bar([1, 2], height=[c/n, (n-c)/n])plt.errorbar([1, 2], [c/n, (n-c)/n], yerr=0.031, fmt=&quot;r&quot;)plt.xticks([1, 2], [&quot;Duda&quot;, &quot;Trzaskowski&quot;])plt.ylabel(&quot;%&quot;)plt.show()\nPareto Chartscategories = np.array([    &quot;Unauthorised drug&quot;, &quot;Wrong IV rate&quot;, &quot;Wrong patient&quot;, &quot;Dose missed&quot;,    &quot;Under dose&quot;, &quot;Wrong calculation&quot;,&quot;Wrong route&quot;, &quot;Wrong drug&quot;,    &quot;Wrong time&quot;, &quot;Technique error&quot;, &quot;Duplicated drugs&quot;, &quot;Over dose&quot;])counts = np.array([1, 4, 53, 92, 7, 16, 27, 76, 83, 3, 9, 59])np.sum(counts)  # total number of medication errors\no = np.argsort(counts)[::-1]  # ordering permutation (reversed = decreasing)categories = categories[o]  # order categories based on countscounts = counts[o]  # equivalent to np.sort(counts)[::-1]for category, count in zip(categories, counts):    print(f&quot;&#123;category:20&#125;: &#123;count:2&#125;&quot;)\nx = np.arange(len(categories))  # 0, 1, 2, ...p = 100.0*counts/np.sum(counts)  # percentagesfig, ax1 = plt.subplots()ax1.set_xticks(x-0.5, categories, rotation=60)ax1.set_ylabel(&quot;%&quot;)ax1.bar(x, height=p)ax2 = ax1.twinx()  # this creates a new coordinate system with a shared X axisax2.plot(x, np.cumsum(p), &quot;ro-&quot;)ax2.grid(visible=False)ax2.set_ylabel(&quot;cumulative %&quot;)fig.tight_layout()plt.show()\nfor category, cumprob in zip(categories, np.round(np.cumsum(p), 1)):    print(f&quot;&#123;category:20&#125;: &#123;cumprob:5&#125;%&quot;)\nAggregatingcategories, counts = np.unique(countries, return_counts=True)categories[np.argmax(counts)]categories2, counts2 = np.unique(countries[:22], return_counts=True)categories2[np.where(counts2 == np.max(counts2))]\nBinary Data and Logical Vectorsnp.array([True, False, True, True, False]).astype(int)np.array([-2, -0.326, -0.000001, 0.0, 0.1, 1, 7643]).astype(bool)np.array([-2, -0.326, -0.000001, 0.0, 0.1, 1, 7643]) != 0x = countries[:16]x  # recallnp.sum(x == &quot;KE&quot;)np.mean(x == &quot;KE&quot;)","categories":["da"],"tags":["da"]},{"title":"Multivariate Categorical and Relational Data","url":"/2025/11/10/big_data/da/05-des-statistics/","content":"Multivariate Categorical and Relational Dataimport numpy as npmarathon = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/37_pzu_warsaw_marathon_3groups_top1000.txt&quot;,    delimiter=&quot;,&quot;, dtype=str)marathon[:6, :]  # preview\nTwo-Way Contingency Tablesnp.unique(marathon[:, 0])np.unique(marathon[:, 1])\nimport scipy.statsl, v = scipy.stats.contingency.crosstab(marathon[:, 0], marathon[:, 1])l, vimport marekmarek.print_labelled_array(l, v)\nmarek.print_labelled_array(l[0], np.sum(v, axis=1))marek.print_labelled_array(l[1], np.sum(v, axis=0))\nVisualisingHeat Mapimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use(&quot;seaborn&quot;)from matplotlib import cmsns.heatmap(    v, xticklabels=l[1], yticklabels=l[0],    annot=True, fmt=&quot;d&quot;, cmap=cm.get_cmap(&quot;copper&quot;))plt.show()\nBar Plotind = np.arange(len(l[1]))plt.bar(ind-0.25, height=v[0, :], width=0.4, label=l[0][0])plt.bar(ind+0.25, height=v[1, :], width=0.4, label=l[0][1])plt.xticks(ind, l[1])plt.legend()plt.show()\nind = np.arange(len(l[0]))p_sex = v/np.sum(v, axis=1).reshape(-1, 1)*100.0  # normalise each row (100%)c_sex = np.insert(np.cumsum(p_sex, axis=1), 0, 0, axis=1)  # prepend column [0]for j in range(p_sex.shape[1]):    plt.barh(ind, left=c_sex[:, j], width=p_sex[:, j], label=l[1][j])plt.yticks(ind, l[0])plt.legend(ncol=len(l[1]))plt.show()\nHigher-Order Contingency Tablesmarathon = np.column_stack((    marathon,    np.where(marathon[:, 2] == &quot;PL&quot;, &quot;PL&quot;,        np.where(marathon[:, 2] == &quot;KE&quot;, &quot;KE&quot;, &quot;XX&quot;))))\nl, v = scipy.stats.contingency.crosstab(marathon[:, 3], marathon[:, 0], marathon[:, 1])l, vmarek.print_labelled_array(l, v)marek.print_labelled_array((l[0], l[1]), np.sum(v, axis=2))marek.print_labelled_array(l[0], np.sum(v, axis=(1, 2)))\nMeasuring Associationl = [    [&quot;Arthritis&quot;, &quot;Asthma&quot;, &quot;Back problems&quot;, &quot;Cancer (malignant neoplasms)&quot;,     &quot;Chronic obstructive pulmonary disease&quot;, &quot;Diabetes mellitus&quot;,     &quot;Heart, stroke and vascular disease&quot;, &quot;Kidney disease&quot;,     &quot;Mental and behavioural conditions&quot;, &quot;Osteoporosis&quot;],    [&quot;15-44&quot;, &quot;45-64&quot;, &quot;65+&quot;]]v = 1000*np.array([    [ 360.2,     1489.0,      1772.2],    [1069.7,      741.9,       433.7],    [1469.6,     1513.3,       955.3],    [  28.1,      162.7,       237.5],    [ 103.8,      207.0,       251.9],    [ 135.4,      427.3,       607.7],    [  94.0,      344.4,       716.0],    [  29.6,       67.7,       123.3],    [2218.9,     1390.6,       725.0],    [  36.1,      312.3,       564.7],]).astype(int)marek.print_labelled_array(l, v)\nscipy.stats.contingency.association(v)e = np.sum(v, axis=1).reshape(-1, 1)*np.sum(v, axis=0).reshape(1, -1)/np.sum(v)chisq = np.sum((v - e)**2/e)np.sqrt(chisq/np.sum(v)/(np.min(v.shape)-1))\n","categories":["da"],"tags":["da"]},{"title":"Continuous Probability Distributions","url":"/2025/11/10/big_data/da/06-des-statistics/","content":"Continuous Probability Distributionsimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use(&quot;seaborn&quot;)\nheights = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/nhanes_adult_female_height_2020.txt&quot;)sns.histplot(heights, stat=&quot;density&quot;, kde=True)plt.show()import scipy.stats\nNormal DistributionNormal DistributionŒº = np.mean(heights)  # an estimator of expected valueœÉ = np.std(heights, ddof=1)  # an estimator of standard deviationŒº, œÉsns.histplot(heights, stat=&quot;density&quot;, kde=True)x = np.linspace(np.min(heights), np.max(heights), 1000)plt.plot(x, scipy.stats.norm.pdf(x, Œº, œÉ), &quot;r:&quot;)  # a dotted red curveplt.show()\nComparing Cumulative Distribution Functionsn = len(heights)x = np.linspace(np.min(heights), np.max(heights), 1001)probs = scipy.stats.norm.cdf(x, Œº, œÉ)  # sample the CDF at many pointsplt.plot(x, probs, &quot;r--&quot;, label=&quot;Theoretical CDF&quot;)heights_sorted = np.sort(heights)plt.plot(heights_sorted, np.arange(1, n+1)/n,    drawstyle=&quot;steps-post&quot;, label=&quot;Empirical CDF&quot;)plt.xlabel(&quot;$x$&quot;)plt.ylabel(&quot;Prob(height $\\\\leq$ x)&quot;)plt.legend()plt.show()\nQ-Q plotsQ-Q plotsn = len(heights)q = np.arange(1, n+1)/(n+1)  # 1/(n+1), 2/(n+2), ..., n/(n+1)heights_sorted = np.sort(heights)          # theoretical quantilesquantiles = scipy.stats.norm.ppf(q, Œº, œÉ)  # sample quantilesplt.plot(quantiles, heights_sorted, &quot;o&quot;)plt.axline((heights_sorted[n//2], heights_sorted[n//2]), slope=1)plt.xlabel(&quot;Theoretical quantiles&quot;)plt.ylabel(&quot;Sample quantiles&quot;)plt.show()\nLog-normal DistributionLog-normal Distributionincome = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/uk_income_simulated_2020.txt&quot;)sns.histplot(income, stat=&quot;density&quot;, kde=True, log_scale=True)plt.show()Œº = np.mean(np.log(income))œÉ = np.std(np.log(income), ddof=1)Œº, œÉ\nsns.histplot(income, stat=&quot;density&quot;, kde=True)x = np.linspace(np.min(income), np.max(income), 1000)plt.plot(x, scipy.stats.lognorm.pdf(x, s=œÉ, scale=np.exp(Œº)), &quot;r:&quot;)plt.show()\nn = len(income)q = np.arange(1, n+1)/(n+1)income_sorted = np.sort(income)plt.subplot(121)quantiles = scipy.stats.lognorm.ppf(q, s=œÉ, scale=np.exp(Œº))plt.plot(quantiles, income_sorted, &quot;o&quot;)plt.axline((income_sorted[n//2], income_sorted[n//2]), slope=1)plt.xlabel(&quot;Log-normal theoretical quantiles&quot;)plt.ylabel(&quot;Sample quantiles&quot;)plt.xscale(&quot;log&quot;)plt.yscale(&quot;log&quot;)plt.subplot(122)quantiles2 = scipy.stats.norm.ppf(    q, np.mean(income), np.std(income_sorted, ddof=1))plt.plot(quantiles2, income_sorted, &quot;o&quot;)plt.axline((income_sorted[n//2], income_sorted[n//2]), slope=1)plt.xlabel(&quot;Normal theoretical quantiles&quot;)plt.show()\nPareto Distributioncities = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/other/us_cities_2000.txt&quot;)len(cities), sum(cities)  # number of cities, total populationsns.histplot(cities, bins=20, log_scale=True)plt.show()min_size = 10_000large_cities = cities[cities &gt;= min_size]len(large_cities), sum(large_cities)  # number of cities, total populationsns.histplot(large_cities, bins=20, log_scale=(True, True))plt.show()Œ± = 1/np.mean(np.log(large_cities/min_size))Œ±b = np.geomspace(min_size, np.max(large_cities), 21)  # bins&#x27; boundariesc = np.histogram(large_cities, bins=b)[0]  # apply binningc = c/np.sum(c)  # normalise so that it adds to 1p = np.diff(scipy.stats.pareto.cdf(b, Œ±, scale=min_size))p = p/np.sum(p)  # normalise so that it adds to 1midb = 0.5*(b[:-1]+b[1:])  # mid-binsplt.bar(midb, width=np.diff(b), height=c, edgecolor=&quot;black&quot;)plt.plot(midb, p, &quot;r:&quot;)plt.xscale(&quot;log&quot;)plt.yscale(&quot;log&quot;)plt.show()n = len(large_cities)q = np.arange(1, n+1)/(n+1)cities_sorted = np.sort(large_cities)quantiles = scipy.stats.pareto.ppf(q, Œ±, scale=min_size)plt.plot(quantiles, cities_sorted, &quot;o&quot;)plt.axline((quantiles[n//2], cities_sorted[n//2]), slope=1)plt.xlabel(&quot;Theoretical quantiles&quot;)plt.ylabel(&quot;Sample quantiles&quot;)plt.xscale(&quot;log&quot;)plt.yscale(&quot;log&quot;)plt.show()\nUniform Distributionlotto = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/lotto_table.txt&quot;)plt.bar(np.arange(1, 50), width=1, height=lotto, edgecolor=&quot;black&quot;)plt.show()x = np.arange(1, 50)plt.bar(x, width=1, height=lotto/np.sum(lotto), edgecolor=&quot;black&quot;)plt.plot(x, scipy.stats.uniform.pdf(x, 1, 49), &quot;r:&quot;)plt.show()\nGenerating Pseudorandom Numbersnp.random.rand(5)scipy.stats.uniform.rvs(-10, 25, size=5)  # from -10 to -10+25np.random.seed(123)  # set seednp.random.rand(5)np.random.seed(123)  # set seednp.random.rand(5)scipy.stats.uniform.rvs(size=5, random_state=123)sample = scipy.stats.norm.rvs(100, 16, size=250, random_state=12641)sns.histplot(sample, stat=&quot;density&quot;, kde=True)x = np.linspace(36, 164, 1001)plt.plot(x, scipy.stats.norm.pdf(x, 100, 16), &quot;r:&quot;)plt.show()\nDistribution Mixturespeds = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/southern_cross_station_peds_2019_dec.txt&quot;)plt.bar(np.arange(0, 24), width=1, height=peds, edgecolor=&quot;black&quot;)plt.show()plt.bar(np.arange(0, 24), width=1, height=peds/np.sum(peds), edgecolor=&quot;black&quot;)x = np.arange(0, 25, 0.1)p1 = scipy.stats.norm.pdf(x, 8, 1)p2 = scipy.stats.norm.pdf(x, 12, 1)p3 = scipy.stats.norm.pdf(x, 17, 2)p = 0.35*p1 + 0.1*p2 + 0.55*p3  # weighted (convex) combination of 3 densitiesplt.plot(x, p, &quot;r:&quot;)plt.show()","categories":["da"],"tags":["da"]},{"title":"Imbalanced Data","url":"/2025/11/11/big_data/da/08-des-statistics/","content":"Imbalanced DataImbalanced data research\nThe editorial on Learning from Imbalanced Data highlights the central challenge that in many real-world applications‚Äîsuch as fraud detection, medical diagnosis, and anomaly detection‚Äîthe important class is often rare, causing standard machine-learning algorithms to bias heavily toward the majority class. It summarizes three major research directions: data-level methods (oversampling, undersampling, synthetic sample generation), algorithm-level methods (cost-sensitive learning, specialized classifiers, ensemble or boosting techniques), and proper evaluation practices that move beyond simple accuracy toward metrics like precision‚Äìrecall, F-measure, and AUC. The editorial emphasizes that imbalanced-data problems remain difficult due to issues such as noisy minority samples, class overlap, and lack of standardized benchmarks, and calls for consistent evaluation protocols and deeper investigation into modern settings such as deep learning, streaming data, and extreme imbalance.\n\nAssessing classifiers for imbalanced dataassessing classifiers for imbalanced data\n‚ÄúLearning from Imbalanced Data‚Äù deals with the problem that, in many real-world tasks (fraud detection, disease diagnosis, anomaly detection, etc.), the class of interest (e.g. fraud, disease, anomaly) is very rare compared to the majority class. Standard machine-learning algorithms often fail on such datasets because they assume balanced classes and minimize overall error ‚Äî leading them to ignore the minority class and yield misleadingly high accuracy by just predicting the majority class. To handle this, the literature proposes methods including data-level strategies (oversampling the minority class, undersampling the majority, synthetic sample generation like SMOTE), algorithm-level strategies (cost-sensitive learning, adjusted decision thresholds, specialized learners), and hybrid / ensemble methods combining both. Moreover, proper evaluation metrics ‚Äî such as precision/recall, F1-score, AUC ‚Äî are essential, because accuracy alone is misleading in imbalanced settings. Despite many advances, the field still faces challenges: noisy or overlapping data, small sample size for minority class, lack of standard benchmarks, and difficulties extending classical methods to modern contexts (deep learning, streaming data, regression tasks). The work calls for principled methods, consistent evaluation standards, and continued research especially in long-tail and deep-learning settings.\n\nData level methodsdata level methods\nSMOTE (Synthetic Minority Over-sampling Technique) is a widely used method for handling imbalanced datasets by generating synthetic minority-class samples instead of simply duplicating existing ones. It works by selecting a minority instance, finding its k nearest minority neighbors, and creating new samples by interpolating points along the line segments between the instance and its neighbors. This enlarges the decision space of the minority class and reduces the bias of standard classifiers toward the majority class, often improving recall and overall minority-class performance. However, SMOTE does not consider the distribution of the majority class, which can cause synthetic samples to appear in overlapping or noisy regions, potentially harming performance. Later research also shows that SMOTE may distort the true minority distribution in high-dimensional or complex feature spaces, motivating many variants (e.g., Borderline-SMOTE, Safe-SMOTE) to address these limitations.\n\npython implementation of SMOTE\nCost sensitive approachesCost sensitive learningLearning from imbalanced data focuses on addressing classification problems where one class is significantly underrepresented, causing standard machine-learning algorithms to become biased toward the majority class. The paper reviews three major solution categories: data-level methods such as oversampling, undersampling, and synthetic sample generation (e.g., SMOTE); algorithm-level methods that modify learning objectives with cost-sensitive training, adjusted decision thresholds, or ensemble techniques like boosting and bagging; and hybrid approaches that combine resampling with specialized classifiers. It highlights that imbalance interacts with other difficulties ‚Äî noise, small sample sizes, and class overlap ‚Äî and therefore robust solutions must consider data quality and distribution, not just class proportions. The paper concludes by emphasizing that no single method works universally, and effective handling of imbalanced data typically requires careful experimentation, evaluation using metrics like recall, precision, F-measure, and ROC/PR curves, and the use of tailored algorithms designed to enhance minority-class recognition.\n\nCost sensitive AdaBoostCost-sensitive boosting for imbalanced data modifies traditional boosting (e.g. AdaBoost) by incorporating misclassification costs into the weight-updating and loss minimization process, so that errors on the minority class are penalized more heavily than errors on the majority class. The paper introduces several boosting algorithms that embed cost items into the boosting framework: by adjusting the weight update formulas, they bias the learning process toward correctly classifying the minority (rare) class rather than optimizing overall accuracy. Empirical experiments on real-world imbalanced datasets ‚Äî often from critical domains like medical diagnosis, fraud detection, or anomaly detection ‚Äî show that these cost-sensitive boosting methods substantially improve minority-class detection (e.g. higher recall / sensitivity), compared with na√Øve boosting or standard classifiers. Because boosting combines many weak learners and reweights samples iteratively, cost-sensitive boosting can preserve more information than naive sampling methods while effectively focusing on rare but important cases.\n\nThe Foundations of Cost-Sensitive Learning\nThe Foundations of Cost-Sensitive Learning investigates the problem of classification when different types of misclassification errors have different penalties (costs). The paper precisely defines when a cost matrix is ‚Äúreasonable‚Äù (i.e., economically and statistically coherent) and warns against arbitrarily chosen cost matrices that might lead to inconsistent or irrational decision rules. For the binary-class case, it proves a theorem showing how, given a cost matrix and class priors, one can derive an optimal decision rule ‚Äî which may differ from the standard equal-cost decision threshold. The paper further argues that simply rebalancing or resampling the training set does not guarantee optimal cost-sensitive decisions under many classifiers (e.g. Bayesian or decision-tree learners). Instead, the recommended approach is to train a probabilistic classifier on the data ‚Äúas is,‚Äù then apply the cost matrix at decision time to choose class labels that minimize expected cost, thus decoupling model training from decision making.\n\n","categories":["da"],"tags":["da"]},{"title":"Outlier","url":"/2025/11/11/big_data/da/07-des-statistics/","content":"Outliersimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snspd.set_option(&quot;display.notebook_repr_html&quot;, False)  # disable &quot;rich&quot; outputplt.style.use(&quot;seaborn&quot;)\nUnidimensional Datax = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/blobs2.txt&quot;)plt.subplot(121)sns.boxplot(data=x, orient=&quot;h&quot;)plt.subplot(122)sns.histplot(x, binwidth=1)plt.show()\nMultidimensional Datax = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/blobs1.txt&quot;, delimiter=&quot;,&quot;)plt.subplot(221)sns.boxplot(data=x[:, 0], orient=&quot;h&quot;)plt.subplot(222)sns.histplot(x[:, 0], bins=20)plt.title(&quot;x[:, 0]&quot;)plt.subplot(223)sns.boxplot(data=x[:, 1], orient=&quot;h&quot;)plt.subplot(224)sns.histplot(x[:, 1], bins=20)plt.title(&quot;x[:, 1]&quot;)plt.show()\nMultidimensional Datax = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/&quot; +    &quot;teaching-data/master/marek/blobs1.txt&quot;, delimiter=&quot;,&quot;)plt.subplot(221)sns.boxplot(data=x[:, 0], orient=&quot;h&quot;)plt.subplot(222)sns.histplot(x[:, 0], bins=20)plt.title(&quot;x[:, 0]&quot;)plt.subplot(223)sns.boxplot(data=x[:, 1], orient=&quot;h&quot;)plt.subplot(224)sns.histplot(x[:, 1], bins=20)plt.title(&quot;x[:, 1]&quot;)plt.show()\nplt.scatter(x[:, 0], x[:, 1])plt.axis(&quot;equal&quot;)plt.show()x[-8:, :]\nimport scipy.spatialt = scipy.spatial.KDTree(x)n = t.query_ball_tree(t, 0.2)  # epsilon=0.2 (radius)c = np.array([len(e) for e in n])c[[0, 1, -2, -1]]  # preview\nplt.scatter(x[c &gt; 1, 0], x[c &gt; 1, 1], label=&quot;normal point&quot;)plt.scatter(x[c == 1, 0], x[c == 1, 1], marker=&quot;v&quot;, label=&quot;outlier&quot;)plt.axis(&quot;equal&quot;)plt.legend()plt.show()\nDistance-based outlier detectionK-nearest neighbour (KNN)[1] Edwin Knorr and Raymond Ng. Algorithms for mining distance-based outliers in large datasets. In Proc. of the VLDB Conference, pages 392‚Äì403, New York, USA, September 1998.Read Section 3.1 of the paper\nThis paper proposes an intuitive and general distance-based definition of outliers, where a data point‚Äôs ‚Äúdegree of being an outlier‚Äù is measured by its distance to the k-th nearest neighbor, and the top n points with the largest distances are considered outliers. To address the high computational cost on large datasets, the authors designed an efficient partition-based pruning algorithm that can exclude partitions that cannot contain outliers in advance, significantly reducing computation. Experimental results show that this method performs well and scales effectively on large and high-dimensional datasets, making it suitable for applications such as database cleaning, fraud detection, network intrusion detection, and IoT sensor data analysis.\nHowever, the method still depends on the choice of parameters n and k, may suffer from distance degradation in high-dimensional spaces, and is sensitive to data standardization and distance metrics. Overall, this paper is pioneering in the field of outlier detection, offering both theoretical significance and practical applicability.\nDensity-based outlier detectionLocal outlier factor (LOF) algorithmRead Definition 3, of the paper\nThe LOF (Local Outlier Factor) algorithm is a density-based local outlier detection method. It measures the degree of abnormality of each data point by comparing its local density with that of its neighbors. Specifically, it computes the local reachability density (lrd) of each point and then forms the LOF value as the ratio of the average density of its neighbors to its own density. An LOF value close to 1 indicates that the point has a density similar to its neighbors and is considered normal, while a significantly larger value indicates local sparsity and a potential outlier. The algorithm is particularly effective for detecting local outliers in datasets with varying density or irregular cluster shapes.\nThe advantages of LOF include its independence from predefined clusters or distribution assumptions, its continuous outlier scoring, and strong adaptability to complex data structures. However, it is sensitive to the choice of the neighborhood parameter k, requires experience-based thresholding for LOF values, and has relatively high computational complexity, making it less efficient for large-scale, high-dimensional, or streaming data. LOF is widely applied in anomaly detection, data cleaning, IoT sensor data analysis, and network traffic monitoring.\n\nRead Page 6 of the paper\n\n\n\n\nMethod Category\nCore Idea\nMain Algorithms / Representative Methods\nAdvantages\nLimitations / Notes\nSuitable Scenarios\n\n\n\n\nStatistical-based\nAssume data follows a certain distribution; detect outliers based on probability or deviation\nZ-score, Grubbs Test, Mixture Models\nSimple and intuitive; works well for low-dimensional, known-distribution data\nDepends on distribution assumption; less effective in high dimensions\nData quality checking, low-dimensional sensor data\n\n\nDistance / Nearest Neighbor\nPoints far from others are likely outliers\nk-NN Outlier, Distance-based Outlier\nDistribution-free; intuitive\nHigh computational cost in high-dimensional or large datasets; needs parameter k\nIoT, RFID, trajectory anomaly detection\n\n\nDensity-based\nCompare local density of a point with its neighbors\nLOF (Local Outlier Factor), DBSCAN Outlier\nDetects local outliers; handles varying density\nSensitive to parameter k; computationally intensive\nLocal outlier detection, datasets with varying density\n\n\nClustering-based\nOutliers do not belong to major clusters or belong to small clusters\nK-means Outlier, DBSCAN Outlier\nCan detect isolated points outside clusters\nSensitive to clustering quality; hard with noise or high-dimensional data\nData exploration, cluster-based anomaly detection\n\n\nLearning / ML-based\nUse supervised, semi-supervised, or unsupervised models to predict outliers\nOne-class SVM, Isolation Forest, Autoencoder\nHandles high-dimensional and complex data\nRequires training; parameter tuning can be complex\nNetwork intrusion detection, anomaly pattern recognition\n\n\nHybrid / Ensemble\nCombine multiple methods to improve robustness\nFeature Bagging + LOF, Ensemble k-NN\nImproves detection accuracy; reduces false positives\nHigh complexity; high computational cost\nHigh-dimensional, complex, or large-scale data\n\n\n\n\n\nlof\nPython scikit-learn (aka sklearn) has a page about LOF\nClustering-based outlier detectionModified k-means and minimum spanning treeRead Algorithm 3.1 and 3.2 of the paper\nThis paper proposes an outlier detection method based on a two-phase clustering process combined with a minimum spanning tree (MST). In the first phase, a modified k-means clustering is applied, where points far from existing cluster centers are treated as new cluster centers, allowing isolated or sparse outliers to form independent clusters. In the second phase, an MST is constructed using the cluster centers, and the longest edges are removed to split the tree into subtrees, identifying outliers in small or isolated clusters. This approach leverages both clustering and graph structure, effectively detecting few and sparsely distributed outliers while providing good interpretability.\nThe method‚Äôs advantages include independence from data distribution assumptions and the ability to detect isolated/sparse outliers, making it suitable for static and multi-modal datasets. Its limitations are sensitivity to parameters (such as initial k and distance thresholds), higher computational cost for high-dimensional or large-scale data, and reduced effectiveness for dense anomalous clusters or streaming data. The method is particularly suitable for scenarios like IoT, RFID, and sensor trajectory data where outliers are sparse and batch processing is feasible.\n\nDensity-based spatial clustering of applications with noise (DBSCAN) SectionRead Section 7.1 of the paper\n\n\n\n\nMethod Category\nCore Idea\nRepresentative Methods / Algorithms\nAdvantages\nLimitations / Notes\nSuitable Scenarios\n\n\n\n\nStatistical-based\nAssume data follows a certain distribution; detect outliers based on deviation or probability\nZ-score, Grubbs Test, Mixture Models\nSimple, intuitive, works well for low-dimensional, known-distribution data\nAssumes data distribution; less effective in high dimensions or unknown distribution\nLow-dimensional data, data cleaning, error detection\n\n\nDistance / Nearest-Neighbor-based\nPoints far from most others are potential outliers\nk-NN Outlier, Distance-based Outlier\nDistribution-free; intuitive; works for moderate dimensions\nComputationally expensive for high-dimensional/large data; sensitive to distance metric and parameter k\nIoT, RFID, trajectory anomaly detection, moderate-size datasets\n\n\nDensity-based\nCompare local density of a point with its neighbors; low-density points are outliers\nLOF (Local Outlier Factor), DBSCAN Outlier\nDetects local outliers; handles clusters with varying density or shape\nSensitive to parameters (neighborhood size, density threshold); computationally intensive; not ideal for streaming/high-dimensional data\nLocal anomaly detection, multi-modal or irregular cluster data\n\n\nClustering-based\nOutliers do not belong to main clusters or belong to small/sparse clusters\nK-means Outlier, DBSCAN Outlier\nEasy to interpret; effective if cluster structure exists\nSensitive to clustering quality; affected by noise; less effective in high-dimensional or complex data\nData with clear cluster structure; batch/static data; exploratory analysis\n\n\nLearning-based / Model-based\nUse supervised, semi-supervised, or unsupervised learning to identify outliers\nOne-class SVM, Isolation Forest, Autoencoder\nHandles high-dimensional, complex, and unstructured data; scalable for large datasets\nRequires training; parameter tuning; interpretability may be low; sensitive to training data\nNetwork intrusion detection, financial fraud, complex sensor/IoT data\n\n\nEnsemble / Hybrid Methods\nCombine multiple methods to improve robustness and accuracy\nFeature Bagging + LOF, Ensemble k-NN\nLeverages strengths of multiple methods; reduces false positives\nComplex to implement; computationally expensive; interpretation may be harder\nHigh-dimensional, large-scale, or complex datasets; critical systems requiring high reliability\n\n\n\n\n\nWikipedia has a page about DBSCAN\nApplications: Cluster Analysis (**)import numpy as npimport scipy.statsimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsplt.style.use(&quot;seaborn&quot;)  # plot stylepd.set_option(&quot;display.notebook_repr_html&quot;, False)  # disable &quot;rich&quot; outputnp.random.seed(123)  # initialise the pseudorandom number generator\nEuclidean Distance\n\\|\\mathbf{u} - \\mathbf{v}\\| = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \\dots + (u_m - v_m)^2} = \\sqrt{\\sum_{i=1}^{m} (u_i - v_i)^2}X = np.array([    [0,    0],    [1,    0],    [-1.5, 1],    [1,    1]])import scipy.spatial.distanceD = scipy.spatial.distance.cdist(X, X)D\nplt.plot(X[:, 0], X[:, 1], &quot;ko&quot;)for i in range(X.shape[0]-1):    for j in range(i+1, X.shape[0]):        plt.plot(X[[i,j], 0], X[[i,j], 1], &quot;k-&quot;, alpha=0.2)        plt.text(np.mean(X[[i,j], 0]), np.mean(X[[i,j], 1]), np.round(D[i, j], 2))plt.show()\nCentroids\n\\min_{c} \\sum_{i=1}^{n} \\| \\mathbf{x}_i - \\mathbf{c} \\|^2\n\\mathbf{c} = \\frac{1}{n} (\\mathbf{x}_1 + \\mathbf{x}_2 + \\dots + \\mathbf{x}_n) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{x}_i\nc_j = \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j}c = np.mean(X, axis=0)c\nnp.sqrt(np.mean(scipy.spatial.distance.cdist(X, c.reshape(1, -1))**2))\nClustering with k-Means\n\\min_{c_1, c_2, \\dots, c_k} \\sum_{i=1}^{n} \\min \\Big\\{ \n\\| \\mathbf{x}_i - \\mathbf{c}_1 \\|^2, \n\\| \\mathbf{x}_i - \\mathbf{c}_2 \\|^2, \n\\dots, \n\\| \\mathbf{x}_i - \\mathbf{c}_k \\|^2 \n\\Big\\}X = np.loadtxt(&quot;https://raw.githubusercontent.com/gagolews/teaching-data/master/marek/blobs1.txt&quot;, delimiter=&quot;,&quot;)import scipy.cluster.vqC, l = scipy.cluster.vq.kmeans2(X, 2)my_colours = np.array([&quot;lightcoral&quot;, &quot;cornflowerblue&quot;])plt.scatter(X[:, 0], X[:, 1], c=my_colours[l])plt.plot(C[:, 0], C[:, 1], &quot;kX&quot;)plt.axis(&quot;equal&quot;)plt.show()\nnp.bincount(l)  # or, e.g., pd.Series(l).value_counts()Xl = pd.DataFrame(dict(X1=X[:, 0], X2=X[:, 1], l=l))Xl.sample(5, random_state=42)  # some randomly chosen rowsXl.groupby(&quot;l&quot;).mean()np.argmin(scipy.spatial.distance.cdist(X, C), axis=1)","categories":["da"],"tags":["da"]},{"title":"Data Matching","url":"/2025/11/11/big_data/da/09-des-statistics/","content":"Data MatchingData Matching\nAfter reading `Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection, I gained a clearer understanding of the challenges involved in working with real-world data. The book highlights that data is often incomplete, inconsistent, and noisy, and that data matching aims to identity records referring to the same real-world entity under these imperfect conditions. Rather than focusing on a single algorithm, the book presents data matching as a structured process that includes data preprocessing, indexing, similarity comparison, match classification, and evaluation.\n\nEntity Resolution%20All%20of%20Entity%20Resolution.pdf?ou=1761137)\n‚Äú(Almost) All of Entity Resolution‚Äù provides a comprehensive literature review of the entity resolution problem, which aims to systematically identify and merge multiple recors that refer to the same real-world entity across noisy and heterogeneous data sources. The paper traces the foundational history of the field from early probabilistic linkage methods in the mid-20th century to modern probabilistic and supervised machine learning approaches. It discusses deterministic and similarity-based techniques, extensions of classical probabilistic frameworks, clustering-based resolution methods, and recent advances in uncertainty quantification. Through practical examples spanning census data, human rights records, citation network, and medical data, it highlights both theoratical and practical challenges of entity resolution. \n","categories":["da"],"tags":["da"]},{"title":"GDPR","url":"/2025/11/13/big_data/da/10-des-statistics/","content":"General Data Protection RegulationGDPRThe General Data Protection Regulation(GDPR) is a comprehensive data protection law enacted by the European Union to enhance individuals‚Äô control over their personal data to harmonise data protection rules across EU member states.\nGDPR was introduced in response to rapid technological developments and the increasing scale of personal data processing in the digital economy.\nGDPR emphasises the protection of individuals‚Äô fundamental rights and freedoms, particularly the right to data protection. The regulation stengthens data subject rights by granting individuals greater control over how their personal data is collected and used. GDPR aims to harmonise data protection laws across the EU, creating a consistent regulatory framework.\nPersonal data under GDPR includes any information that can directly or indirectly identify an individual. The data subject refers to an identifiable individual whose personal data is being processed. GDPR clearly distinguishes between data controllers and data processors, assigning different responsibilities to each.\nUnder GDPR, personal data must be processed lawfully, fairly, and transparently, and must be based on a valid lawful basis such as consent or contractual necessity. GDPR significantly enhances individual rights, including the right of access, rectification, erasure, restriction of processing, data portability, and the right to object.\nThe substantial administrative fines under GDPR serve as a strong deterrent against non-compliance.\nIn conclusion, GDPR represents a significant advancement in data protection law by reinforcing individual rights, enhancing transparency, and establishing a unified regulatory framework across the European Union. However, its practical implementation continues to present challenges for organisations worldwide.\n","categories":["da"],"tags":["da"]},{"title":"Box-Cox and Yeo-Johnson Transformations","url":"/2025/11/14/big_data/da/11-des-statistics/","content":"Box-Cox and Yeo-Johnson TransformationsThe main motivations for transforming variables in data wrangling are to make data more suitable for analysis and modeling. Transformation often address specific challenges or requirements in the dataset, ensuring that it meets the assumptions of analytical methods or enhances interpretability and usability.\nIn this post, we will discuss two common transformations: Box-Cox and Yeo-Johnson. Both transformations are used to transform skewed data to make it more normal or Gaussian-like. The main difference between the two is that Box-Cox transformation is a simple and commonly used transformation, while Yeo-Johnson transformation is more flexible and can handle non-normal data.\nTransforming variables to central normality\nBox‚ÄìCox TransformationThe Box‚ÄìCox transformation is a family of power transformations designed for strictly positive data.\nIt is defined as:\n\ny(\\lambda) =\n\\begin{cases}\n\\dfrac{x^{\\lambda} - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n\\ln(x), & \\lambda = 0\n\\end{cases}Key ideas:\n\nThe parameter Œª (lambda) controls the strength of the transformation.\nWhen Œª = 1, the data are almost unchanged.\nWhen Œª = 0, the transformation becomes the logarithmic transformation.\nSmaller values of Œª compress large observations more strongly, reducing right skewness.\n\nPurpose:\n\nReduce skewness\nStabilize variance\nMake data more compatible with statistical models that assume normality\n\nLimitation:\n\nBox‚ÄìCox cannot be applied to zero or negative values, which restricts its use in some datasets.\n\nYeo-Johnson TransformationThe Yeo‚ÄìJohnson transformation is an extension of the Box‚ÄìCox transformation that allows zero and negative values.\nIt is defined as:\n\ny(\\lambda) =\n\\begin{cases}\n\\dfrac{(x+1)^{\\lambda} - 1}{\\lambda}, & x \\ge 0,\\ \\lambda \\neq 0 \\\\\n\\ln(x+1), & x \\ge 0,\\ \\lambda = 0 \\\\\n-\\dfrac{(-x+1)^{2-\\lambda} - 1}{2-\\lambda}, & x < 0,\\ \\lambda \\neq 2 \\\\\n-\\ln(-x+1), & x < 0,\\ \\lambda = 2\n\\end{cases}¬∑Key ideas:\n\nPositive and negative values are transformed using symmetrical power functions.\nThe transformation is continuous and smooth around zero.\nThe parameter Œª again controls the degree of skewness correction.\n\nAdvantage:\n\nCan be applied to any real-valued data\nMaintains similar interpretability to Box‚ÄìCox\nSuitable for datasets containing negative values or zeros\n\n","categories":["da"],"tags":["da"]},{"title":"Dimensionality Reduction and PCA","url":"/2025/11/12/big_data/da/12-des-statistics/","content":"Dimensionality Reduction and PCADimensionality Reduction: A Comparative Review\nDimensionality reduction is not just a technical trick. It‚Äôs a way of seeing data more clearly, almost like finding the right angle to view a complex sculpture. By reducing dimensions, we reveal the ‚Äúshape‚Äù of the data‚Äîclusters, patterns, and relationships that would otherwise be hidden.\nPrincipal component analysis (PCA) is a fancy name for the whole process of reflecting upon what happens along the projections onto the most variable dimensions. It can be used not only for data visualisation and deduplication, but also feature engineering (as in fact it creates new columns that are linear combinations of existing ones).\nDimension Reduction: A Guided Tour (Chapter 3.2)\nLinear Dimensionality Reduction: Survey, Insights, and Generalizations\n","categories":["da"],"tags":["da"]},{"title":"Data Warehouse Concept","url":"/2025/07/14/big_data/data_warehouse/01-concept/","content":"Data Warehouse ConceptWhat is Data Warehouse?Data Warehouse is a system that stores and processes large amounts of data from multiple sources to provide a single source of truth for business intelligence. It is a collection of data from various sources, including transactional systems, analytical systems, and data sources such as social media, web logs, and customer feedback. The data is transformed, cleaned, and loaded into a central repository for analysis and reporting. Data Warehouse is a critical component of business intelligence, providing a single source of truth for decision-making and insights.\nWhy Data Warehouse?Data Warehouse is a powerful tool for business intelligence because it provides a single source of truth for data from various sources. It enables businesses to analyze and make informed business decisions based on a comprehensive view of their data. Data Warehouse is a central repository for data, which makes it easier to access, analyze, and share data across multiple departments. It also helps to reduce data redundancy and improve data quality.\nData Warehouse ArchitectureData Warehouse architecture consists of several components, including:\n\nData sources: The data sources include transactional systems, analytical systems, and data sources such as social media, web logs, and customer feedback.\nData transformation: Data is transformed, cleaned, and loaded into a central repository for analysis and reporting.\nData integration: Data is integrated from various sources to ensure that it is accurate and consistent.\nData quality: Data quality is maintained by ensuring that data is accurate, complete, and consistent.\nData modeling: Data is modeled to ensure that it is easy to understand and query.\nData storage: Data is stored in a central repository for analysis and reporting.\nData access: Data is accessed through various interfaces, such as a data warehouse user interface, a reporting tool, or an analytical tool.\nData analysis: Data is analyzed to provide insights and make informed business decisions.\nData visualization: Data is visualized to provide a clear and easy-to-understand representation of the data.\n\nData Warehouse TypesThere are several types of Data Warehouses, including:\n\nOLAP (Online Analytical Processing): OLAP is used for complex analytical queries that require fast response times.\nOLTP (Online Transaction Processing): OLTP is used for transactional data, such as sales transactions, inventory transactions, and customer orders.\nDW (Data Warehouse): DW is a central repository for data that is used for complex analytical queries.\nDM (Data Mart): DM is a subset of data that is used for specific analytical queries.\nEDW (Enterprise Data Warehouse): EDW is a multi-dimensional data warehouse that is used for enterprise-level analytical queries.\nIDW (Integrated Data Warehouse): IDW is a data warehouse that is integrated from multiple sources to provide a single source of truth.\nDWH (Data Warehouse Hub): DWH is a central repository for data that is used for complex analytical queries.\n\nData Warehouse ToolsThere are several tools used for Data Warehouse, including:\n\nETL (Extract, Transform, Load): ETL is a process that involves extracting data from various sources, transforming it, and loading it into a data warehouse.\nELT (Extract, Load, Transform): ELT is a process that involves extracting data from various sources, loading it into a data warehouse, and transforming it.\nData Quality Management: Data Quality Management is a process that involves monitoring and managing data quality.\nData Governance: Data Governance is a process that involves managing data across the organization.\nData Stewardship: Data Stewardship is a process that involves ensuring that data is accurate, complete, and consistent.\nData Warehouse Management: Data Warehouse Management is a process that involves managing a data warehouse.\n\nFeatures\nSubject-Oriented: Data Warehouse is subject-oriented, meaning that it is designed to store and process data related to a specific subject or topic.\nIntegrated: Data Warehouse is integrated, meaning that it is designed to integrate data from various sources to provide a single source of truth.\nNon-volatile: Data Warehouse is non-volatile, meaning that it is designed to store data in a central repository and is not subject to data loss.\nTime-variant: Data Warehouse is time-variant, meaning that it is designed to store and process data that is constantly changing.\nDynamic: Data Warehouse is dynamic, meaning that it is designed to continuously update and improve over time.\n\n","categories":["data-warehouse"],"tags":["dw"]},{"title":"Exploratory Data Analysis","url":"/2025/11/10/big_data/eda/01-concept/","content":"Exploratory Data AnalysisEDA%20is,step%20in%20any%20data%20analysis.)\nExploratory Data Analysis(EDA) is the process of looking at data before doing any detailed analysis. Its goal is to understand what the data looks like, find mistakes or unusual values, and discover simple patterns. In EDA, we check how individual variables are distributed, see whether two variables seem related, and identify outliers that may need attention. This is often done using simple charts and basic statistics.\nEDA does not try to prove conclusions. Instead, it helps us understand the data and decide how to analyze it properly. Without EDA, data analysis can easily lead to wrong results.\n","categories":["DA","EDA"],"tags":["EDA"]}]